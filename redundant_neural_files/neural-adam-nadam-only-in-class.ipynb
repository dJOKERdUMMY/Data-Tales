{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the less hidden layes didn't work out as per the graph above so using more hidden layers\n",
    "#as per above diagram using only adam and nadam\n",
    "#importing pandas to visualise the dataset in tabular format\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#introducing stable analysis through random seed\n",
    "np.random.seed(42)\n",
    "#importing time function to create animated behaviour\n",
    "import time\n",
    "#importing regExp for data cleaning\n",
    "import re\n",
    "from numpy import NaN\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/scaled_train.csv\")\n",
    "test = pd.read_csv(\"../data/scaled_test.csv\")\n",
    "\n",
    "X_train = df.drop(['PRT_ID','SALES_PRICE'],axis=1)\n",
    "y_train = df['SALES_PRICE']\n",
    "\n",
    "X_test = test.drop(['PRT_ID'],axis=1)\n",
    "\n",
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "predictors = X_train.values\n",
    "target = y_train.values.reshape((-1,1)).astype('int32')\n",
    "\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model 1\n",
    "model11 = Sequential()\n",
    "model11.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model11.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 2\n",
    "model22 = Sequential()\n",
    "model22.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model22.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 3\n",
    "model33 = Sequential()\n",
    "model33.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model33.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model33.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model33.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model33.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 4\n",
    "model44 = Sequential()\n",
    "model44.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model44.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model44.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model44.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model44.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model11 loss: mean_squared_error\n",
      "Train on 3554 samples, validate on 3555 samples\n",
      "Epoch 1/1000\n",
      "3554/3554 [==============================] - 1s 206us/step - loss: 129568961725006.0781 - val_loss: 104716618222354.3594\n",
      "Epoch 2/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 33853947514176.9727 - val_loss: 7357990539583.1543\n",
      "Epoch 3/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 6733321095987.4307 - val_loss: 6055402029742.6992\n",
      "Epoch 4/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 5586409688569.3730 - val_loss: 5231592685830.6973\n",
      "Epoch 5/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 4921341431151.0723 - val_loss: 4768096144376.7988\n",
      "Epoch 6/1000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 4475839797770.0840 - val_loss: 4734379488378.4189\n",
      "Epoch 7/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 4167320073853.9111 - val_loss: 4211435150956.7368\n",
      "Epoch 8/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 3941275029271.1938 - val_loss: 4063465656431.7612\n",
      "Epoch 9/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 3762204024403.2681 - val_loss: 3954568367470.1050\n",
      "Epoch 10/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 3618941133486.8926 - val_loss: 3796561051191.7368\n",
      "Epoch 11/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 3496673345499.1196 - val_loss: 3649574788315.7783\n",
      "Epoch 12/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 3380948372137.1299 - val_loss: 3559074923740.3545\n",
      "Epoch 13/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 3276624372666.2734 - val_loss: 3451935427433.9287\n",
      "Epoch 14/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 3167874518005.0513 - val_loss: 3333975878130.3179\n",
      "Epoch 15/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 3074197166500.6641 - val_loss: 3286329089842.9121\n",
      "Epoch 16/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2984537068826.9399 - val_loss: 3135338148276.6763\n",
      "Epoch 17/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2882049338214.1406 - val_loss: 3166463339206.3188\n",
      "Epoch 18/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2795139717937.1255 - val_loss: 3149408509760.7383\n",
      "Epoch 19/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2710273376191.4595 - val_loss: 2909820853426.2998\n",
      "Epoch 20/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2616020592643.4570 - val_loss: 2804561955722.1895\n",
      "Epoch 21/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2535268504499.3584 - val_loss: 2747009545343.0278\n",
      "Epoch 22/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2447823394360.7607 - val_loss: 2585283984873.3887\n",
      "Epoch 23/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2365768221769.1841 - val_loss: 2500302022942.3169\n",
      "Epoch 24/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2290818745389.5239 - val_loss: 2443711683402.8198\n",
      "Epoch 25/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2215504241013.4116 - val_loss: 2450348955048.2905\n",
      "Epoch 26/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2152000475815.9775 - val_loss: 2275783962102.0625\n",
      "Epoch 27/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2078524192188.2905 - val_loss: 2191114776823.7188\n",
      "Epoch 28/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2011785413594.5437 - val_loss: 2124060561736.3713\n",
      "Epoch 29/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1952161065235.4485 - val_loss: 2060603098673.3997\n",
      "Epoch 30/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1888467390406.3748 - val_loss: 2008057845091.1594\n",
      "Epoch 31/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1829938202014.3254 - val_loss: 1920510905696.2791\n",
      "Epoch 32/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1769360869531.5881 - val_loss: 1948553637098.7566\n",
      "Epoch 33/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 1729510943046.1587 - val_loss: 1884081756468.2083\n",
      "Epoch 34/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1667451735425.5127 - val_loss: 1744424915802.3740\n",
      "Epoch 35/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 1612213368750.1721 - val_loss: 1726294192629.1982\n",
      "Epoch 36/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 1569115153891.4756 - val_loss: 1740640632007.0391\n",
      "Epoch 37/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1518792664851.7366 - val_loss: 1599113939463.0571\n",
      "Epoch 38/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 1471471857970.5662 - val_loss: 1542239414664.3174\n",
      "Epoch 39/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1432600918266.0933 - val_loss: 1525109426727.3181\n",
      "Epoch 40/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1392030580736.0000 - val_loss: 1487389724262.9761\n",
      "Epoch 41/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1357409846077.8032 - val_loss: 1415471541035.7109\n",
      "Epoch 42/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1318214835629.3079 - val_loss: 1392276776555.0088\n",
      "Epoch 43/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 1289119301650.4402 - val_loss: 1345200303702.8455\n",
      "Epoch 44/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 1250260519353.4092 - val_loss: 1327799470797.2322\n",
      "Epoch 45/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 1226508481427.0884 - val_loss: 1302135851748.2756\n",
      "Epoch 46/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 1198388537107.7366 - val_loss: 1294569587890.8760\n",
      "Epoch 47/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 1172298728585.7244 - val_loss: 1249801547065.1050\n",
      "Epoch 48/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 1149213034909.7490 - val_loss: 1221784673163.3418\n",
      "Epoch 49/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1131375064499.0703 - val_loss: 1241358392392.2993\n",
      "Epoch 50/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1114828280746.1384 - val_loss: 1299931815048.8213\n",
      "Epoch 51/1000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 1095670012490.6246 - val_loss: 1220172608090.5901\n",
      "Epoch 52/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1081772288565.8796 - val_loss: 1159684813798.9402\n",
      "Epoch 53/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 1065176095057.6837 - val_loss: 1167984630197.2524\n",
      "Epoch 54/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 1049119882081.5306 - val_loss: 1116121884336.1394\n",
      "Epoch 55/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 1040501361348.7900 - val_loss: 1144289807564.8000\n",
      "Epoch 56/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 1030363512850.4401 - val_loss: 1122957107847.5251\n",
      "Epoch 57/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1020120199686.6270 - val_loss: 1091150815335.6962\n",
      "Epoch 58/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1015484756449.7468 - val_loss: 1099467958973.1016\n",
      "Epoch 59/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1004006243485.3168 - val_loss: 1074482230740.0731\n",
      "Epoch 60/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 994334759987.2864 - val_loss: 1059150335950.4563\n",
      "Epoch 61/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 986240986243.9618 - val_loss: 1054113419062.3685\n",
      "Epoch 62/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 984794634706.7642 - val_loss: 1047791610400.4050\n",
      "Epoch 63/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 978568834908.3444 - val_loss: 1058903112700.2554\n",
      "Epoch 64/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 971998821625.5172 - val_loss: 1046125570984.1462\n",
      "Epoch 65/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 967184908380.2003 - val_loss: 1031252938840.7179\n",
      "Epoch 66/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 964968848960.8284 - val_loss: 1077177557814.6565\n",
      "Epoch 67/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 963113670551.1221 - val_loss: 1061006574369.9174\n",
      "Epoch 68/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 958800897525.9156 - val_loss: 1068398982519.8987\n",
      "Epoch 69/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 958660127396.5200 - val_loss: 1040213235895.7727\n",
      "Epoch 70/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 947505428384.3420 - val_loss: 1080808960925.2006\n",
      "Epoch 71/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 950182687598.2083 - val_loss: 1012377612263.8042\n",
      "Epoch 72/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 943218531886.3883 - val_loss: 1141546716857.9331\n",
      "Epoch 73/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 946151270744.5988 - val_loss: 1003402347339.3958\n",
      "Epoch 74/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 939270027122.2419 - val_loss: 1005499905312.3330\n",
      "Epoch 75/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 938604073234.2960 - val_loss: 998011773529.4380\n",
      "Epoch 76/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 932313307519.7839 - val_loss: 1026426805638.3010\n",
      "Epoch 77/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 932308353153.0804 - val_loss: 998511215292.5255\n",
      "Epoch 78/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 931091739806.4694 - val_loss: 993099156956.7145\n",
      "Epoch 79/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 928733492769.7108 - val_loss: 1016042777309.3625\n",
      "Epoch 80/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 926350526946.8993 - val_loss: 987988761695.3429\n",
      "Epoch 81/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 923441931581.5149 - val_loss: 986132890646.1794\n",
      "Epoch 82/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 923185268928.4683 - val_loss: 1016377385358.6543\n",
      "Epoch 83/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 921836765082.5796 - val_loss: 1009998591771.2922\n",
      "Epoch 84/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 923131948588.0833 - val_loss: 1043013106993.0397\n",
      "Epoch 85/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 920409614791.2391 - val_loss: 1053364303611.0312\n",
      "Epoch 86/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 918863965509.5824 - val_loss: 1017471654237.6866\n",
      "Epoch 87/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 915475799367.8876 - val_loss: 976158017225.4874\n",
      "Epoch 88/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 911278137929.4722 - val_loss: 972171199980.2689\n",
      "Epoch 89/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 906927244605.5149 - val_loss: 992097873172.2351\n",
      "Epoch 90/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 910859791519.6218 - val_loss: 1021500520302.2493\n",
      "Epoch 91/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 907811842383.3788 - val_loss: 966172632272.8326\n",
      "Epoch 92/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 908909028134.7529 - val_loss: 969421250391.7817\n",
      "Epoch 93/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 905865569016.6528 - val_loss: 985027325093.6259\n",
      "Epoch 94/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 903546843922.0079 - val_loss: 966436269055.7119\n",
      "Epoch 95/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 904162455655.7253 - val_loss: 962510813784.8619\n",
      "Epoch 96/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 900069221232.5132 - val_loss: 960257878638.7533\n",
      "Epoch 97/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 897551860726.2037 - val_loss: 981961992750.2312\n",
      "Epoch 98/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 896511588500.6731 - val_loss: 972264829668.8518\n",
      "Epoch 99/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 899090606175.6580 - val_loss: 975255547644.7595\n",
      "Epoch 100/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 897985734972.9387 - val_loss: 959111940698.0140\n",
      "Epoch 101/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 894307913532.0742 - val_loss: 961694626413.0250\n",
      "Epoch 102/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 892089762584.9230 - val_loss: 954823012926.9379\n",
      "Epoch 103/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 893025551608.3646 - val_loss: 959361415695.1223\n",
      "Epoch 104/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 887887091293.6410 - val_loss: 971727872555.3508\n",
      "Epoch 105/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 891840821343.0095 - val_loss: 992974380468.3882\n",
      "Epoch 106/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 891021101336.0585 - val_loss: 958582086175.2529\n",
      "Epoch 107/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 886394455712.4862 - val_loss: 972412781235.3080\n",
      "Epoch 108/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 889292170409.9944 - val_loss: 946049643753.0284\n",
      "Epoch 109/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 886041981418.9668 - val_loss: 942068438196.0281\n",
      "Epoch 110/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 884530161344.7563 - val_loss: 949213216606.6948\n",
      "Epoch 111/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 883973101638.3027 - val_loss: 941797480885.2523\n",
      "Epoch 112/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 880247877090.3230 - val_loss: 980059137753.6180\n",
      "Epoch 113/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 886176786278.7169 - val_loss: 957535777223.1111\n",
      "Epoch 114/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 883810696884.0787 - val_loss: 977230583991.7727\n",
      "Epoch 115/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 884261762768.8915 - val_loss: 943931859545.4380\n",
      "Epoch 116/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 880733225596.1824 - val_loss: 955540680942.5012\n",
      "Epoch 117/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 877544685964.4614 - val_loss: 948039771243.4408\n",
      "Epoch 118/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 879150336312.3286 - val_loss: 933197927488.8102\n",
      "Epoch 119/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 878394580231.3472 - val_loss: 933002254764.0349\n",
      "Epoch 120/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 877473783056.5673 - val_loss: 930365217306.6442\n",
      "Epoch 121/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 878917523692.2633 - val_loss: 995956244392.4343\n",
      "Epoch 122/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 875313307458.9893 - val_loss: 1114732038658.4485\n",
      "Epoch 123/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 887799611687.0410 - val_loss: 943753997053.6237\n",
      "Epoch 124/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 873340795895.3562 - val_loss: 933314855732.3522\n",
      "Epoch 125/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 873198042684.2184 - val_loss: 939318558881.3052\n",
      "Epoch 126/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 79us/step - loss: 873307794444.1012 - val_loss: 933614103984.9316\n",
      "Epoch 127/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 869227549674.6787 - val_loss: 929778455192.8079\n",
      "Epoch 128/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 869414329339.9663 - val_loss: 924845256260.4106\n",
      "Epoch 129/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 868794055625.2561 - val_loss: 968815018739.8301\n",
      "Epoch 130/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 870760269868.9478 - val_loss: 931770261215.9550\n",
      "Epoch 131/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 873601527810.3049 - val_loss: 921623009496.0338\n",
      "Epoch 132/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 864937915578.1295 - val_loss: 931681892510.4248\n",
      "Epoch 133/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 865186852027.2819 - val_loss: 921031562599.7682\n",
      "Epoch 134/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 870658573339.0839 - val_loss: 954589958117.4999\n",
      "Epoch 135/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 869379636491.9573 - val_loss: 923837471857.7778\n",
      "Epoch 136/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 865359421813.4114 - val_loss: 921806335312.1486\n",
      "Epoch 137/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 864761865616.4952 - val_loss: 922500076641.0712\n",
      "Epoch 138/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 863858914038.3478 - val_loss: 915362600489.3345\n",
      "Epoch 139/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 860106126715.7501 - val_loss: 916708629977.2579\n",
      "Epoch 140/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 860666313691.6962 - val_loss: 915415233805.8982\n",
      "Epoch 141/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 861825005661.3528 - val_loss: 933168606553.3660\n",
      "Epoch 142/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 860769834654.1813 - val_loss: 920439362583.3317\n",
      "Epoch 143/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 859768468882.8002 - val_loss: 920965889075.5601\n",
      "Epoch 144/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 861184743356.0022 - val_loss: 912749647281.5077\n",
      "Epoch 145/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 860045105471.2437 - val_loss: 913446907847.5432\n",
      "Epoch 146/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 858664237610.3545 - val_loss: 930411535459.9517\n",
      "Epoch 147/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 859980103815.9955 - val_loss: 912671939356.7325\n",
      "Epoch 148/1000\n",
      "3554/3554 [==============================] - 0s 132us/step - loss: 856513042474.6427 - val_loss: 954169801420.0798\n",
      "Epoch 149/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 858229688180.5470 - val_loss: 916487816850.1829\n",
      "Epoch 150/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 855036420450.9713 - val_loss: 913966908392.0923\n",
      "Epoch 151/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 856023186782.9376 - val_loss: 923530356335.6174\n",
      "Epoch 152/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 857692652511.1537 - val_loss: 921730823165.9836\n",
      "Epoch 153/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 859307289234.0798 - val_loss: 920405089836.2150\n",
      "Epoch 154/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 856788166972.9387 - val_loss: 909958813644.7280\n",
      "Epoch 155/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 853230343663.0006 - val_loss: 904524035701.3783\n",
      "Epoch 156/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 854883710801.9719 - val_loss: 940326029411.6636\n",
      "Epoch 157/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 853227865834.2465 - val_loss: 960439433673.9916\n",
      "Epoch 158/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 854390249062.8610 - val_loss: 905080368427.2788\n",
      "Epoch 159/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 851300425926.2307 - val_loss: 915651666622.8298\n",
      "Epoch 160/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 850782922727.7974 - val_loss: 931477955041.6112\n",
      "Epoch 161/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 848918086959.6849 - val_loss: 970273566958.5012\n",
      "Epoch 162/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 851092376076.3895 - val_loss: 931975144560.0496\n",
      "Epoch 163/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 850511767140.5559 - val_loss: 940820062307.0875\n",
      "Epoch 164/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 856002493388.7136 - val_loss: 930724481160.5333\n",
      "Epoch 165/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 849129381883.3900 - val_loss: 965021934925.2681\n",
      "Epoch 166/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 849272350013.5149 - val_loss: 934096142501.9139\n",
      "Epoch 167/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 849937217527.9325 - val_loss: 909525420684.7100\n",
      "Epoch 168/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 847599947672.2747 - val_loss: 897024192275.8031\n",
      "Epoch 169/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 844660464138.0844 - val_loss: 897102302154.7117\n",
      "Epoch 170/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 849819119775.0455 - val_loss: 898774121200.9497\n",
      "Epoch 171/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 843031820279.9325 - val_loss: 895015749768.5333\n",
      "Epoch 172/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 844320952188.6145 - val_loss: 900502430011.6974\n",
      "Epoch 173/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 843132054901.9877 - val_loss: 919497398723.0785\n",
      "Epoch 174/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 848971429027.0793 - val_loss: 891986813925.7880\n",
      "Epoch 175/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 842758370822.0507 - val_loss: 897546108149.1263\n",
      "Epoch 176/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 843759548361.8323 - val_loss: 892004667738.8062\n",
      "Epoch 177/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 840628792774.6630 - val_loss: 897794621140.4332\n",
      "Epoch 178/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 839330859523.1694 - val_loss: 902304204696.8799\n",
      "Epoch 179/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 846472780030.7035 - val_loss: 901253727102.0917\n",
      "Epoch 180/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 840724287651.6556 - val_loss: 939993980824.3038\n",
      "Epoch 181/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 840515097993.5802 - val_loss: 903613371189.5044\n",
      "Epoch 182/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 839071165396.7811 - val_loss: 893896678453.8644\n",
      "Epoch 183/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 838578136704.2161 - val_loss: 889496387560.9564\n",
      "Epoch 184/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 840135755784.0675 - val_loss: 1004422278042.3201\n",
      "Epoch 185/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 844354044706.1429 - val_loss: 889474292379.6884\n",
      "Epoch 186/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 843343064721.5037 - val_loss: 889157835885.1691\n",
      "Epoch 187/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 836396114809.7333 - val_loss: 886161142758.9401\n",
      "Epoch 188/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 838599594009.3550 - val_loss: 894987278367.6849\n",
      "Epoch 189/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 839246189353.0580 - val_loss: 885365685963.2157\n",
      "Epoch 190/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 836181180794.0214 - val_loss: 886905359933.4976\n",
      "Epoch 191/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 835105654764.4075 - val_loss: 889941715539.1010\n",
      "Epoch 192/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 835261201731.2775 - val_loss: 890495278977.2603\n",
      "Epoch 193/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 830861565680.0090 - val_loss: 1008395615076.1676\n",
      "Epoch 194/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 848567875188.6910 - val_loss: 913238637849.9961\n",
      "Epoch 195/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 839645782942.6133 - val_loss: 883755952681.6226\n",
      "Epoch 196/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 833853826171.8943 - val_loss: 883424992084.0371\n",
      "Epoch 197/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 832478694693.3123 - val_loss: 1042996822357.0453\n",
      "Epoch 198/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 843487658327.4463 - val_loss: 899437009139.6860\n",
      "Epoch 199/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 831272066495.7479 - val_loss: 882705510629.2838\n",
      "Epoch 200/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 833923929251.0793 - val_loss: 927662857933.5201\n",
      "Epoch 201/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 831029050359.3562 - val_loss: 892736265703.0841\n",
      "Epoch 202/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 831856411011.2415 - val_loss: 886305531711.0099\n",
      "Epoch 203/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 830731478305.8547 - val_loss: 885254876975.4554\n",
      "Epoch 204/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 833433684280.3286 - val_loss: 906295961052.1384\n",
      "Epoch 205/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 835429751047.9235 - val_loss: 906240260742.9491\n",
      "Epoch 206/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 839349367643.7682 - val_loss: 883502487620.5547\n",
      "Epoch 207/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 827795526339.6377 - val_loss: 895462312514.9705\n",
      "Epoch 208/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 830850544743.1492 - val_loss: 876276326088.9114\n",
      "Epoch 209/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 829255252835.8356 - val_loss: 886063141865.2444\n",
      "Epoch 210/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 831449946560.3241 - val_loss: 889152549461.9814\n",
      "Epoch 211/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 832956818500.5740 - val_loss: 948254623064.7899\n",
      "Epoch 212/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 835359790510.1722 - val_loss: 899703560716.8180\n",
      "Epoch 213/1000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 828666294523.2460 - val_loss: 876841770475.6929\n",
      "Epoch 214/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 830099534831.2887 - val_loss: 934546415433.9556\n",
      "Epoch 215/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 831941154079.5498 - val_loss: 875631086008.7089\n",
      "Epoch 216/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 827157853886.4513 - val_loss: 881168803568.6616\n",
      "Epoch 217/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 827648864156.8846 - val_loss: 885006956978.9480\n",
      "Epoch 218/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 829982847466.9668 - val_loss: 878210519534.2853\n",
      "Epoch 219/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 828576065345.8368 - val_loss: 913724333939.4341\n",
      "Epoch 220/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 831180339864.9950 - val_loss: 902205695332.3116\n",
      "Epoch 221/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 829412619191.9685 - val_loss: 870214757730.5834\n",
      "Epoch 222/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 825104354873.9133 - val_loss: 882675141597.1466\n",
      "Epoch 223/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 822994190308.3398 - val_loss: 870803701243.8234\n",
      "Epoch 224/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 825710337978.8497 - val_loss: 941222759034.2751\n",
      "Epoch 225/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 830223742184.2296 - val_loss: 891750707277.1960\n",
      "Epoch 226/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 825086061153.8187 - val_loss: 869308917258.2256\n",
      "Epoch 227/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 823431920557.5959 - val_loss: 870813939017.8115\n",
      "Epoch 228/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 823514977406.1992 - val_loss: 870987536618.4686\n",
      "Epoch 229/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 826698173429.0511 - val_loss: 931838607853.1331\n",
      "Epoch 230/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 826633219070.2712 - val_loss: 875644030115.3215\n",
      "Epoch 231/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 823586349613.8120 - val_loss: 868145030675.4430\n",
      "Epoch 232/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 825717472957.2988 - val_loss: 868116802485.9724\n",
      "Epoch 233/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 824903769028.0697 - val_loss: 881813396013.0791\n",
      "Epoch 234/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 820779814799.0546 - val_loss: 886553001398.6925\n",
      "Epoch 235/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 822220836027.2819 - val_loss: 871381766444.7190\n",
      "Epoch 236/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 822120694340.8621 - val_loss: 885343582025.3795\n",
      "Epoch 237/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 821650637251.7816 - val_loss: 871239751573.4233\n",
      "Epoch 238/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 822413363936.4502 - val_loss: 864582402702.4382\n",
      "Epoch 239/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 820270273136.6573 - val_loss: 867350127257.9601\n",
      "Epoch 240/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 821000778938.1295 - val_loss: 877965505188.3297\n",
      "Epoch 241/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 822974002564.9702 - val_loss: 871659538762.0996\n",
      "Epoch 242/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 826158813071.0546 - val_loss: 894535410965.9634\n",
      "Epoch 243/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 822226478148.5740 - val_loss: 888062815379.1910\n",
      "Epoch 244/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 821473380922.4896 - val_loss: 906795439316.2892\n",
      "Epoch 245/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 827575854727.1312 - val_loss: 871502218405.0498\n",
      "Epoch 246/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 819714798888.7698 - val_loss: 910364317211.5083\n",
      "Epoch 247/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 825432521322.8948 - val_loss: 876202754269.2185\n",
      "Epoch 248/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 820728913869.2898 - val_loss: 862464285542.4720\n",
      "Epoch 249/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 819393368080.1351 - val_loss: 885802386859.7468\n",
      "Epoch 250/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 818779029421.0197 - val_loss: 872926083589.9049\n",
      "Epoch 251/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 815763759505.0714 - val_loss: 870962786630.0669\n",
      "Epoch 252/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 92us/step - loss: 819099583782.4648 - val_loss: 863059831434.4056\n",
      "Epoch 253/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 817936172206.6044 - val_loss: 862409813936.4996\n",
      "Epoch 254/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 819104141447.9955 - val_loss: 894402485965.5201\n",
      "Epoch 255/1000\n",
      "3554/3554 [==============================] - 0s 137us/step - loss: 824770484321.9629 - val_loss: 963191037940.4781\n",
      "Epoch 256/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 824118201411.4215 - val_loss: 859880995163.3823\n",
      "Epoch 257/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 817200793082.5256 - val_loss: 858577604281.9331\n",
      "Epoch 258/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 815348559994.1655 - val_loss: 857335735463.3541\n",
      "Epoch 259/1000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 814419772074.2825 - val_loss: 880803694180.0956\n",
      "Epoch 260/1000\n",
      "3554/3554 [==============================] - 0s 133us/step - loss: 817125449031.8876 - val_loss: 857003570109.4615\n",
      "Epoch 261/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 816098158401.8368 - val_loss: 868057309826.9165\n",
      "Epoch 262/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 818874525985.2786 - val_loss: 856663705759.5769\n",
      "Epoch 263/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 817987520713.6882 - val_loss: 897760826317.5922\n",
      "Epoch 264/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 820405303168.0720 - val_loss: 881172830354.9030\n",
      "Epoch 265/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 815853057892.4120 - val_loss: 858300569101.6821\n",
      "Epoch 266/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 812563544277.2133 - val_loss: 930343007829.1173\n",
      "Epoch 267/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 819603155011.9978 - val_loss: 853522876165.4009\n",
      "Epoch 268/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 813132115324.9027 - val_loss: 853293802534.3099\n",
      "Epoch 269/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 814740579276.7136 - val_loss: 868718716559.8785\n",
      "Epoch 270/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 812306214831.3247 - val_loss: 902755475195.6073\n",
      "Epoch 271/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 817381929718.3478 - val_loss: 916835124872.9655\n",
      "Epoch 272/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 822298997568.1080 - val_loss: 865419196085.9004\n",
      "Epoch 273/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 811420774420.7451 - val_loss: 884973708012.9170\n",
      "Epoch 274/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 819915535408.9814 - val_loss: 853206391557.6888\n",
      "Epoch 275/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 812449912607.2617 - val_loss: 876057598776.3848\n",
      "Epoch 276/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 812864069055.7479 - val_loss: 859959163333.3828\n",
      "Epoch 277/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 811405384497.1256 - val_loss: 873089515589.1309\n",
      "Epoch 278/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 812211770042.4176 - val_loss: 921028991929.4290\n",
      "Epoch 279/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 819395844996.1057 - val_loss: 850533580329.9105\n",
      "Epoch 280/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 810826493235.1425 - val_loss: 866080975664.3196\n",
      "Epoch 281/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 811314279951.2706 - val_loss: 853105719781.6439\n",
      "Epoch 282/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 812069267874.3590 - val_loss: 852249255882.9998\n",
      "Epoch 283/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 808632595660.5695 - val_loss: 853227962100.6942\n",
      "Epoch 284/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 809663101159.6534 - val_loss: 856812801431.5837\n",
      "Epoch 285/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 810491060956.9927 - val_loss: 850562995633.5077\n",
      "Epoch 286/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 814162667449.6973 - val_loss: 863816757375.8920\n",
      "Epoch 287/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 814586394210.2510 - val_loss: 858222871297.6562\n",
      "Epoch 288/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 808297009426.2960 - val_loss: 853138836975.1494\n",
      "Epoch 289/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 809565288393.8323 - val_loss: 888037113395.1279\n",
      "Epoch 290/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 808722061391.5228 - val_loss: 856218914973.8486\n",
      "Epoch 291/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 814340743698.1520 - val_loss: 852612110575.0774\n",
      "Epoch 292/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 810967305057.5306 - val_loss: 861244224081.3727\n",
      "Epoch 293/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 809245448206.9825 - val_loss: 853991294753.0532\n",
      "Epoch 294/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 814982187274.2285 - val_loss: 905915019124.5862\n",
      "Epoch 295/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 813117154658.3951 - val_loss: 850302680908.2599\n",
      "Epoch 296/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 806551487791.6849 - val_loss: 856358038665.9735\n",
      "Epoch 297/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 808940610121.4722 - val_loss: 847615778832.1305\n",
      "Epoch 298/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 808118614625.6748 - val_loss: 852995591022.5372\n",
      "Epoch 299/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 807979258296.2566 - val_loss: 909031577760.7291\n",
      "Epoch 300/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 813254471995.7861 - val_loss: 862551883596.2599\n",
      "Epoch 301/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 803493233313.0624 - val_loss: 844612663915.2968\n",
      "Epoch 302/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 810602413111.8965 - val_loss: 860179163168.8372\n",
      "Epoch 303/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 806627463267.1154 - val_loss: 869230573201.0306\n",
      "Epoch 304/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 807561577637.3844 - val_loss: 845940125146.4102\n",
      "Epoch 305/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 804297395142.9510 - val_loss: 845355918328.2228\n",
      "Epoch 306/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 808523209530.3455 - val_loss: 844874452417.0621\n",
      "Epoch 307/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 806151064601.9314 - val_loss: 854143154145.4673\n",
      "Epoch 308/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 808585317171.4305 - val_loss: 843255359783.5342\n",
      "Epoch 309/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 807541743463.8694 - val_loss: 844068108091.5533\n",
      "Epoch 310/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 807104468315.4800 - val_loss: 866546303994.5271\n",
      "Epoch 311/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 808428716022.7799 - val_loss: 867450509864.4703\n",
      "Epoch 312/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 806400441510.5369 - val_loss: 890998120206.0422\n",
      "Epoch 313/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 806267216162.4312 - val_loss: 844957533077.1353\n",
      "Epoch 314/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 808301283400.6077 - val_loss: 875507838066.9299\n",
      "Epoch 315/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 812089045432.2566 - val_loss: 841331117965.9342\n",
      "Epoch 316/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 805301688058.3816 - val_loss: 864539607694.4382\n",
      "Epoch 317/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 810002695609.9854 - val_loss: 840519743535.8154\n",
      "Epoch 318/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 803990875931.8041 - val_loss: 846811719510.0535\n",
      "Epoch 319/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 800918899571.9707 - val_loss: 848732610751.2618\n",
      "Epoch 320/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 807335943754.0483 - val_loss: 839116491533.7542\n",
      "Epoch 321/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 804870923581.5149 - val_loss: 844576903676.1115\n",
      "Epoch 322/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 803798333690.0934 - val_loss: 841735610914.1333\n",
      "Epoch 323/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 802531099808.7743 - val_loss: 851867972781.1150\n",
      "Epoch 324/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 805882207716.6281 - val_loss: 851302106161.8318\n",
      "Epoch 325/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 806730919790.7844 - val_loss: 863456663694.2942\n",
      "Epoch 326/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 806742405758.4873 - val_loss: 846321721751.8717\n",
      "Epoch 327/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 808239252064.5222 - val_loss: 857571376409.9961\n",
      "Epoch 328/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 801394545642.6787 - val_loss: 842288997838.3123\n",
      "Epoch 329/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 803818546438.1947 - val_loss: 845751604675.9426\n",
      "Epoch 330/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 799588534704.1891 - val_loss: 923712113122.1874\n",
      "Epoch 331/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 811364023227.4260 - val_loss: 844622319700.1091\n",
      "Epoch 332/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 800659185620.7811 - val_loss: 839484349360.2115\n",
      "Epoch 333/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 803177643653.9786 - val_loss: 847205999866.3112\n",
      "Epoch 334/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 805159584795.0839 - val_loss: 852956028589.2590\n",
      "Epoch 335/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 802681169951.1176 - val_loss: 908213636107.2338\n",
      "Epoch 336/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 805315167407.7568 - val_loss: 843123976234.0546\n",
      "Epoch 337/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 800740165603.1874 - val_loss: 839825168452.5547\n",
      "Epoch 338/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 800288852324.1238 - val_loss: 841062847649.5933\n",
      "Epoch 339/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 801528436421.9426 - val_loss: 845055949511.4712\n",
      "Epoch 340/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 799942486932.2408 - val_loss: 873469449810.8130\n",
      "Epoch 341/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 806942563010.4851 - val_loss: 845641924031.9100\n",
      "Epoch 342/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 802387641214.3433 - val_loss: 847727195433.5505\n",
      "Epoch 343/1000\n",
      "3554/3554 [==============================] - 0s 139us/step - loss: 803530330924.5155 - val_loss: 850861639092.9642\n",
      "Epoch 344/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 799694776931.9797 - val_loss: 898559483758.8253\n",
      "Epoch 345/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 803358468210.0979 - val_loss: 841981577827.2314\n",
      "Epoch 346/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 796077846345.9044 - val_loss: 887666689838.8793\n",
      "Epoch 347/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 804732623517.6049 - val_loss: 868071859316.6582\n",
      "Epoch 348/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 799868823583.1176 - val_loss: 835989255304.2452\n",
      "Epoch 349/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 798731368495.2527 - val_loss: 836481816838.1210\n",
      "Epoch 350/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 801518557738.3545 - val_loss: 832876096686.8433\n",
      "Epoch 351/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 796676540018.3861 - val_loss: 912743856664.3398\n",
      "Epoch 352/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 807807753977.2290 - val_loss: 862211711641.9601\n",
      "Epoch 353/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 799436581134.8385 - val_loss: 833704614265.3390\n",
      "Epoch 354/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 798964782344.4998 - val_loss: 862967603829.0903\n",
      "Epoch 355/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 801512925915.8401 - val_loss: 832795665487.7885\n",
      "Epoch 356/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 798419469235.3584 - val_loss: 834722370684.7235\n",
      "Epoch 357/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 796655775875.3855 - val_loss: 836111815680.2881\n",
      "Epoch 358/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 796601872709.0062 - val_loss: 833194378142.3528\n",
      "Epoch 359/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 797674990604.1012 - val_loss: 836310459462.5710\n",
      "Epoch 360/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 803446396094.7395 - val_loss: 880311622940.3004\n",
      "Epoch 361/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 802985039988.4030 - val_loss: 831628676314.6262\n",
      "Epoch 362/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 795623203825.5936 - val_loss: 846079206180.2217\n",
      "Epoch 363/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 793995189566.6675 - val_loss: 898198913406.5238\n",
      "Epoch 364/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 799739089834.1385 - val_loss: 873746487276.9890\n",
      "Epoch 365/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 796588376580.3219 - val_loss: 828147946901.8555\n",
      "Epoch 366/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 791104669888.4683 - val_loss: 828910624009.8656\n",
      "Epoch 367/1000\n",
      "3554/3554 [==============================] - 0s 137us/step - loss: 797584027521.2246 - val_loss: 843728350258.4078\n",
      "Epoch 368/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 799027664620.5515 - val_loss: 827621616612.0597\n",
      "Epoch 369/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 791513718456.6888 - val_loss: 896548734716.1833\n",
      "Epoch 370/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 796270822261.1232 - val_loss: 828205614341.8329\n",
      "Epoch 371/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 790264042529.9989 - val_loss: 891581579213.3041\n",
      "Epoch 372/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 796506196206.5684 - val_loss: 829289734155.5219\n",
      "Epoch 373/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 788189007215.0725 - val_loss: 821941018478.8253\n",
      "Epoch 374/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 791094227351.9865 - val_loss: 833738723384.4568\n",
      "Epoch 375/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 788536016720.8193 - val_loss: 829250202324.1451\n",
      "Epoch 376/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 790832027275.7411 - val_loss: 845982655023.9595\n",
      "Epoch 377/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 782829316726.4198 - val_loss: 826071640917.1893\n",
      "Epoch 378/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 127us/step - loss: 788091586314.5166 - val_loss: 854033692378.7701\n",
      "Epoch 379/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 780908564072.0135 - val_loss: 819979919565.0880\n",
      "Epoch 380/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 780084123810.5031 - val_loss: 861679051109.1758\n",
      "Epoch 381/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 787353559053.8300 - val_loss: 816111716636.0123\n",
      "Epoch 382/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 779944098724.3760 - val_loss: 812525204063.1989\n",
      "Epoch 383/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 781900659140.9342 - val_loss: 817575304379.5173\n",
      "Epoch 384/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 778625859556.3398 - val_loss: 838349610683.0852\n",
      "Epoch 385/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 779100218455.0140 - val_loss: 818587707641.4470\n",
      "Epoch 386/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 780695845243.1740 - val_loss: 818361364087.1066\n",
      "Epoch 387/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 778947557220.4120 - val_loss: 826843033907.6321\n",
      "Epoch 388/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 776478999809.5846 - val_loss: 832625310453.5583\n",
      "Epoch 389/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 781376795041.2065 - val_loss: 809441380686.7083\n",
      "Epoch 390/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 776779168556.5155 - val_loss: 805454434968.8079\n",
      "Epoch 391/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 770942028844.3713 - val_loss: 817167074257.0487\n",
      "Epoch 392/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 772553542563.2234 - val_loss: 802957541518.8704\n",
      "Epoch 393/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 774255307363.9797 - val_loss: 837930311078.5620\n",
      "Epoch 394/1000\n",
      "3554/3554 [==============================] - 1s 155us/step - loss: 774635134897.6298 - val_loss: 816242070560.5491\n",
      "Epoch 395/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 772711199994.6697 - val_loss: 812064520714.5137\n",
      "Epoch 396/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 771098872673.5306 - val_loss: 958330927741.7317\n",
      "Epoch 397/1000\n",
      "3554/3554 [==============================] - 1s 151us/step - loss: 776595741959.3472 - val_loss: 816691527361.9983\n",
      "Epoch 398/1000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 765344364499.0522 - val_loss: 798116990396.4535\n",
      "Epoch 399/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 767597380229.4025 - val_loss: 797202690304.9362\n",
      "Epoch 400/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 764342023261.9291 - val_loss: 808677219976.3893\n",
      "Epoch 401/1000\n",
      "3554/3554 [==============================] - 0s 139us/step - loss: 771989851669.0332 - val_loss: 804052525191.9573\n",
      "Epoch 402/1000\n",
      "3554/3554 [==============================] - 0s 139us/step - loss: 763915825725.9471 - val_loss: 810548842192.9767\n",
      "Epoch 403/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 764764775640.0945 - val_loss: 801895202700.7820\n",
      "Epoch 404/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 765177781373.0466 - val_loss: 804668690792.6323\n",
      "Epoch 405/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 761853606848.0360 - val_loss: 825827455219.1100\n",
      "Epoch 406/1000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 767767684155.3539 - val_loss: 827100534437.7699\n",
      "Epoch 407/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 765876775736.0405 - val_loss: 828695270630.7240\n",
      "Epoch 408/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 765987751602.9263 - val_loss: 805691599650.4934\n",
      "Epoch 409/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 757276566707.2145 - val_loss: 791434322765.7002\n",
      "Epoch 410/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 763054533694.8114 - val_loss: 792597127128.8259\n",
      "Epoch 411/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 760255721962.9668 - val_loss: 811322941202.6509\n",
      "Epoch 412/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 768092445950.1272 - val_loss: 794840096562.9120\n",
      "Epoch 413/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 759965363737.6433 - val_loss: 793240959816.5154\n",
      "Epoch 414/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 756087836015.0725 - val_loss: 787375931525.6528\n",
      "Epoch 415/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 755231309925.4204 - val_loss: 788172876255.8829\n",
      "Epoch 416/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 755768560407.7704 - val_loss: 874255327308.0438\n",
      "Epoch 417/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 764217687197.3168 - val_loss: 818295616270.3302\n",
      "Epoch 418/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 758743176696.2206 - val_loss: 792269839201.8633\n",
      "Epoch 419/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 755971877420.6595 - val_loss: 838417073164.9620\n",
      "Epoch 420/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 757589332749.3979 - val_loss: 816235043681.8633\n",
      "Epoch 421/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 757397487597.5599 - val_loss: 813611471024.2836\n",
      "Epoch 422/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 759940068132.4480 - val_loss: 792645898540.7190\n",
      "Epoch 423/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 757371289056.0179 - val_loss: 800880341073.8048\n",
      "Epoch 424/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 759621372225.5487 - val_loss: 803680381698.2323\n",
      "Epoch 425/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 752545191056.6393 - val_loss: 784818697854.5958\n",
      "Epoch 426/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 755354249537.5487 - val_loss: 781821563286.7196\n",
      "Epoch 427/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 753809680922.2195 - val_loss: 792551997158.8680\n",
      "Epoch 428/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 753201393730.8452 - val_loss: 793938185006.0151\n",
      "Epoch 429/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 751648320348.9207 - val_loss: 782203755301.9500\n",
      "Epoch 430/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 753598618454.0056 - val_loss: 853174838397.8756\n",
      "Epoch 431/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 758403506189.8300 - val_loss: 814536660194.1154\n",
      "Epoch 432/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 748300580671.5317 - val_loss: 804687287365.4188\n",
      "Epoch 433/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 756062889443.4756 - val_loss: 781855654708.3522\n",
      "Epoch 434/1000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 749954633781.0151 - val_loss: 779009134404.1947\n",
      "Epoch 435/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 747816127331.8356 - val_loss: 780654405323.7919\n",
      "Epoch 436/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 746012576435.2145 - val_loss: 779237782171.1122\n",
      "Epoch 437/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 752414253632.2521 - val_loss: 778584740947.2450\n",
      "Epoch 438/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 751968498523.7682 - val_loss: 785229226714.4822\n",
      "Epoch 439/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 747140896109.9202 - val_loss: 779127583875.6366\n",
      "Epoch 440/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 750629051885.8480 - val_loss: 782260221801.6405\n",
      "Epoch 441/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 746919507277.6500 - val_loss: 781304327587.1055\n",
      "Epoch 442/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 748493439547.6422 - val_loss: 791063039590.9761\n",
      "Epoch 443/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 749412523013.1863 - val_loss: 823698938316.2959\n",
      "Epoch 444/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 749735970280.0854 - val_loss: 777119834695.0031\n",
      "Epoch 445/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 745280795612.8485 - val_loss: 783716391810.1244\n",
      "Epoch 446/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 747685482714.9758 - val_loss: 779963787545.7080\n",
      "Epoch 447/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 747114476596.4390 - val_loss: 800411020227.7986\n",
      "Epoch 448/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 744575590589.0107 - val_loss: 784944583929.1589\n",
      "Epoch 449/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 750476850260.7091 - val_loss: 804618011481.2219\n",
      "Epoch 450/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 747807127488.6122 - val_loss: 796402512474.5902\n",
      "Epoch 451/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 748291853098.2104 - val_loss: 794354105478.5170\n",
      "Epoch 452/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 747508327474.1340 - val_loss: 774794971558.2739\n",
      "Epoch 453/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 746789240961.0804 - val_loss: 809273869159.0481\n",
      "Epoch 454/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 745677021538.9713 - val_loss: 782565888120.9789\n",
      "Epoch 455/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 743784741713.3956 - val_loss: 774199419453.7856\n",
      "Epoch 456/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 743876069319.5272 - val_loss: 775371695575.5297\n",
      "Epoch 457/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 742320930871.3202 - val_loss: 830651555275.4318\n",
      "Epoch 458/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 747553531345.0354 - val_loss: 774982970441.4514\n",
      "Epoch 459/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 743457193928.6798 - val_loss: 776731775143.0662\n",
      "Epoch 460/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 744510507587.7096 - val_loss: 779730546774.4135\n",
      "Epoch 461/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 745580956688.7113 - val_loss: 772323602287.9775\n",
      "Epoch 462/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 740925774539.7052 - val_loss: 773292518782.2357\n",
      "Epoch 463/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 742623286289.8638 - val_loss: 778196476095.2618\n",
      "Epoch 464/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 742582929416.0675 - val_loss: 808597094390.7826\n",
      "Epoch 465/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 746444245331.9888 - val_loss: 795630706883.8706\n",
      "Epoch 466/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 741430517653.9696 - val_loss: 772841932595.4880\n",
      "Epoch 467/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 742645887015.1852 - val_loss: 775070587520.0360\n",
      "Epoch 468/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 741903312834.9174 - val_loss: 791478543601.0936\n",
      "Epoch 469/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 749265028694.7260 - val_loss: 782794796861.2816\n",
      "Epoch 470/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 740399562871.8604 - val_loss: 778534861986.1693\n",
      "Epoch 471/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 740448955477.8615 - val_loss: 778417510809.6000\n",
      "Epoch 472/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 751386495657.1300 - val_loss: 868968479288.6008\n",
      "Epoch 473/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 751451585495.0861 - val_loss: 791049380312.3938\n",
      "Epoch 474/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 738678412109.9381 - val_loss: 772194402838.6116\n",
      "Epoch 475/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 743255374359.9146 - val_loss: 792606756026.3651\n",
      "Epoch 476/1000\n",
      "3554/3554 [==============================] - 1s 145us/step - loss: 742501227267.0253 - val_loss: 785982904715.7738\n",
      "Epoch 477/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 743835525069.2898 - val_loss: 777810330314.3516\n",
      "Epoch 478/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 741042561425.0714 - val_loss: 776140293450.9637\n",
      "Epoch 479/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 741465471253.7535 - val_loss: 774079948717.6191\n",
      "Epoch 480/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 737359442208.1261 - val_loss: 788992802064.7786\n",
      "Epoch 481/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 740045304501.8075 - val_loss: 801640698244.2847\n",
      "Epoch 482/1000\n",
      "3554/3554 [==============================] - 0s 131us/step - loss: 743344283634.1700 - val_loss: 769329383371.5758\n",
      "Epoch 483/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 741070464418.9353 - val_loss: 792676720757.5223\n",
      "Epoch 484/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 742320101584.6031 - val_loss: 769764639839.9189\n",
      "Epoch 485/1000\n",
      "3554/3554 [==============================] - 0s 137us/step - loss: 739306180384.4142 - val_loss: 798263310402.2504\n",
      "Epoch 486/1000\n",
      "3554/3554 [==============================] - 1s 142us/step - loss: 740909474110.0911 - val_loss: 777311962235.5713\n",
      "Epoch 487/1000\n",
      "3554/3554 [==============================] - 1s 162us/step - loss: 741798664308.4030 - val_loss: 818761930451.8571\n",
      "Epoch 488/1000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 742957175012.1959 - val_loss: 805397942697.1544\n",
      "Epoch 489/1000\n",
      "3554/3554 [==============================] - 1s 157us/step - loss: 744491318005.1953 - val_loss: 777991271219.2000\n",
      "Epoch 490/1000\n",
      "3554/3554 [==============================] - 1s 202us/step - loss: 744679790505.5621 - val_loss: 772402450529.9353\n",
      "Epoch 491/1000\n",
      "3554/3554 [==============================] - 1s 194us/step - loss: 738505377161.8683 - val_loss: 775782260567.7817\n",
      "Epoch 492/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 735675007722.8228 - val_loss: 787769842115.0785\n",
      "Epoch 493/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 741720242287.7928 - val_loss: 769369578029.0791\n",
      "Epoch 494/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 740087887043.3494 - val_loss: 772404409623.1156\n",
      "Epoch 495/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 742318122227.7546 - val_loss: 815024981161.6586\n",
      "Epoch 496/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 749290474291.4305 - val_loss: 801792597797.0858\n",
      "Epoch 497/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 738066871094.8881 - val_loss: 769240384552.6144\n",
      "Epoch 498/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 735616179057.6658 - val_loss: 773623986901.2974\n",
      "Epoch 499/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 738938589716.4570 - val_loss: 770748800828.1294\n",
      "Epoch 500/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 739182216292.8441 - val_loss: 787177411330.8085\n",
      "Epoch 501/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 743817621607.7253 - val_loss: 820463999007.6849\n",
      "Epoch 502/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 743160197988.9882 - val_loss: 815497072972.9800\n",
      "Epoch 503/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 742422479806.8835 - val_loss: 768524794415.3834\n",
      "Epoch 504/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 100us/step - loss: 738698673466.0574 - val_loss: 813278862182.4720\n",
      "Epoch 505/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 744358087042.0889 - val_loss: 774695099146.2976\n",
      "Epoch 506/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 738899150339.7456 - val_loss: 858551987393.2782\n",
      "Epoch 507/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 751695485311.7839 - val_loss: 778634955874.7994\n",
      "Epoch 508/1000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 737601096453.3303 - val_loss: 783394140338.5879\n",
      "Epoch 509/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 738579733297.1256 - val_loss: 770599457769.2444\n",
      "Epoch 510/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 736346247700.4570 - val_loss: 789157640166.0759\n",
      "Epoch 511/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 736975292064.4862 - val_loss: 774686108915.9741\n",
      "Epoch 512/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 735056962232.1124 - val_loss: 784261712810.4507\n",
      "Epoch 513/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 740713764842.1024 - val_loss: 773904654637.8712\n",
      "Epoch 514/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 738061640237.8120 - val_loss: 769674579955.3260\n",
      "Epoch 515/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 742376163757.8842 - val_loss: 766741400006.5350\n",
      "Epoch 516/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 736606376551.4373 - val_loss: 773303332996.5007\n",
      "Epoch 517/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 734241713846.3839 - val_loss: 771161067359.8469\n",
      "Epoch 518/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 739413070374.8971 - val_loss: 791878458984.1283\n",
      "Epoch 519/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 740278917254.8429 - val_loss: 771181859591.4171\n",
      "Epoch 520/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 734916200589.7581 - val_loss: 774432185222.7330\n",
      "Epoch 521/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 736002645605.7085 - val_loss: 773936689740.7640\n",
      "Epoch 522/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 738602193262.4963 - val_loss: 807326005964.0798\n",
      "Epoch 523/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 736718486814.3973 - val_loss: 770698197883.2113\n",
      "Epoch 524/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 735921737006.5323 - val_loss: 772471017795.7626\n",
      "Epoch 525/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 736042310873.2471 - val_loss: 768426206149.5269\n",
      "Epoch 526/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 736445294263.5363 - val_loss: 767921567510.6836\n",
      "Epoch 527/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 735849400991.9100 - val_loss: 774420830359.5117\n",
      "Epoch 528/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 739227022946.2510 - val_loss: 794888359955.5870\n",
      "Epoch 529/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 737003193343.4238 - val_loss: 775338610702.9784\n",
      "Epoch 530/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 741868984436.4030 - val_loss: 767080034307.4565\n",
      "Epoch 531/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 733179718363.8401 - val_loss: 794381423510.2875\n",
      "Epoch 532/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 738764351675.8582 - val_loss: 774079589043.8842\n",
      "Epoch 533/1000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 739698459368.72 - 0s 85us/step - loss: 737133453381.1503 - val_loss: 771572192293.1578\n",
      "Epoch 534/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 735224169757.2448 - val_loss: 768399554603.2068\n",
      "Epoch 535/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 736794693362.3140 - val_loss: 774066277428.7123\n",
      "Epoch 536/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 735323147077.8705 - val_loss: 777767515922.0748\n",
      "Epoch 537/1000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 736005765277.8932 - val_loss: 791697666904.0697\n",
      "Epoch 538/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 739411982595.3134 - val_loss: 776109674690.7184\n",
      "Epoch 539/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 732636199770.6156 - val_loss: 773598820385.9893\n",
      "Epoch 540/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 735794755308.5515 - val_loss: 766401670581.5404\n",
      "Epoch 541/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 734678688145.0714 - val_loss: 786755580008.8484\n",
      "Epoch 542/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 742358137616.8553 - val_loss: 797306473653.7563\n",
      "Epoch 543/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 735763389060.2499 - val_loss: 766596539020.1339\n",
      "Epoch 544/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 733949301222.3567 - val_loss: 775798961730.6824\n",
      "Epoch 545/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 735875846187.7952 - val_loss: 768311550908.3094\n",
      "Epoch 546/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 736688235072.2521 - val_loss: 826003312654.9784\n",
      "Epoch 547/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 742114257692.3805 - val_loss: 794034033323.2428\n",
      "Epoch 548/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 743038335262.9735 - val_loss: 768243723083.9719\n",
      "Epoch 549/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 733444121013.9517 - val_loss: 793354044055.6556\n",
      "Epoch 550/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 736391258632.9320 - val_loss: 772929298508.0438\n",
      "Epoch 551/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 735256128643.9618 - val_loss: 774981742512.4996\n",
      "Epoch 552/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 734714765473.3506 - val_loss: 771019395211.4138\n",
      "Epoch 553/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 730421804127.6580 - val_loss: 765913711260.2644\n",
      "Epoch 554/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 734844837698.9893 - val_loss: 772826887179.8098\n",
      "Epoch 555/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 732357322626.3771 - val_loss: 809129158993.0127\n",
      "Epoch 556/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 736730399983.1447 - val_loss: 778080783643.1482\n",
      "Epoch 557/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 738883147237.7805 - val_loss: 771723360829.2096\n",
      "Epoch 558/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 738180041551.0906 - val_loss: 794437200803.5376\n",
      "Epoch 559/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 735237143819.9573 - val_loss: 828139840098.6554\n",
      "Epoch 560/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 737735971514.4176 - val_loss: 768121034696.1193\n",
      "Epoch 561/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 732331856229.2762 - val_loss: 768173088512.2161\n",
      "Epoch 562/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 734322743246.4424 - val_loss: 767375945422.6722\n",
      "Epoch 563/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 731446991665.1256 - val_loss: 777613786578.3448\n",
      "Epoch 564/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 740887353460.6910 - val_loss: 774929771837.7136\n",
      "Epoch 565/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 734003811288.2386 - val_loss: 792488039958.8995\n",
      "Epoch 566/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 738325687916.6234 - val_loss: 783039491813.7159\n",
      "Epoch 567/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 733493428058.0394 - val_loss: 771883854492.8405\n",
      "Epoch 568/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 731781716840.4457 - val_loss: 780245842589.9927\n",
      "Epoch 569/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 733402715501.9202 - val_loss: 789981663597.2411\n",
      "Epoch 570/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 736795810148.1238 - val_loss: 770941744084.5052\n",
      "Epoch 571/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 732355798407.2751 - val_loss: 772776860905.6045\n",
      "Epoch 572/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 731535862095.3788 - val_loss: 787762923983.7524\n",
      "Epoch 573/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 739479411542.0056 - val_loss: 766089919663.9955\n",
      "Epoch 574/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 733747721346.8092 - val_loss: 795467481032.6953\n",
      "Epoch 575/1000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 736444867929.1750 - val_loss: 842145623689.5415\n",
      "Epoch 576/1000\n",
      "3554/3554 [==============================] - 0s 130us/step - loss: 739148668941.8300 - val_loss: 870560705673.9735\n",
      "Epoch 577/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 740341918704.4413 - val_loss: 770384728254.1097\n",
      "Epoch 578/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 732768414243.4396 - val_loss: 778069346394.4462\n",
      "Epoch 579/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 734255672553.3822 - val_loss: 852482115623.7502\n",
      "Epoch 580/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 741220011043.1515 - val_loss: 784200009611.0537\n",
      "Epoch 581/1000\n",
      "3554/3554 [==============================] - 1s 144us/step - loss: 731899518298.3275 - val_loss: 833274810005.0632\n",
      "Epoch 582/1000\n",
      "3554/3554 [==============================] - 1s 141us/step - loss: 735500535106.1249 - val_loss: 778769203706.9592\n",
      "Epoch 583/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 734910243226.2915 - val_loss: 769971251089.9668\n",
      "Epoch 584/1000\n",
      "3554/3554 [==============================] - 1s 163us/step - loss: 731991959205.0962 - val_loss: 768568335022.1232\n",
      "Epoch 585/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 730699711727.1447 - val_loss: 797789326033.8408\n",
      "Epoch 586/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 733068801062.6089 - val_loss: 840631740670.0557\n",
      "Epoch 587/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 742251151585.8909 - val_loss: 767165761091.8346\n",
      "Epoch 588/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 732443620445.9291 - val_loss: 792919615525.4458\n",
      "Epoch 589/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 732878668200.1215 - val_loss: 767330140984.9609\n",
      "Epoch 590/1000\n",
      "3554/3554 [==============================] - 1s 162us/step - loss: 734304836880.5673 - val_loss: 776752626624.3420\n",
      "Epoch 591/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 731535645369.2650 - val_loss: 775728556590.2312\n",
      "Epoch 592/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 733967960514.6293 - val_loss: 796611673254.7781\n",
      "Epoch 593/1000\n",
      "3554/3554 [==============================] - 1s 155us/step - loss: 737933622272.5762 - val_loss: 774767268523.2428\n",
      "Epoch 594/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 731351980323.5835 - val_loss: 773183379939.3395\n",
      "Epoch 595/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 729992390041.7152 - val_loss: 773910842047.4059\n",
      "Epoch 596/1000\n",
      "3554/3554 [==============================] - 0s 130us/step - loss: 734280866350.9646 - val_loss: 772833532410.3832\n",
      "Epoch 597/1000\n",
      "3554/3554 [==============================] - 0s 139us/step - loss: 731367406647.8965 - val_loss: 764411269001.3254\n",
      "Epoch 598/1000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 732680876347.7861 - val_loss: 783041359958.4135\n",
      "Epoch 599/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 735660090040.1124 - val_loss: 770912982408.0293\n",
      "Epoch 600/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 733424855495.2391 - val_loss: 837442381391.3564\n",
      "Epoch 601/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 736030930235.2100 - val_loss: 775406107045.1218\n",
      "Epoch 602/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 732026319495.7074 - val_loss: 790921105289.3254\n",
      "Epoch 603/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 731036852883.2324 - val_loss: 765102139696.7516\n",
      "Epoch 604/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 731368761153.8368 - val_loss: 772599168079.2124\n",
      "Epoch 605/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 734122310490.6156 - val_loss: 780904486137.1589\n",
      "Epoch 606/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 730547756129.3867 - val_loss: 769058161252.9597\n",
      "Epoch 607/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 731281612583.3291 - val_loss: 762966106524.4805\n",
      "Epoch 608/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 731649685606.5729 - val_loss: 768503341081.3479\n",
      "Epoch 609/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 735611707055.4688 - val_loss: 763256131575.9347\n",
      "Epoch 610/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 730265727688.2476 - val_loss: 797099355158.7556\n",
      "Epoch 611/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 731411107035.5520 - val_loss: 859272093633.7822\n",
      "Epoch 612/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 740671132401.7378 - val_loss: 767461842757.3468\n",
      "Epoch 613/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 736560996559.4508 - val_loss: 772703300321.9713\n",
      "Epoch 614/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 731092508358.5188 - val_loss: 770680375054.6183\n",
      "Epoch 615/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 731044877410.5391 - val_loss: 776398362936.8169\n",
      "Epoch 616/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 731511603402.8407 - val_loss: 788394672791.6556\n",
      "Epoch 617/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 731914474520.7788 - val_loss: 763458614127.1134\n",
      "Epoch 618/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 731455406605.5419 - val_loss: 775826916438.7015\n",
      "Epoch 619/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 730615510543.2706 - val_loss: 801020859005.1555\n",
      "Epoch 620/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 731996202375.2751 - val_loss: 776438247722.4147\n",
      "Epoch 621/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 734550906609.1615 - val_loss: 779961262317.3491\n",
      "Epoch 622/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 728965886627.3674 - val_loss: 762207941288.9384\n",
      "Epoch 623/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 727997482933.0872 - val_loss: 794183710196.3342\n",
      "Epoch 624/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 730641322765.3979 - val_loss: 782196620496.2566\n",
      "Epoch 625/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 729492970879.2076 - val_loss: 770373781622.0985\n",
      "Epoch 626/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 733194560184.1124 - val_loss: 760239310104.2678\n",
      "Epoch 627/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 730335241413.6545 - val_loss: 816180123214.2042\n",
      "Epoch 628/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 733924696022.5099 - val_loss: 763818659159.0616\n",
      "Epoch 629/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 728991627196.0022 - val_loss: 777015664020.1272\n",
      "Epoch 630/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 91us/step - loss: 733003091099.0118 - val_loss: 772527857832.2183\n",
      "Epoch 631/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 729140210063.9191 - val_loss: 764334273294.3302\n",
      "Epoch 632/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 727998470048.3420 - val_loss: 764075805366.1885\n",
      "Epoch 633/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 731411708734.9556 - val_loss: 793609476689.9489\n",
      "Epoch 634/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 736804242975.4058 - val_loss: 773321361277.2275\n",
      "Epoch 635/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 728236951643.0479 - val_loss: 777120380206.1592\n",
      "Epoch 636/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 735541109293.8120 - val_loss: 759481750168.8079\n",
      "Epoch 637/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 731484747205.5104 - val_loss: 768792637374.3257\n",
      "Epoch 638/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 731513785794.0529 - val_loss: 764828350779.1212\n",
      "Epoch 639/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 729647690421.8075 - val_loss: 761396353465.8611\n",
      "Epoch 640/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 730369000954.5256 - val_loss: 770958369885.6146\n",
      "Epoch 641/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 730982366356.0967 - val_loss: 778164420031.9100\n",
      "Epoch 642/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 729251814257.6658 - val_loss: 772405702240.0630\n",
      "Epoch 643/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 732290169215.7839 - val_loss: 764375394521.1859\n",
      "Epoch 644/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 728083694766.6044 - val_loss: 773902995977.9375\n",
      "Epoch 645/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 738069358641.5576 - val_loss: 767744203977.3435\n",
      "Epoch 646/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 729453810021.2762 - val_loss: 768000866153.6405\n",
      "Epoch 647/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 727512548120.9230 - val_loss: 764072583184.1305\n",
      "Epoch 648/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 728969421735.8334 - val_loss: 773531614692.7798\n",
      "Epoch 649/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 728767841500.7046 - val_loss: 759406766817.9713\n",
      "Epoch 650/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 721984235929.1390 - val_loss: 762976145625.7621\n",
      "Epoch 651/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 720142716437.0332 - val_loss: 755289233550.2942\n",
      "Epoch 652/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 721834983783.5813 - val_loss: 766054505115.6884\n",
      "Epoch 653/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 725773487567.8831 - val_loss: 761448165453.1960\n",
      "Epoch 654/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 722410940979.5745 - val_loss: 754828301593.4199\n",
      "Epoch 655/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 722439301178.7777 - val_loss: 763794841606.9131\n",
      "Epoch 656/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 721633510657.5846 - val_loss: 758296394886.5170\n",
      "Epoch 657/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 719174082288.0090 - val_loss: 754172133532.6965\n",
      "Epoch 658/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 724894684092.5785 - val_loss: 758268759866.9772\n",
      "Epoch 659/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 726487142056.5537 - val_loss: 762425661715.9471\n",
      "Epoch 660/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 722752013044.0427 - val_loss: 770619691165.8486\n",
      "Epoch 661/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 727503302307.9437 - val_loss: 757355148852.8562\n",
      "Epoch 662/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 725600277316.7181 - val_loss: 762002487541.9905\n",
      "Epoch 663/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 718199900407.7883 - val_loss: 759702520245.8284\n",
      "Epoch 664/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 719476603867.6962 - val_loss: 758018745474.4844\n",
      "Epoch 665/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 722459425119.5138 - val_loss: 752779300869.7609\n",
      "Epoch 666/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 719651188083.6826 - val_loss: 785091249837.8352\n",
      "Epoch 667/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 721928980017.8457 - val_loss: 785240656745.9286\n",
      "Epoch 668/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 718465679719.5813 - val_loss: 754567825894.7960\n",
      "Epoch 669/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 717608224527.1267 - val_loss: 754503919464.4883\n",
      "Epoch 670/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 716866411589.7266 - val_loss: 754479082228.4061\n",
      "Epoch 671/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 721325032807.0050 - val_loss: 822901720381.1376\n",
      "Epoch 672/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 728094042465.8187 - val_loss: 753223013768.0293\n",
      "Epoch 673/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 717288947647.4598 - val_loss: 784367753662.4697\n",
      "Epoch 674/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 715575478280.0675 - val_loss: 821973557711.1764\n",
      "Epoch 675/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 717793094483.7007 - val_loss: 813086365414.2920\n",
      "Epoch 676/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 722547965140.6371 - val_loss: 764354400598.1975\n",
      "Epoch 677/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 717627815125.7896 - val_loss: 778752639948.4399\n",
      "Epoch 678/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 720375077893.1863 - val_loss: 760512313456.3375\n",
      "Epoch 679/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 716690708596.4030 - val_loss: 749558289058.0253\n",
      "Epoch 680/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 715620644422.5908 - val_loss: 773252328023.7097\n",
      "Epoch 681/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 716450646655.6399 - val_loss: 751084343928.5469\n",
      "Epoch 682/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 714006263269.7805 - val_loss: 749769801479.1292\n",
      "Epoch 683/1000\n",
      "3554/3554 [==============================] - 0s 130us/step - loss: 715412258605.0917 - val_loss: 766066187262.5598\n",
      "Epoch 684/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 714578518315.6511 - val_loss: 752357002592.2791\n",
      "Epoch 685/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 713563437997.5959 - val_loss: 784370472964.0326\n",
      "Epoch 686/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 713964667903.4238 - val_loss: 759088071987.9202\n",
      "Epoch 687/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 714893302936.7068 - val_loss: 756142912307.2000\n",
      "Epoch 688/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 713650645207.5183 - val_loss: 760063150073.6630\n",
      "Epoch 689/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 711232216599.3381 - val_loss: 749413914167.1606\n",
      "Epoch 690/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 711950047302.3027 - val_loss: 747159970319.6985\n",
      "Epoch 691/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 711332490639.3427 - val_loss: 763147749148.4445\n",
      "Epoch 692/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 715550098809.4452 - val_loss: 862240741020.5525\n",
      "Epoch 693/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 718602068415.1716 - val_loss: 763080100059.4902\n",
      "Epoch 694/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 708338071341.0917 - val_loss: 755001575201.0532\n",
      "Epoch 695/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 707131584634.1655 - val_loss: 745502516400.5715\n",
      "Epoch 696/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 708221084159.7119 - val_loss: 747592053006.7623\n",
      "Epoch 697/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 707498222596.0337 - val_loss: 788714980987.1393\n",
      "Epoch 698/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 712065705323.6150 - val_loss: 743653944656.1486\n",
      "Epoch 699/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 705981373684.9072 - val_loss: 776071056934.4540\n",
      "Epoch 700/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 709684643479.2661 - val_loss: 756044383793.3997\n",
      "Epoch 701/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 712781464990.3252 - val_loss: 767243200752.2295\n",
      "Epoch 702/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 706188691390.3073 - val_loss: 761853419144.3893\n",
      "Epoch 703/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 709752591476.9792 - val_loss: 738527094282.2256\n",
      "Epoch 704/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 703418089628.7406 - val_loss: 751603455874.9885\n",
      "Epoch 705/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 704290919375.0186 - val_loss: 741627257415.0031\n",
      "Epoch 706/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 704039948970.2825 - val_loss: 739659978244.4647\n",
      "Epoch 707/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 701733807227.3179 - val_loss: 736906516751.3384\n",
      "Epoch 708/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 700417390417.9719 - val_loss: 790933207917.3851\n",
      "Epoch 709/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 701624271594.8228 - val_loss: 745648134447.5995\n",
      "Epoch 710/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 697159465099.4530 - val_loss: 736982217541.6349\n",
      "Epoch 711/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 695159992866.2870 - val_loss: 737325039749.6528\n",
      "Epoch 712/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 693213034105.3010 - val_loss: 730298447629.4661\n",
      "Epoch 713/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 691301168812.0112 - val_loss: 768287373926.6880\n",
      "Epoch 714/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 693000523865.8954 - val_loss: 725206087571.1190\n",
      "Epoch 715/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 683652469314.5571 - val_loss: 733786551109.6349\n",
      "Epoch 716/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 683046324102.9871 - val_loss: 714701154650.2301\n",
      "Epoch 717/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 678277194663.8334 - val_loss: 791265975778.4753\n",
      "Epoch 718/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 675297519711.6580 - val_loss: 711290050292.1182\n",
      "Epoch 719/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 672984041655.2482 - val_loss: 700645548131.0875\n",
      "Epoch 720/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 671402280551.4373 - val_loss: 700658941159.3002\n",
      "Epoch 721/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 664164624774.6990 - val_loss: 700671650391.7097\n",
      "Epoch 722/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 664153985740.8575 - val_loss: 691404554816.0900\n",
      "Epoch 723/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 655283535789.0197 - val_loss: 695375355795.6951\n",
      "Epoch 724/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 651483101920.4502 - val_loss: 701851768382.0737\n",
      "Epoch 725/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 644777595984.0990 - val_loss: 696627641824.1710\n",
      "Epoch 726/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 641750078912.9004 - val_loss: 679971108612.2487\n",
      "Epoch 727/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 640839168675.3674 - val_loss: 670682063361.8723\n",
      "Epoch 728/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 629548232088.5627 - val_loss: 685527722786.4934\n",
      "Epoch 729/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 630258510961.5216 - val_loss: 736594559446.0895\n",
      "Epoch 730/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 627162951673.0850 - val_loss: 738590600452.1046\n",
      "Epoch 731/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 623183837102.1721 - val_loss: 731470072734.9288\n",
      "Epoch 732/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 623118410709.9336 - val_loss: 653019827842.6284\n",
      "Epoch 733/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 609708243822.2083 - val_loss: 641432958500.7257\n",
      "Epoch 734/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 603006465555.8807 - val_loss: 653907464045.6731\n",
      "Epoch 735/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 599733168850.6201 - val_loss: 630167271780.5997\n",
      "Epoch 736/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 592840493617.2695 - val_loss: 626353402513.8948\n",
      "Epoch 737/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 586831281779.5386 - val_loss: 624487135113.9015\n",
      "Epoch 738/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 583689614116.4480 - val_loss: 614064644743.5251\n",
      "Epoch 739/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 579111104136.2836 - val_loss: 713429655671.8268\n",
      "Epoch 740/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 579550510744.4187 - val_loss: 637046329635.5016\n",
      "Epoch 741/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 568648109358.5323 - val_loss: 605702867453.8397\n",
      "Epoch 742/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 564158810525.7490 - val_loss: 605562837816.3848\n",
      "Epoch 743/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 558646780074.5707 - val_loss: 606735599194.8782\n",
      "Epoch 744/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 557842733167.7928 - val_loss: 588490831952.0765\n",
      "Epoch 745/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 547825325394.2600 - val_loss: 626629140745.0015\n",
      "Epoch 746/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 548893397644.3174 - val_loss: 598667510739.3530\n",
      "Epoch 747/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 543766054357.6454 - val_loss: 592810115758.6993\n",
      "Epoch 748/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 537841565105.9178 - val_loss: 584370807528.3083\n",
      "Epoch 749/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 536045375771.5161 - val_loss: 595978769371.4183\n",
      "Epoch 750/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 533331595270.9150 - val_loss: 567032899160.5739\n",
      "Epoch 751/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 527427682313.2200 - val_loss: 565237523351.4397\n",
      "Epoch 752/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 524251418566.3748 - val_loss: 558154107864.5378\n",
      "Epoch 753/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 521308193514.8228 - val_loss: 556491636851.2180\n",
      "Epoch 754/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 512639446954.7147 - val_loss: 548594398611.5510\n",
      "Epoch 755/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 510665923884.2274 - val_loss: 563105862287.5905\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 77us/step - loss: 506403933320.5718 - val_loss: 569466587691.9269\n",
      "Epoch 757/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 503948218125.9741 - val_loss: 599222755279.6084\n",
      "Epoch 758/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 502358895683.4214 - val_loss: 538828273748.6852\n",
      "Epoch 759/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 499731875368.6258 - val_loss: 551164175372.9620\n",
      "Epoch 760/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 495366680919.4463 - val_loss: 544005162794.2706\n",
      "Epoch 761/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 494822020673.9809 - val_loss: 534480936266.3876\n",
      "Epoch 762/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 489290652741.7265 - val_loss: 554647230726.1210\n",
      "Epoch 763/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 491134164120.7068 - val_loss: 521362083804.2824\n",
      "Epoch 764/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 480794844810.5886 - val_loss: 517593516371.3170\n",
      "Epoch 765/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 481332911856.0090 - val_loss: 572502521903.8154\n",
      "Epoch 766/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 480971582683.5520 - val_loss: 568030542459.4272\n",
      "Epoch 767/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 476737727057.5397 - val_loss: 510558188675.6366\n",
      "Epoch 768/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 475027295119.6309 - val_loss: 512263882953.0554\n",
      "Epoch 769/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 471908446228.7451 - val_loss: 581155521525.9185\n",
      "Epoch 770/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 470896296521.4722 - val_loss: 513319942887.7322\n",
      "Epoch 771/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 468118608292.6640 - val_loss: 524136992771.4565\n",
      "Epoch 772/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 467707989958.3748 - val_loss: 519603786272.9811\n",
      "Epoch 773/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 467131959682.0889 - val_loss: 506524077233.1477\n",
      "Epoch 774/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 462894141603.6556 - val_loss: 499350145833.4065\n",
      "Epoch 775/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 459858993082.8497 - val_loss: 541300340615.5972\n",
      "Epoch 776/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 461989163097.8953 - val_loss: 511755050074.7342\n",
      "Epoch 777/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 456830443450.8497 - val_loss: 493996473602.9525\n",
      "Epoch 778/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 455671797461.5014 - val_loss: 498452271930.1131\n",
      "Epoch 779/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 455587705157.0062 - val_loss: 495274353098.5676\n",
      "Epoch 780/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 451703477245.1187 - val_loss: 489631047986.4799\n",
      "Epoch 781/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 449447179837.3708 - val_loss: 541210082696.6053\n",
      "Epoch 782/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 455579167470.2802 - val_loss: 509833325558.4945\n",
      "Epoch 783/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 449624584719.8469 - val_loss: 505105638394.5272\n",
      "Epoch 784/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 449202252749.8661 - val_loss: 500832214516.9103\n",
      "Epoch 785/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 447556911130.5076 - val_loss: 506117474657.1432\n",
      "Epoch 786/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 445839882937.2650 - val_loss: 496357722621.8397\n",
      "Epoch 787/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 444429847388.9207 - val_loss: 488615514145.4132\n",
      "Epoch 788/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 443942735298.0529 - val_loss: 500232096984.3218\n",
      "Epoch 789/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 445318005160.1216 - val_loss: 481205624064.6481\n",
      "Epoch 790/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 440262361143.3203 - val_loss: 493039958213.5989\n",
      "Epoch 791/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 439466777983.2077 - val_loss: 492851642782.7848\n",
      "Epoch 792/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 435303956507.6601 - val_loss: 476062105475.5646\n",
      "Epoch 793/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 443951563544.9229 - val_loss: 506150922558.5778\n",
      "Epoch 794/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 438436705281.7288 - val_loss: 499526654144.9902\n",
      "Epoch 795/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 437561232409.9313 - val_loss: 470498888146.0568\n",
      "Epoch 796/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 433009460306.4041 - val_loss: 469502011143.7052\n",
      "Epoch 797/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 430454455351.8964 - val_loss: 495703365995.5128\n",
      "Epoch 798/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 431140731093.2133 - val_loss: 464383000396.2599\n",
      "Epoch 799/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 428621733214.9376 - val_loss: 464371300556.5120\n",
      "Epoch 800/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 430708924983.0320 - val_loss: 465021460085.9544\n",
      "Epoch 801/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 431053183627.1649 - val_loss: 465079678037.5494\n",
      "Epoch 802/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 426521714286.9285 - val_loss: 464144790127.3294\n",
      "Epoch 803/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 426445831159.3562 - val_loss: 476186445262.3123\n",
      "Epoch 804/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 423985649960.1936 - val_loss: 464119474642.0568\n",
      "Epoch 805/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 423499946579.8447 - val_loss: 467229080058.3831\n",
      "Epoch 806/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 423297841208.4727 - val_loss: 520623248338.2009\n",
      "Epoch 807/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 421901098828.2094 - val_loss: 470072191856.8416\n",
      "Epoch 808/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 419036865443.2234 - val_loss: 467239529158.3190\n",
      "Epoch 809/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 420292426485.1953 - val_loss: 458750717148.9305\n",
      "Epoch 810/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 419955535542.3838 - val_loss: 539766297282.2864\n",
      "Epoch 811/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 419886082809.8052 - val_loss: 451355943771.5263\n",
      "Epoch 812/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 416316905247.2617 - val_loss: 454314089175.6017\n",
      "Epoch 813/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 416351005440.7203 - val_loss: 462268579462.0850\n",
      "Epoch 814/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 416717455157.1592 - val_loss: 461110607938.8264\n",
      "Epoch 815/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 414194695445.1772 - val_loss: 449421132144.6976\n",
      "Epoch 816/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 410553716171.2729 - val_loss: 448852013912.0698\n",
      "Epoch 817/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 410390444226.7732 - val_loss: 473320281074.4619\n",
      "Epoch 818/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 413250198422.5458 - val_loss: 485931491208.1733\n",
      "Epoch 819/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 414404409384.3376 - val_loss: 443473441674.7657\n",
      "Epoch 820/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 406055765507.7457 - val_loss: 465488072723.5870\n",
      "Epoch 821/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 411031547097.8233 - val_loss: 440110073960.2723\n",
      "Epoch 822/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 405093915929.7872 - val_loss: 438402842773.7834\n",
      "Epoch 823/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 404297588162.0529 - val_loss: 450470233635.5735\n",
      "Epoch 824/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 402141024179.9347 - val_loss: 474320842170.1491\n",
      "Epoch 825/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 404121077030.4648 - val_loss: 443321069405.5426\n",
      "Epoch 826/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 405263677551.7929 - val_loss: 430143158877.4706\n",
      "Epoch 827/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 399323646402.6292 - val_loss: 434499246964.2982\n",
      "Epoch 828/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 400639254643.2504 - val_loss: 436750438977.2422\n",
      "Epoch 829/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 398322576995.4035 - val_loss: 434175623555.1325\n",
      "Epoch 830/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 397552841775.8290 - val_loss: 467528537291.0717\n",
      "Epoch 831/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 396848358431.6939 - val_loss: 516388738518.3775\n",
      "Epoch 832/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 398832158096.4952 - val_loss: 424466373248.0360\n",
      "Epoch 833/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 392298063995.3180 - val_loss: 423164650723.2675\n",
      "Epoch 834/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 392128691006.9556 - val_loss: 447569467073.4222\n",
      "Epoch 835/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 392104892513.9629 - val_loss: 479936130960.2385\n",
      "Epoch 836/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 391690838352.5312 - val_loss: 445767881207.5027\n",
      "Epoch 837/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 391438914723.0793 - val_loss: 430123651391.1539\n",
      "Epoch 838/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 388884439127.5903 - val_loss: 420978564177.5167\n",
      "Epoch 839/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 387980229978.3275 - val_loss: 458761533340.3364\n",
      "Epoch 840/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 388867879882.4085 - val_loss: 418522254628.3657\n",
      "Epoch 841/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 389797982717.4069 - val_loss: 418272576170.0906\n",
      "Epoch 842/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 387091967246.2622 - val_loss: 427502381902.2762\n",
      "Epoch 843/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 386134503315.6646 - val_loss: 421509710487.6557\n",
      "Epoch 844/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 383595933716.7451 - val_loss: 495196669065.3975\n",
      "Epoch 845/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 386733376151.8425 - val_loss: 419844465017.9150\n",
      "Epoch 846/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 383305110646.7079 - val_loss: 413721857811.8031\n",
      "Epoch 847/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 380211408243.6826 - val_loss: 450678413210.0320\n",
      "Epoch 848/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 389068148314.1834 - val_loss: 423189102246.3460\n",
      "Epoch 849/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 381237875008.3962 - val_loss: 476602106600.3083\n",
      "Epoch 850/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 383469005651.7006 - val_loss: 422280605784.7178\n",
      "Epoch 851/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 382307110882.0349 - val_loss: 411593393261.7451\n",
      "Epoch 852/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 381568079596.5515 - val_loss: 428586692272.7156\n",
      "Epoch 853/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 380027828381.8931 - val_loss: 456842458202.1581\n",
      "Epoch 854/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 379289787968.8284 - val_loss: 414919698179.0965\n",
      "Epoch 855/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 376034794372.1058 - val_loss: 434424286832.1935\n",
      "Epoch 856/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 381435583308.7856 - val_loss: 444369626625.0082\n",
      "Epoch 857/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 377639603189.0512 - val_loss: 417743484975.8155\n",
      "Epoch 858/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 378035624402.7642 - val_loss: 452550390728.9834\n",
      "Epoch 859/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 377983220939.4170 - val_loss: 447825952966.1750\n",
      "Epoch 860/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 377469656539.4080 - val_loss: 415961345292.1699\n",
      "Epoch 861/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 376883554797.2718 - val_loss: 406312393514.5587\n",
      "Epoch 862/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 374009983328.6663 - val_loss: 409205956541.1736\n",
      "Epoch 863/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 370671009284.8981 - val_loss: 404833832177.6697\n",
      "Epoch 864/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 375002265933.0737 - val_loss: 542805820894.4427\n",
      "Epoch 865/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 383825406822.1407 - val_loss: 422661087922.1558\n",
      "Epoch 866/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 374274486606.2263 - val_loss: 435107307256.1508\n",
      "Epoch 867/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 372589740354.7012 - val_loss: 404277444073.3885\n",
      "Epoch 868/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 370219177399.6804 - val_loss: 438927864488.9384\n",
      "Epoch 869/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 372120964020.5110 - val_loss: 403002896719.2844\n",
      "Epoch 870/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 370057636359.7794 - val_loss: 416553200211.1010\n",
      "Epoch 871/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 374360741718.5819 - val_loss: 405724865611.1797\n",
      "Epoch 872/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 368041538421.1232 - val_loss: 419257391790.1232\n",
      "Epoch 873/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 369077352320.0721 - val_loss: 398766084779.2427\n",
      "Epoch 874/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 369518547508.7271 - val_loss: 404070380758.5935\n",
      "Epoch 875/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 365730445894.0146 - val_loss: 405250812217.9691\n",
      "Epoch 876/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 367030311111.3832 - val_loss: 400556442507.9178\n",
      "Epoch 877/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 364969292644.9882 - val_loss: 412801249076.0641\n",
      "Epoch 878/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 365707054553.9674 - val_loss: 398814066150.5080\n",
      "Epoch 879/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 365729367389.7850 - val_loss: 391790756697.2219\n",
      "Epoch 880/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 365838346934.9601 - val_loss: 410642060080.8956\n",
      "Epoch 881/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 364804568996.9522 - val_loss: 396068017327.4194\n",
      "Epoch 882/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 97us/step - loss: 364879373979.8762 - val_loss: 412164510700.7010\n",
      "Epoch 883/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 363795170543.1446 - val_loss: 395210598377.5325\n",
      "Epoch 884/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 364582891503.8649 - val_loss: 402813337563.1302\n",
      "Epoch 885/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 361690563666.4041 - val_loss: 396601418684.3094\n",
      "Epoch 886/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 362455784702.1272 - val_loss: 432019112229.8059\n",
      "Epoch 887/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 366521140979.4666 - val_loss: 400675950570.1086\n",
      "Epoch 888/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 362483172397.5239 - val_loss: 413798533307.2292\n",
      "Epoch 889/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 360707048945.3055 - val_loss: 398303507617.5933\n",
      "Epoch 890/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 364300780229.9426 - val_loss: 388672609485.9522\n",
      "Epoch 891/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 359355836465.5577 - val_loss: 405187826849.5933\n",
      "Epoch 892/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 361779527449.4991 - val_loss: 402054357473.6113\n",
      "Epoch 893/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 358296658800.5132 - val_loss: 394662254040.6819\n",
      "Epoch 894/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 360186427826.4941 - val_loss: 392385046547.0110\n",
      "Epoch 895/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 362593139643.4260 - val_loss: 388593838956.2329\n",
      "Epoch 896/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 359832507721.0399 - val_loss: 392637822291.0290\n",
      "Epoch 897/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 359828930126.6584 - val_loss: 397748645015.7997\n",
      "Epoch 898/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 357142060581.1682 - val_loss: 386839690233.9510\n",
      "Epoch 899/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 355349381645.5419 - val_loss: 441887743794.9119\n",
      "Epoch 900/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 362247547381.3394 - val_loss: 389733024568.9609\n",
      "Epoch 901/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 356159376824.8329 - val_loss: 420513346696.2453\n",
      "Epoch 902/1000\n",
      "3554/3554 [==============================] - 0s 138us/step - loss: 357286243155.7006 - val_loss: 408128583488.1620\n",
      "Epoch 903/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 357290754721.0624 - val_loss: 398485220577.5392\n",
      "Epoch 904/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 356963834828.7136 - val_loss: 393601175428.7167\n",
      "Epoch 905/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 357983831740.1463 - val_loss: 390201424378.3831\n",
      "Epoch 906/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 355370798050.6111 - val_loss: 404486113400.9789\n",
      "Epoch 907/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 357805005110.0236 - val_loss: 382821577477.1128\n",
      "Epoch 908/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 355128057233.6477 - val_loss: 386913319499.0357\n",
      "Epoch 909/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 356110465688.9949 - val_loss: 421204074364.6515\n",
      "Epoch 910/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 358915091071.6398 - val_loss: 394298368495.4374\n",
      "Epoch 911/1000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 354896290063.9910 - val_loss: 391242602200.4658\n",
      "Epoch 912/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 352080076320.5582 - val_loss: 382138070776.7269\n",
      "Epoch 913/1000\n",
      "3554/3554 [==============================] - 0s 133us/step - loss: 354308828568.5628 - val_loss: 396621359420.2734\n",
      "Epoch 914/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 356058824538.0394 - val_loss: 398237992870.7061\n",
      "Epoch 915/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 354148140427.8852 - val_loss: 424996041965.6371\n",
      "Epoch 916/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 355098561040.9995 - val_loss: 382975527313.5347\n",
      "Epoch 917/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 351532692216.6528 - val_loss: 383486793056.8552\n",
      "Epoch 918/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 350371003447.8964 - val_loss: 444642434267.7783\n",
      "Epoch 919/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 357000737378.8273 - val_loss: 379844539415.3317\n",
      "Epoch 920/1000\n",
      "3554/3554 [==============================] - 1s 148us/step - loss: 355787920879.0005 - val_loss: 424072671488.9362\n",
      "Epoch 921/1000\n",
      "3554/3554 [==============================] - 1s 192us/step - loss: 354866178880.6843 - val_loss: 388645789117.6057\n",
      "Epoch 922/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 351700328369.6297 - val_loss: 383004949878.1705\n",
      "Epoch 923/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 349588005320.3917 - val_loss: 384336059588.7347\n",
      "Epoch 924/1000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 352366554843.2639 - val_loss: 452913292734.1817\n",
      "Epoch 925/1000\n",
      "3554/3554 [==============================] - 1s 145us/step - loss: 357371426208.6302 - val_loss: 386477486313.8925\n",
      "Epoch 926/1000\n",
      "3554/3554 [==============================] - 1s 148us/step - loss: 349846884566.3658 - val_loss: 379829362488.6729\n",
      "Epoch 927/1000\n",
      "3554/3554 [==============================] - 1s 158us/step - loss: 348600358890.1024 - val_loss: 406037039038.3257\n",
      "Epoch 928/1000\n",
      "3554/3554 [==============================] - 0s 138us/step - loss: 350910537308.4885 - val_loss: 382605234513.5887\n",
      "Epoch 929/1000\n",
      "3554/3554 [==============================] - 1s 141us/step - loss: 349343871891.0884 - val_loss: 383966132059.8143\n",
      "Epoch 930/1000\n",
      "3554/3554 [==============================] - 1s 154us/step - loss: 347967832981.3934 - val_loss: 385167851589.1308\n",
      "Epoch 931/1000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 348578420953.8233 - val_loss: 382480045918.1187\n",
      "Epoch 932/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 351506041098.2285 - val_loss: 387926622846.3077\n",
      "Epoch 933/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 348558430254.1002 - val_loss: 404288707312.9496\n",
      "Epoch 934/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 349792315957.8796 - val_loss: 400071121879.3857\n",
      "Epoch 935/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 349658993697.9989 - val_loss: 384329013079.2056\n",
      "Epoch 936/1000\n",
      "3554/3554 [==============================] - 1s 147us/step - loss: 348887271579.5881 - val_loss: 377121293866.4866\n",
      "Epoch 937/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 348894905288.1036 - val_loss: 397007597918.2628\n",
      "Epoch 938/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 353349234321.5037 - val_loss: 385019581562.4191\n",
      "Epoch 939/1000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 349397792381.9111 - val_loss: 378525599378.1829\n",
      "Epoch 940/1000\n",
      "3554/3554 [==============================] - 0s 131us/step - loss: 346585402541.4519 - val_loss: 377440492745.3434\n",
      "Epoch 941/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 349107868687.5588 - val_loss: 396656163852.3859\n",
      "Epoch 942/1000\n",
      "3554/3554 [==============================] - 1s 143us/step - loss: 351946515773.5149 - val_loss: 415325780668.5255\n",
      "Epoch 943/1000\n",
      "3554/3554 [==============================] - 1s 144us/step - loss: 350793521674.0844 - val_loss: 378316764399.9415\n",
      "Epoch 944/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 347504201725.6950 - val_loss: 378957791899.1122\n",
      "Epoch 945/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 346283678342.5548 - val_loss: 378411016800.3511\n",
      "Epoch 946/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 347195969718.0957 - val_loss: 373580906194.1288\n",
      "Epoch 947/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 347002570375.1311 - val_loss: 375706822647.3586\n",
      "Epoch 948/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 346362912007.3472 - val_loss: 459075158882.4394\n",
      "Epoch 949/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 350853834100.8351 - val_loss: 418459588206.4653\n",
      "Epoch 950/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 348406126265.8413 - val_loss: 395208276932.9507\n",
      "Epoch 951/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 347102921349.4023 - val_loss: 374944225159.5972\n",
      "Epoch 952/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 345771493070.5864 - val_loss: 388032370035.2900\n",
      "Epoch 953/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 345537149205.7535 - val_loss: 384821380000.6571\n",
      "Epoch 954/1000\n",
      "3554/3554 [==============================] - 1s 155us/step - loss: 343651493107.7546 - val_loss: 388388202474.6847\n",
      "Epoch 955/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 350392063330.3950 - val_loss: 377789525514.8017\n",
      "Epoch 956/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 346700653691.8942 - val_loss: 379277658529.6653\n",
      "Epoch 957/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 343613995198.1631 - val_loss: 374779990081.6743\n",
      "Epoch 958/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 342281639572.9612 - val_loss: 406773239154.7139\n",
      "Epoch 959/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 350396111554.4851 - val_loss: 480030767362.6644\n",
      "Epoch 960/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 352306959218.8182 - val_loss: 404378647990.6926\n",
      "Epoch 961/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 345189035939.7996 - val_loss: 373056844042.1536\n",
      "Epoch 962/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 343760457163.2729 - val_loss: 370706072046.2852\n",
      "Epoch 963/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 344798277210.1834 - val_loss: 387054023228.6335\n",
      "Epoch 964/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 343032121112.3467 - val_loss: 373239329112.5018\n",
      "Epoch 965/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 344989770740.4750 - val_loss: 381288286245.4459\n",
      "Epoch 966/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 342638265573.9246 - val_loss: 441656553092.9328\n",
      "Epoch 967/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 346684838189.9561 - val_loss: 379032545073.7598\n",
      "Epoch 968/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 341962096695.3203 - val_loss: 377516757578.4597\n",
      "Epoch 969/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 344193973654.2578 - val_loss: 374225689644.6470\n",
      "Epoch 970/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 341923698110.5954 - val_loss: 393667887674.3291\n",
      "Epoch 971/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 342256514077.3889 - val_loss: 371171296277.0273\n",
      "Epoch 972/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 341473563764.9792 - val_loss: 385214516470.2785\n",
      "Epoch 973/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 346395311160.4727 - val_loss: 381788229791.2889\n",
      "Epoch 974/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 341027894716.2903 - val_loss: 389745363882.7386\n",
      "Epoch 975/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 342712246590.0912 - val_loss: 376596850028.3770\n",
      "Epoch 976/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 342226494163.7726 - val_loss: 371135213663.6309\n",
      "Epoch 977/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 342243065985.0805 - val_loss: 400429574651.5353\n",
      "Epoch 978/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 340602630920.7878 - val_loss: 373729313093.4908\n",
      "Epoch 979/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 340586823527.8694 - val_loss: 367214237610.7386\n",
      "Epoch 980/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 339676036474.5977 - val_loss: 371666525109.6844\n",
      "Epoch 981/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 340735590621.2808 - val_loss: 371407403393.4042\n",
      "Epoch 982/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 340644828627.3405 - val_loss: 393352385870.4202\n",
      "Epoch 983/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 340968916090.7417 - val_loss: 365670646233.5460\n",
      "Epoch 984/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 339124629042.9983 - val_loss: 381280924740.8428\n",
      "Epoch 985/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 338849491500.6595 - val_loss: 387874206800.0765\n",
      "Epoch 986/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 336844647768.0225 - val_loss: 379786516745.5775\n",
      "Epoch 987/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 340351800178.8182 - val_loss: 387087136747.2607\n",
      "Epoch 988/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 336754334623.7659 - val_loss: 366194750296.0698\n",
      "Epoch 989/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 334316990180.4839 - val_loss: 367147143730.2639\n",
      "Epoch 990/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 337774483724.5334 - val_loss: 369492531583.0999\n",
      "Epoch 991/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 335235210641.6477 - val_loss: 364942576731.3102\n",
      "Epoch 992/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 339047899223.5903 - val_loss: 364810912403.0470\n",
      "Epoch 993/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 336976596740.7541 - val_loss: 405252398120.0383\n",
      "Epoch 994/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 334572406530.4491 - val_loss: 377939624442.3831\n",
      "Epoch 995/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 333882486422.1137 - val_loss: 363710933626.2751\n",
      "Epoch 996/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 333465189090.7552 - val_loss: 371626370535.6602\n",
      "Epoch 997/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 336575048974.2622 - val_loss: 359443313137.7418\n",
      "Epoch 998/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 334249671394.7552 - val_loss: 364424201972.1182\n",
      "Epoch 999/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 335387037686.7800 - val_loss: 360762105642.8467\n",
      "Epoch 1000/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 329171464470.9060 - val_loss: 366772577820.0844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09f08eff60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model11.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "print(\"model11 loss:\",model11.loss)\n",
    "\n",
    "model_train11 = model11.fit(predictors,target, epochs=1000, validation_split=0.5, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model22 loss: mean_squared_error\n",
      "Train on 3554 samples, validate on 3555 samples\n",
      "Epoch 1/1000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 133453528067674.1875 - val_loss: 132174084957752.3125\n",
      "Epoch 2/1000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 129709926173915.5469 - val_loss: 117877849025853.7188\n",
      "Epoch 3/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 79717210867830.7031 - val_loss: 27363463878968.8164\n",
      "Epoch 4/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 11399166340836.4844 - val_loss: 7633188115318.0264\n",
      "Epoch 5/1000\n",
      "3554/3554 [==============================] - 1s 145us/step - loss: 7320471345203.8623 - val_loss: 6694279874807.1426\n",
      "Epoch 6/1000\n",
      "3554/3554 [==============================] - 0s 136us/step - loss: 6526665711565.2900 - val_loss: 6131061439631.7344\n",
      "Epoch 7/1000\n",
      "3554/3554 [==============================] - 1s 169us/step - loss: 5926249300728.6523 - val_loss: 5660149628821.9990\n",
      "Epoch 8/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 5491043050592.2344 - val_loss: 5336898414104.0518\n",
      "Epoch 9/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 5129661908234.8047 - val_loss: 5038690293058.8984\n",
      "Epoch 10/1000\n",
      "3554/3554 [==============================] - 1s 159us/step - loss: 4831562492110.2979 - val_loss: 4842817674303.0820\n",
      "Epoch 11/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 4609374338896.8193 - val_loss: 4630857307726.2041\n",
      "Epoch 12/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 4410035534544.3154 - val_loss: 4474286502628.8516\n",
      "Epoch 13/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 4248396418341.3120 - val_loss: 4341025335503.9683\n",
      "Epoch 14/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 4115157848633.3369 - val_loss: 4224997952030.1006\n",
      "Epoch 15/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 3996504877409.8188 - val_loss: 4151608436234.2256\n",
      "Epoch 16/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 3886389587970.3047 - val_loss: 4046929886190.4292\n",
      "Epoch 17/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 3815139144106.4263 - val_loss: 3965694398225.4985\n",
      "Epoch 18/1000\n",
      "3554/3554 [==============================] - 1s 162us/step - loss: 3720666397401.5356 - val_loss: 3885827863393.2871\n",
      "Epoch 19/1000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 3637888061677.4155 - val_loss: 3857611676062.2085\n",
      "Epoch 20/1000\n",
      "3554/3554 [==============================] - 1s 142us/step - loss: 3574838411365.4199 - val_loss: 3751189346195.1191\n",
      "Epoch 21/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 3502780170101.6997 - val_loss: 3708939334907.4634\n",
      "Epoch 22/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 3433755839513.9312 - val_loss: 3673080200478.6050\n",
      "Epoch 23/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 3379711863082.4985 - val_loss: 3571640150790.8413\n",
      "Epoch 24/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3312638100414.8838 - val_loss: 3529670765023.8828\n",
      "Epoch 25/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 3259562272264.3555 - val_loss: 3472475076807.9033\n",
      "Epoch 26/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 3202419578019.0791 - val_loss: 3435989107333.2207\n",
      "Epoch 27/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 3144160504088.0586 - val_loss: 3342830057708.7729\n",
      "Epoch 28/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 3088030473811.2681 - val_loss: 3290952201734.7690\n",
      "Epoch 29/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 3043596857807.3066 - val_loss: 3234845115921.1387\n",
      "Epoch 30/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 2989978207212.9839 - val_loss: 3188916416202.0635\n",
      "Epoch 31/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 2941613745395.7549 - val_loss: 3131213985567.9009\n",
      "Epoch 32/1000\n",
      "3554/3554 [==============================] - 1s 145us/step - loss: 2888004584281.4629 - val_loss: 3111745408794.7163\n",
      "Epoch 33/1000\n",
      "3554/3554 [==============================] - 1s 150us/step - loss: 2840919013063.6714 - val_loss: 3062403328438.4043\n",
      "Epoch 34/1000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 2796538373750.9961 - val_loss: 2987382929022.5957\n",
      "Epoch 35/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 2745635907412.2769 - val_loss: 2926767086714.9951\n",
      "Epoch 36/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2698929177249.0625 - val_loss: 2878106936441.2671\n",
      "Epoch 37/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2650874638117.0244 - val_loss: 2831354202064.1846\n",
      "Epoch 38/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2617410101888.7925 - val_loss: 2783512823548.1836\n",
      "Epoch 39/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2576489278033.5400 - val_loss: 2753863009504.9634\n",
      "Epoch 40/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2529214769146.2373 - val_loss: 2703997217681.1025\n",
      "Epoch 41/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2484182741547.5068 - val_loss: 2656390277198.9243\n",
      "Epoch 42/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2449843573130.1562 - val_loss: 2619654179247.4912\n",
      "Epoch 43/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 2396934705739.7773 - val_loss: 2565307664815.4912\n",
      "Epoch 44/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2363902527874.0889 - val_loss: 2525052158970.5273\n",
      "Epoch 45/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2331228295101.1548 - val_loss: 2482642627712.7563\n",
      "Epoch 46/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2292445849999.9189 - val_loss: 2452078284506.1943\n",
      "Epoch 47/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2259646764324.1597 - val_loss: 2411277100411.3555\n",
      "Epoch 48/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2224987382972.4346 - val_loss: 2387745424148.0913\n",
      "Epoch 49/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2192104875611.3359 - val_loss: 2334812071203.7896\n",
      "Epoch 50/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2168332895120.7834 - val_loss: 2327871247682.8984\n",
      "Epoch 51/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2137970734699.4709 - val_loss: 2276102805895.1650\n",
      "Epoch 52/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2116329862929.4316 - val_loss: 2242772784165.1577\n",
      "Epoch 53/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 2079052971967.4597 - val_loss: 2208344862032.1484\n",
      "Epoch 54/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2058580314412.8037 - val_loss: 2193719482819.0784\n",
      "Epoch 55/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2029098126753.7830 - val_loss: 2171835594297.4651\n",
      "Epoch 56/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2009671213383.3113 - val_loss: 2192271394496.2700\n",
      "Epoch 57/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1987199469271.8066 - val_loss: 2105245158696.9744\n",
      "Epoch 58/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1966848085248.4321 - val_loss: 2084443969765.8599\n",
      "Epoch 59/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 1940120793127.1851 - val_loss: 2067915659715.0784\n",
      "Epoch 60/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1922031231192.6707 - val_loss: 2041226487840.8372\n",
      "Epoch 61/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1902163022439.4373 - val_loss: 2012086266285.1870\n",
      "Epoch 62/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1888802548874.8767 - val_loss: 2014208731108.9238\n",
      "Epoch 63/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1873582400866.3950 - val_loss: 1975744896030.8208\n",
      "Epoch 64/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1849755039947.9932 - val_loss: 1952618858554.1851\n",
      "Epoch 65/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1830217371286.1135 - val_loss: 1953940671964.7146\n",
      "Epoch 66/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1812804579634.5662 - val_loss: 2036065711415.3767\n",
      "Epoch 67/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 1820917714028.9116 - val_loss: 1900633764004.7617\n",
      "Epoch 68/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1784709547809.5667 - val_loss: 1913307231087.1133\n",
      "Epoch 69/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 1769591452194.2871 - val_loss: 1914116422384.9497\n",
      "Epoch 70/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1755428200646.2307 - val_loss: 1848766264709.7249\n",
      "Epoch 71/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1734976781718.2578 - val_loss: 1832219048767.2979\n",
      "Epoch 72/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 1725798253152.5222 - val_loss: 1814685011609.6721\n",
      "Epoch 73/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 1726404070739.4126 - val_loss: 1822855854044.8584\n",
      "Epoch 74/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1692915002582.9421 - val_loss: 1783099959416.4028\n",
      "Epoch 75/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1680582283980.8577 - val_loss: 1766188272614.3640\n",
      "Epoch 76/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1668185992102.6809 - val_loss: 1750243139240.6504\n",
      "Epoch 77/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1651878801290.4446 - val_loss: 1733775730402.8354\n",
      "Epoch 78/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 1632507277102.2441 - val_loss: 1718832874416.7876\n",
      "Epoch 79/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 1613392495043.7817 - val_loss: 1737434656937.3704\n",
      "Epoch 80/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1611827378527.5137 - val_loss: 1692137444343.0706\n",
      "Epoch 81/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1599709308986.7778 - val_loss: 1672952984140.7639\n",
      "Epoch 82/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1574261802850.1069 - val_loss: 1654895091871.0007\n",
      "Epoch 83/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 1563117458493.6589 - val_loss: 1651413515596.6919\n",
      "Epoch 84/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1553466167275.2551 - val_loss: 1659580203289.1318\n",
      "Epoch 85/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1529186021230.7847 - val_loss: 1609419473287.7412\n",
      "Epoch 86/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1522557213341.6050 - val_loss: 1596832330408.9385\n",
      "Epoch 87/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 1505248566175.7661 - val_loss: 1579687017334.6025\n",
      "Epoch 88/1000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 1491132928080.6753 - val_loss: 1565920923294.2808\n",
      "Epoch 89/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 1480609571290.8318 - val_loss: 1571296422791.8853\n",
      "Epoch 90/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 1471838262444.2996 - val_loss: 1538914316494.2402\n",
      "Epoch 91/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1447226413293.4160 - val_loss: 1551800572276.7302\n",
      "Epoch 92/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 1435369027225.5710 - val_loss: 1513716969733.5449\n",
      "Epoch 93/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1422936191411.6465 - val_loss: 1501361341079.6558\n",
      "Epoch 94/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1412909772793.0850 - val_loss: 1486561264533.1353\n",
      "Epoch 95/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 1401030700283.8223 - val_loss: 1490581280638.3796\n",
      "Epoch 96/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1380904440377.3372 - val_loss: 1463562141401.0420\n",
      "Epoch 97/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1379225875679.0095 - val_loss: 1449826598123.9089\n",
      "Epoch 98/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1358102015642.7236 - val_loss: 1432407891200.6482\n",
      "Epoch 99/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1350080038007.2844 - val_loss: 1420159738084.1316\n",
      "Epoch 100/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1340747834159.9729 - val_loss: 1420709041000.2002\n",
      "Epoch 101/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 1325015223202.0708 - val_loss: 1396996815877.4729\n",
      "Epoch 102/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 1320795033809.7559 - val_loss: 1386260878363.9404\n",
      "Epoch 103/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1302892675496.1216 - val_loss: 1373108715395.5645\n",
      "Epoch 104/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1291339804600.5447 - val_loss: 1363061009227.9719\n",
      "Epoch 105/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1279629930738.0259 - val_loss: 1355267017559.4937\n",
      "Epoch 106/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1269463053322.9487 - val_loss: 1344831686248.7043\n",
      "Epoch 107/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1257953650285.7761 - val_loss: 1334351262187.9810\n",
      "Epoch 108/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1257645961333.5554 - val_loss: 1321389051711.5859\n",
      "Epoch 109/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1239336263804.4705 - val_loss: 1320863200544.3330\n",
      "Epoch 110/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1230137484430.3342 - val_loss: 1298430750151.9753\n",
      "Epoch 111/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 1221487804462.1001 - val_loss: 1287584982819.3574\n",
      "Epoch 112/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 1208152881150.8474 - val_loss: 1291457875563.5847\n",
      "Epoch 113/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 1199160872671.8738 - val_loss: 1286976505043.1370\n",
      "Epoch 114/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1189294855589.2402 - val_loss: 1266829830725.2749\n",
      "Epoch 115/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1184016286421.0691 - val_loss: 1255068255622.8770\n",
      "Epoch 116/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1175835069936.1531 - val_loss: 1253560523567.1675\n",
      "Epoch 117/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1169277014668.3174 - val_loss: 1234641994405.4819\n",
      "Epoch 118/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1158987186194.4402 - val_loss: 1236298846246.0220\n",
      "Epoch 119/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1153582583678.9197 - val_loss: 1222876929875.1731\n",
      "Epoch 120/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1142782892710.8250 - val_loss: 1223475075306.4688\n",
      "Epoch 121/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1137316031278.8206 - val_loss: 1207236364581.2297\n",
      "Epoch 122/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1133449297541.9785 - val_loss: 1205622836516.3657\n",
      "Epoch 123/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1127615646238.8293 - val_loss: 1193379296028.1565\n",
      "Epoch 124/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1117266896106.5347 - val_loss: 1195996862638.5552\n",
      "Epoch 125/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1112315785199.8650 - val_loss: 1190857692704.1169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1107052031237.0422 - val_loss: 1175445344298.6306\n",
      "Epoch 127/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1101072090581.6455 - val_loss: 1171203809250.0432\n",
      "Epoch 128/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 1093539944084.9612 - val_loss: 1170135765057.6743\n",
      "Epoch 129/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1093812745780.1509 - val_loss: 1162511637096.9924\n",
      "Epoch 130/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 1085123367702.6178 - val_loss: 1192585951611.6433\n",
      "Epoch 131/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1083468478041.0309 - val_loss: 1163069214546.0208\n",
      "Epoch 132/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1080823880856.7068 - val_loss: 1153655697651.1101\n",
      "Epoch 133/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1072952561523.3945 - val_loss: 1145315498209.2512\n",
      "Epoch 134/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1082900257243.9843 - val_loss: 1138830262857.3074\n",
      "Epoch 135/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 1063718215490.9893 - val_loss: 1150845968980.8293\n",
      "Epoch 136/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1066285234638.1543 - val_loss: 1128869565712.2026\n",
      "Epoch 137/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1058769635843.1694 - val_loss: 1127255217073.9397\n",
      "Epoch 138/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1051061550200.4368 - val_loss: 1122245791311.9324\n",
      "Epoch 139/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 1047515605318.1587 - val_loss: 1118945321146.6531\n",
      "Epoch 140/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1047372929592.1846 - val_loss: 1114750834973.1646\n",
      "Epoch 141/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 1042787448750.1722 - val_loss: 1126133493496.1509\n",
      "Epoch 142/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1044174414520.1124 - val_loss: 1108805729340.7776\n",
      "Epoch 143/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1038416755694.1362 - val_loss: 1106088987239.8403\n",
      "Epoch 144/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 1031236139940.3760 - val_loss: 1109838300914.9661\n",
      "Epoch 145/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 1031060972409.7333 - val_loss: 1104633369337.5911\n",
      "Epoch 146/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1028526407366.5188 - val_loss: 1104794014794.0276\n",
      "Epoch 147/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1032042783685.7985 - val_loss: 1096913540529.5077\n",
      "Epoch 148/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1027049324138.3185 - val_loss: 1102376313258.3066\n",
      "Epoch 149/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1022563201939.0883 - val_loss: 1089338334390.9086\n",
      "Epoch 150/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1023553786950.8790 - val_loss: 1097452599071.9010\n",
      "Epoch 151/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1021707215730.8182 - val_loss: 1093001943974.9941\n",
      "Epoch 152/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 1011670891704.9769 - val_loss: 1089568495974.9041\n",
      "Epoch 153/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 1007263160740.6641 - val_loss: 1078910940918.4225\n",
      "Epoch 154/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1013553951054.2262 - val_loss: 1076525132315.7964\n",
      "Epoch 155/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1007152587702.5278 - val_loss: 1091831991910.9761\n",
      "Epoch 156/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1003938753465.1210 - val_loss: 1080869444477.5156\n",
      "Epoch 157/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1010457241621.8976 - val_loss: 1070949433814.0895\n",
      "Epoch 158/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 998284493666.6832 - val_loss: 1070032454581.3964\n",
      "Epoch 159/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 996977915859.0522 - val_loss: 1076652601107.8031\n",
      "Epoch 160/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 997832655364.3219 - val_loss: 1069017546682.8691\n",
      "Epoch 161/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 993592645370.3816 - val_loss: 1071319111126.3774\n",
      "Epoch 162/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 993118959718.5729 - val_loss: 1073192031674.7252\n",
      "Epoch 163/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 991655187526.8790 - val_loss: 1060251420905.3164\n",
      "Epoch 164/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 991727614432.5942 - val_loss: 1056653672365.0431\n",
      "Epoch 165/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 986457269058.4131 - val_loss: 1057947035630.1412\n",
      "Epoch 166/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 982157119269.0242 - val_loss: 1059876248745.0824\n",
      "Epoch 167/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 983508296486.7529 - val_loss: 1054103113890.4574\n",
      "Epoch 168/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 985096917976.2386 - val_loss: 1048537723399.6332\n",
      "Epoch 169/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 978516961375.6580 - val_loss: 1062692611133.3536\n",
      "Epoch 170/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 980457502029.6500 - val_loss: 1052273135773.8486\n",
      "Epoch 171/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 976329972022.0237 - val_loss: 1046010517826.0343\n",
      "Epoch 172/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 971252114325.3934 - val_loss: 1045016733129.7035\n",
      "Epoch 173/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 975411551476.9072 - val_loss: 1042080685192.8214\n",
      "Epoch 174/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 972659410933.6276 - val_loss: 1074800132437.0453\n",
      "Epoch 175/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 969991383025.0175 - val_loss: 1037644500186.6262\n",
      "Epoch 176/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 965841199975.2932 - val_loss: 1040886088975.0503\n",
      "Epoch 177/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 963684863019.2189 - val_loss: 1051622546462.8208\n",
      "Epoch 178/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 963394841068.1193 - val_loss: 1044320441943.4216\n",
      "Epoch 179/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 961347175441.8638 - val_loss: 1031063929845.9185\n",
      "Epoch 180/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 960883109198.8025 - val_loss: 1035233095784.8484\n",
      "Epoch 181/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 960747306473.2380 - val_loss: 1030584524180.1272\n",
      "Epoch 182/1000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 960960600410.9038 - val_loss: 1026978848346.3021\n",
      "Epoch 183/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 959122001355.2729 - val_loss: 1031887647350.2424\n",
      "Epoch 184/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 956304521309.3528 - val_loss: 1024372982170.1761\n",
      "Epoch 185/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 961428957030.7169 - val_loss: 1029094502329.7170\n",
      "Epoch 186/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 963086769332.3668 - val_loss: 1029930344454.9131\n",
      "Epoch 187/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 954240124327.5453 - val_loss: 1032204104969.2894\n",
      "Epoch 188/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 952726648132.4299 - val_loss: 1036633180674.4484\n",
      "Epoch 189/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 948665755268.8260 - val_loss: 1036628199606.6205\n",
      "Epoch 190/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 947824786072.9950 - val_loss: 1016153813838.8523\n",
      "Epoch 191/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 948055711739.3900 - val_loss: 1030200467624.2183\n",
      "Epoch 192/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 947920603503.6489 - val_loss: 1022558083564.5570\n",
      "Epoch 193/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 950429712218.0394 - val_loss: 1026620658840.3759\n",
      "Epoch 194/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 953574647663.9370 - val_loss: 1010033690548.5322\n",
      "Epoch 195/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 944315795860.5289 - val_loss: 1015647754027.4227\n",
      "Epoch 196/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 941775437206.2577 - val_loss: 1009214852130.2773\n",
      "Epoch 197/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 941228695252.3489 - val_loss: 1011189454133.0723\n",
      "Epoch 198/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 939070476442.4355 - val_loss: 1006410361067.6208\n",
      "Epoch 199/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 941651541513.5082 - val_loss: 1006217241020.7415\n",
      "Epoch 200/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 936682896393.7963 - val_loss: 1002233589523.8031\n",
      "Epoch 201/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 933769883253.8435 - val_loss: 999974830651.4813\n",
      "Epoch 202/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 933772233431.2302 - val_loss: 999146580858.0591\n",
      "Epoch 203/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 933545244949.7535 - val_loss: 999275985423.1223\n",
      "Epoch 204/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 930758570604.0472 - val_loss: 996159357766.7871\n",
      "Epoch 205/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 934121404085.2313 - val_loss: 1004600066720.8732\n",
      "Epoch 206/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 927548150661.2583 - val_loss: 1017183105958.4180\n",
      "Epoch 207/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 929694684977.7018 - val_loss: 993241515380.4421\n",
      "Epoch 208/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 931852132384.2701 - val_loss: 992964965738.0726\n",
      "Epoch 209/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 921183934396.0022 - val_loss: 988841519740.5795\n",
      "Epoch 210/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 925925506629.4384 - val_loss: 1004825112219.9763\n",
      "Epoch 211/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 921285136289.4946 - val_loss: 987284288445.4615\n",
      "Epoch 212/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 918879180216.8328 - val_loss: 986655804547.0604\n",
      "Epoch 213/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 917136530612.3668 - val_loss: 993587983141.3738\n",
      "Epoch 214/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 922066156885.7175 - val_loss: 987590273798.8411\n",
      "Epoch 215/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 918100127995.2460 - val_loss: 981482511980.7369\n",
      "Epoch 216/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 912105518196.9792 - val_loss: 983839502662.0669\n",
      "Epoch 217/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 916966666087.8694 - val_loss: 992828893276.7505\n",
      "Epoch 218/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 912762070748.4165 - val_loss: 983853607060.0552\n",
      "Epoch 219/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 912520986061.5779 - val_loss: 978563247564.5840\n",
      "Epoch 220/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 908699927191.8424 - val_loss: 976518859381.6664\n",
      "Epoch 221/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 910310047541.1593 - val_loss: 972821975981.3311\n",
      "Epoch 222/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 905924198612.0608 - val_loss: 971486930640.4005\n",
      "Epoch 223/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 909737296259.2415 - val_loss: 969208528038.7781\n",
      "Epoch 224/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 904870586260.8170 - val_loss: 1017871010675.1460\n",
      "Epoch 225/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 910676954303.8920 - val_loss: 970068254763.7828\n",
      "Epoch 226/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 899703413285.7445 - val_loss: 965463468802.2323\n",
      "Epoch 227/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 899294994630.2307 - val_loss: 966265734226.0928\n",
      "Epoch 228/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 899962832655.1267 - val_loss: 972213744937.5505\n",
      "Epoch 229/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 902884894585.1571 - val_loss: 963070721310.6047\n",
      "Epoch 230/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 905920255092.9792 - val_loss: 974045734944.8372\n",
      "Epoch 231/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 899485811084.4614 - val_loss: 987414910837.1624\n",
      "Epoch 232/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 893718189864.4817 - val_loss: 960862947455.3159\n",
      "Epoch 233/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 890272471322.3635 - val_loss: 959330843768.6909\n",
      "Epoch 234/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 892439953942.7620 - val_loss: 966705003691.3867\n",
      "Epoch 235/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 892134442241.5846 - val_loss: 955093917716.7393\n",
      "Epoch 236/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 891001398635.6150 - val_loss: 953149848189.7317\n",
      "Epoch 237/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 894628038359.8064 - val_loss: 951801981647.5364\n",
      "Epoch 238/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 886764359262.7935 - val_loss: 954570194303.3879\n",
      "Epoch 239/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 891708748193.2065 - val_loss: 950605527673.1229\n",
      "Epoch 240/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 884211097281.9089 - val_loss: 953408449079.4486\n",
      "Epoch 241/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 882650360355.4396 - val_loss: 1005494480424.7584\n",
      "Epoch 242/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 885719576679.7253 - val_loss: 951523488120.7628\n",
      "Epoch 243/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 897490311395.0433 - val_loss: 944560611616.0450\n",
      "Epoch 244/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 883635980231.5272 - val_loss: 945565141082.4462\n",
      "Epoch 245/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 878985907797.5734 - val_loss: 961780819175.8762\n",
      "Epoch 246/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 879313017178.9038 - val_loss: 980417133453.6461\n",
      "Epoch 247/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 880053818614.0596 - val_loss: 965318143966.5868\n",
      "Epoch 248/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 880872253535.6580 - val_loss: 942070502644.5502\n",
      "Epoch 249/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 879297916838.1046 - val_loss: 956663285302.0084\n",
      "Epoch 250/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 884413294245.0962 - val_loss: 939619903474.7499\n",
      "Epoch 251/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 76us/step - loss: 876624788104.8599 - val_loss: 935358815838.9109\n",
      "Epoch 252/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 873631608769.7648 - val_loss: 942196516537.3569\n",
      "Epoch 253/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 874257937732.4299 - val_loss: 939682001443.2855\n",
      "Epoch 254/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 871865410075.3719 - val_loss: 950248103546.5631\n",
      "Epoch 255/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 876335280028.8846 - val_loss: 931827865368.9879\n",
      "Epoch 256/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 869598957381.2943 - val_loss: 996386799708.1744\n",
      "Epoch 257/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 875608747139.9618 - val_loss: 930620597862.4000\n",
      "Epoch 258/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 868756024903.7434 - val_loss: 927541279048.9474\n",
      "Epoch 259/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 871715469571.3134 - val_loss: 928134389988.1316\n",
      "Epoch 260/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 868453580504.9590 - val_loss: 929750151106.9344\n",
      "Epoch 261/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 871256536950.8519 - val_loss: 927364328456.9294\n",
      "Epoch 262/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 869637568448.6122 - val_loss: 925575747726.2942\n",
      "Epoch 263/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 865531599674.3455 - val_loss: 934738508657.9938\n",
      "Epoch 264/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 865403624093.6049 - val_loss: 924747016893.1016\n",
      "Epoch 265/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 860978518049.4226 - val_loss: 952877322033.7598\n",
      "Epoch 266/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 865260960150.8340 - val_loss: 939474903424.8281\n",
      "Epoch 267/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 866461878178.6472 - val_loss: 919483274728.8124\n",
      "Epoch 268/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 861626846049.5306 - val_loss: 921392839671.6467\n",
      "Epoch 269/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 858438912671.9100 - val_loss: 945374538281.6226\n",
      "Epoch 270/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 866371184763.3179 - val_loss: 917816072974.9064\n",
      "Epoch 271/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 870608600534.7980 - val_loss: 967383793797.6528\n",
      "Epoch 272/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 866270362366.4154 - val_loss: 916401707468.2959\n",
      "Epoch 273/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 858584639910.9691 - val_loss: 934546634126.9424\n",
      "Epoch 274/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 856611013576.1035 - val_loss: 912960085774.0422\n",
      "Epoch 275/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 854898548786.7102 - val_loss: 911971700525.1510\n",
      "Epoch 276/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 853871161456.9454 - val_loss: 911828495073.9713\n",
      "Epoch 277/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 853068664064.4323 - val_loss: 920217918411.8639\n",
      "Epoch 278/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 855441174917.5464 - val_loss: 915878988829.6686\n",
      "Epoch 279/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 852735043670.4379 - val_loss: 918436880109.7811\n",
      "Epoch 280/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 850294922272.8463 - val_loss: 926086572216.9249\n",
      "Epoch 281/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 849525098006.1858 - val_loss: 907200539103.8829\n",
      "Epoch 282/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 848087246929.8278 - val_loss: 909762330264.5198\n",
      "Epoch 283/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 849757296138.6608 - val_loss: 903766054925.2501\n",
      "Epoch 284/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 849267086628.1599 - val_loss: 903528860139.4048\n",
      "Epoch 285/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 846900142757.6725 - val_loss: 901517447754.4596\n",
      "Epoch 286/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 847686426610.1700 - val_loss: 900544234688.7021\n",
      "Epoch 287/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 846356387052.8396 - val_loss: 920804941130.3876\n",
      "Epoch 288/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 842485079506.1880 - val_loss: 904647823195.2383\n",
      "Epoch 289/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 845695075235.2234 - val_loss: 903562681222.7330\n",
      "Epoch 290/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 843752134016.3601 - val_loss: 909540838171.5803\n",
      "Epoch 291/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 853530625705.7063 - val_loss: 895418222392.0968\n",
      "Epoch 292/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 845692399286.3839 - val_loss: 894566088846.8704\n",
      "Epoch 293/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 841773314004.2048 - val_loss: 908093927127.8898\n",
      "Epoch 294/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 844505104282.5796 - val_loss: 917047554056.0653\n",
      "Epoch 295/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 846777065559.5902 - val_loss: 899997832316.7235\n",
      "Epoch 296/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 839780112620.2633 - val_loss: 890731614292.1091\n",
      "Epoch 297/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 838136631201.4946 - val_loss: 911835789691.6433\n",
      "Epoch 298/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 838056649566.0731 - val_loss: 891010652210.1199\n",
      "Epoch 299/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 834960863612.3263 - val_loss: 892320545239.8177\n",
      "Epoch 300/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 838672773140.7451 - val_loss: 896932480856.3578\n",
      "Epoch 301/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 835149502147.0613 - val_loss: 888323361102.7083\n",
      "Epoch 302/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 839876487567.9191 - val_loss: 885489322005.3153\n",
      "Epoch 303/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 830762055109.5104 - val_loss: 885226164999.4171\n",
      "Epoch 304/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 835886475156.8170 - val_loss: 884753883373.3491\n",
      "Epoch 305/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 830442678247.7974 - val_loss: 891220209349.4548\n",
      "Epoch 306/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 829256336918.1858 - val_loss: 888890175606.6746\n",
      "Epoch 307/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 829318750876.4525 - val_loss: 891262044831.1449\n",
      "Epoch 308/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 828051818476.4075 - val_loss: 880671153795.7806\n",
      "Epoch 309/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 828416031763.0164 - val_loss: 882857568122.9232\n",
      "Epoch 310/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 823100083871.3336 - val_loss: 883509502369.0891\n",
      "Epoch 311/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 827651139744.1981 - val_loss: 881736619805.5967\n",
      "Epoch 312/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 820932326291.0883 - val_loss: 877703623433.7216\n",
      "Epoch 313/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 830134189323.9573 - val_loss: 886197254071.9888\n",
      "Epoch 314/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 824651016453.6184 - val_loss: 880270307395.9786\n",
      "Epoch 315/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 826987937626.0394 - val_loss: 909936400028.5525\n",
      "Epoch 316/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 823650673234.6923 - val_loss: 895345044415.1898\n",
      "Epoch 317/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 822659364560.8915 - val_loss: 883832401015.5387\n",
      "Epoch 318/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 821323450302.3073 - val_loss: 902436062695.6602\n",
      "Epoch 319/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 823025033755.3719 - val_loss: 878026830320.5896\n",
      "Epoch 320/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 819708885896.7158 - val_loss: 876706893433.4110\n",
      "Epoch 321/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 816123670078.5233 - val_loss: 876752968135.6873\n",
      "Epoch 322/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 821567316691.7726 - val_loss: 871602968391.0751\n",
      "Epoch 323/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 819095170461.1729 - val_loss: 874833903579.1302\n",
      "Epoch 324/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 818619442014.0731 - val_loss: 880062237131.1438\n",
      "Epoch 325/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 815476650032.4053 - val_loss: 872747055838.5148\n",
      "Epoch 326/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 817182281910.0956 - val_loss: 868644532232.0653\n",
      "Epoch 327/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 813254095283.6466 - val_loss: 870992827696.4636\n",
      "Epoch 328/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 812933786294.9601 - val_loss: 873653025603.9066\n",
      "Epoch 329/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 815755598447.5048 - val_loss: 866381022995.2271\n",
      "Epoch 330/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 817677879350.1677 - val_loss: 866411909853.9387\n",
      "Epoch 331/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 819691037423.4327 - val_loss: 866002171269.7249\n",
      "Epoch 332/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 812331666697.6522 - val_loss: 868864766190.5012\n",
      "Epoch 333/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 819755113763.0073 - val_loss: 865894568669.9387\n",
      "Epoch 334/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 812418233235.0883 - val_loss: 863372988242.0209\n",
      "Epoch 335/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 810875039884.6055 - val_loss: 885082680748.3229\n",
      "Epoch 336/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 808428105813.2853 - val_loss: 927120044528.5896\n",
      "Epoch 337/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 813004491728.1711 - val_loss: 862603498874.2031\n",
      "Epoch 338/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 810566017625.6072 - val_loss: 861608902443.9989\n",
      "Epoch 339/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 811594161562.8677 - val_loss: 891172361875.3350\n",
      "Epoch 340/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 806930462407.6714 - val_loss: 865525982038.9176\n",
      "Epoch 341/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 808012282299.7141 - val_loss: 871406958713.5551\n",
      "Epoch 342/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 807750228531.5745 - val_loss: 870931165436.3274\n",
      "Epoch 343/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 812345071151.5408 - val_loss: 859466577142.8546\n",
      "Epoch 344/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 807492271381.1772 - val_loss: 867097084742.7871\n",
      "Epoch 345/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 815828199399.7974 - val_loss: 857951922513.0127\n",
      "Epoch 346/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 804989014732.8575 - val_loss: 857942747897.0149\n",
      "Epoch 347/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 810257537587.5745 - val_loss: 856938429528.4298\n",
      "Epoch 348/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 805749865730.7372 - val_loss: 870644608087.2776\n",
      "Epoch 349/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 803918828817.7197 - val_loss: 857858698373.3649\n",
      "Epoch 350/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 802247964175.2706 - val_loss: 869936969531.5533\n",
      "Epoch 351/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 804290663043.0973 - val_loss: 866187867728.5085\n",
      "Epoch 352/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 812585465744.2072 - val_loss: 854493496245.3964\n",
      "Epoch 353/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 801116705549.3979 - val_loss: 857380412995.2585\n",
      "Epoch 354/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 802972944631.7883 - val_loss: 865396089815.9618\n",
      "Epoch 355/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 801546927998.9196 - val_loss: 899943869660.9305\n",
      "Epoch 356/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 815030648066.7372 - val_loss: 855204425030.9310\n",
      "Epoch 357/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 804259449983.3517 - val_loss: 852795158420.8473\n",
      "Epoch 358/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 805604918040.9230 - val_loss: 857990092421.5089\n",
      "Epoch 359/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 801480300494.4424 - val_loss: 864922464443.8053\n",
      "Epoch 360/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 802020094747.2279 - val_loss: 851816804493.7181\n",
      "Epoch 361/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 800029276013.6321 - val_loss: 854773107737.0599\n",
      "Epoch 362/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 804963410330.2915 - val_loss: 868451438491.7604\n",
      "Epoch 363/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 799092495301.7985 - val_loss: 851627840461.5922\n",
      "Epoch 364/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 800871303102.3073 - val_loss: 849169629437.4796\n",
      "Epoch 365/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 796996437351.0050 - val_loss: 852449997397.9814\n",
      "Epoch 366/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 803028310381.3438 - val_loss: 870967807664.7156\n",
      "Epoch 367/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 800205468922.6697 - val_loss: 848688655221.4503\n",
      "Epoch 368/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 795690669080.7788 - val_loss: 865281263701.8374\n",
      "Epoch 369/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 800025915551.0455 - val_loss: 849473540166.2830\n",
      "Epoch 370/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 795586833705.9224 - val_loss: 857036563982.8344\n",
      "Epoch 371/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 800820300678.4109 - val_loss: 848406678783.4960\n",
      "Epoch 372/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 795441860873.0759 - val_loss: 853910085242.5631\n",
      "Epoch 373/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 795033923807.5858 - val_loss: 854185553430.3235\n",
      "Epoch 374/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 803548037562.5616 - val_loss: 867612320322.3944\n",
      "Epoch 375/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 795342389939.5026 - val_loss: 847227268409.1050\n",
      "Epoch 376/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 796070208986.2555 - val_loss: 853882831096.8708\n",
      "Epoch 377/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 90us/step - loss: 795625541362.3140 - val_loss: 852186292977.5258\n",
      "Epoch 378/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 791214949541.3844 - val_loss: 848240428690.1829\n",
      "Epoch 379/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 793314933970.9083 - val_loss: 846696566130.7139\n",
      "Epoch 380/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 795457426438.3387 - val_loss: 844427744459.0718\n",
      "Epoch 381/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 793408363617.3867 - val_loss: 859495124864.1080\n",
      "Epoch 382/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 794741557937.7738 - val_loss: 845176568777.5594\n",
      "Epoch 383/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 795080154310.8070 - val_loss: 846622400772.3927\n",
      "Epoch 384/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 791164344381.0826 - val_loss: 844596799926.4045\n",
      "Epoch 385/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 791719041011.3224 - val_loss: 844578894071.7188\n",
      "Epoch 386/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 793122672121.3730 - val_loss: 843488347268.7887\n",
      "Epoch 387/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 794294631627.4170 - val_loss: 843666159691.4678\n",
      "Epoch 388/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 787553608956.9747 - val_loss: 843096592288.3690\n",
      "Epoch 389/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 798755011492.3760 - val_loss: 840859642782.9288\n",
      "Epoch 390/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 793211938773.3573 - val_loss: 843202982877.7227\n",
      "Epoch 391/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 791049802131.3766 - val_loss: 840921472321.1702\n",
      "Epoch 392/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 793211597290.9668 - val_loss: 842260756790.5126\n",
      "Epoch 393/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 795289318226.5481 - val_loss: 839971634149.2118\n",
      "Epoch 394/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 800986372000.3420 - val_loss: 851976900719.7615\n",
      "Epoch 395/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 788064217119.1176 - val_loss: 852782334776.9609\n",
      "Epoch 396/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 794140373270.9061 - val_loss: 851595893034.9907\n",
      "Epoch 397/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 791434262123.4711 - val_loss: 847675456263.9933\n",
      "Epoch 398/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 790552160619.0388 - val_loss: 848610524382.0826\n",
      "Epoch 399/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 789855801083.5341 - val_loss: 838609572666.1130\n",
      "Epoch 400/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 789369908365.1818 - val_loss: 838694432946.5879\n",
      "Epoch 401/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 800258806264.7969 - val_loss: 849665049806.5283\n",
      "Epoch 402/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 790267421709.2538 - val_loss: 860302812078.4833\n",
      "Epoch 403/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 785980118404.9702 - val_loss: 837859633917.6237\n",
      "Epoch 404/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 787404747510.3478 - val_loss: 843968957322.1896\n",
      "Epoch 405/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 788408032629.9877 - val_loss: 837497931373.3131\n",
      "Epoch 406/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 786247729179.6602 - val_loss: 849655252285.1376\n",
      "Epoch 407/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 788217250575.7029 - val_loss: 845144832513.2961\n",
      "Epoch 408/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 782455277511.5272 - val_loss: 836996072566.9626\n",
      "Epoch 409/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 785170728110.6044 - val_loss: 841309725964.7460\n",
      "Epoch 410/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 786451356908.8396 - val_loss: 839577969259.8728\n",
      "Epoch 411/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 786994065428.7451 - val_loss: 836395353257.6586\n",
      "Epoch 412/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 793274820318.7214 - val_loss: 837185983414.5486\n",
      "Epoch 413/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 790558546654.1453 - val_loss: 837642227699.3260\n",
      "Epoch 414/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 785751086049.4586 - val_loss: 842481812529.8318\n",
      "Epoch 415/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 793767010769.6117 - val_loss: 837589409499.3463\n",
      "Epoch 416/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 783820026724.9882 - val_loss: 837480694764.7009\n",
      "Epoch 417/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 784554408333.6139 - val_loss: 839653712192.3060\n",
      "Epoch 418/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 790139099123.3224 - val_loss: 834520377690.2301\n",
      "Epoch 419/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 789336677667.0073 - val_loss: 838436810414.4113\n",
      "Epoch 420/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 781760451212.3174 - val_loss: 840657771827.3440\n",
      "Epoch 421/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 787364167910.5009 - val_loss: 838348026933.2883\n",
      "Epoch 422/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 785458970596.3398 - val_loss: 835827858027.2968\n",
      "Epoch 423/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 783523404925.0466 - val_loss: 831930729701.5719\n",
      "Epoch 424/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 783691656231.7614 - val_loss: 832932489013.7924\n",
      "Epoch 425/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 782337828369.5757 - val_loss: 833053011782.4990\n",
      "Epoch 426/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 781801381136.5673 - val_loss: 831208200988.7325\n",
      "Epoch 427/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 786419153597.8751 - val_loss: 834505494824.1102\n",
      "Epoch 428/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 783992485354.3906 - val_loss: 841728653590.2515\n",
      "Epoch 429/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 781487346841.8593 - val_loss: 829381633933.3580\n",
      "Epoch 430/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 779516607078.2848 - val_loss: 833970999108.1947\n",
      "Epoch 431/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 789422013210.0753 - val_loss: 836343532297.1454\n",
      "Epoch 432/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 780153060675.2775 - val_loss: 835033726542.7803\n",
      "Epoch 433/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 783545152771.8898 - val_loss: 848044275718.3370\n",
      "Epoch 434/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 781215033458.0979 - val_loss: 837009964281.7350\n",
      "Epoch 435/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 788150026166.2397 - val_loss: 828245742099.1550\n",
      "Epoch 436/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 780212732879.0186 - val_loss: 830890854660.9688\n",
      "Epoch 437/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 782239473696.2701 - val_loss: 831545490557.2996\n",
      "Epoch 438/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 782770809969.5216 - val_loss: 841692121694.6228\n",
      "Epoch 439/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 785952460659.3945 - val_loss: 828152941740.5389\n",
      "Epoch 440/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 784469491877.9606 - val_loss: 847637613528.5378\n",
      "Epoch 441/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 785411309586.4401 - val_loss: 831968493047.2146\n",
      "Epoch 442/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 782677717980.8485 - val_loss: 826816350008.0968\n",
      "Epoch 443/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 782390856370.3500 - val_loss: 828787195857.9128\n",
      "Epoch 444/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 779961834418.2059 - val_loss: 828576260941.7002\n",
      "Epoch 445/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 778613940191.7299 - val_loss: 829528898118.4270\n",
      "Epoch 446/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 778159422547.5565 - val_loss: 824740991069.3265\n",
      "Epoch 447/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 779951856470.0056 - val_loss: 825919996171.3058\n",
      "Epoch 448/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 781701430856.8960 - val_loss: 824506531236.2577\n",
      "Epoch 449/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 778548448042.2104 - val_loss: 852492007448.7719\n",
      "Epoch 450/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 781792289703.8334 - val_loss: 826351553726.9739\n",
      "Epoch 451/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 779379212693.6815 - val_loss: 825015075461.2208\n",
      "Epoch 452/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 778454057762.1429 - val_loss: 826021100595.5601\n",
      "Epoch 453/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 775785190719.8198 - val_loss: 844312283530.6217\n",
      "Epoch 454/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 780293179298.6472 - val_loss: 823270799094.4225\n",
      "Epoch 455/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 775336364602.4896 - val_loss: 826358912885.1624\n",
      "Epoch 456/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 775295732983.2122 - val_loss: 834786107412.7393\n",
      "Epoch 457/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 779933596674.3049 - val_loss: 823020130005.5853\n",
      "Epoch 458/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 779781117874.7822 - val_loss: 838952715198.6138\n",
      "Epoch 459/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 777591167083.1830 - val_loss: 824326617790.2538\n",
      "Epoch 460/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 776217058134.0056 - val_loss: 825582802943.4240\n",
      "Epoch 461/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 778142566693.8887 - val_loss: 824689128602.6802\n",
      "Epoch 462/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 776678726707.2864 - val_loss: 821857221282.6014\n",
      "Epoch 463/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 777316338861.4519 - val_loss: 823929991848.9384\n",
      "Epoch 464/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 776904903300.2499 - val_loss: 823822127902.4608\n",
      "Epoch 465/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 775243824193.1165 - val_loss: 820346127414.4406\n",
      "Epoch 466/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 773251934773.8796 - val_loss: 820783766627.9517\n",
      "Epoch 467/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 773924015157.5914 - val_loss: 827404050060.9980\n",
      "Epoch 468/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 781229439502.6945 - val_loss: 826556622636.5750\n",
      "Epoch 469/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 774971432826.3094 - val_loss: 820827475812.4557\n",
      "Epoch 470/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 773767189833.0399 - val_loss: 827273516101.4188\n",
      "Epoch 471/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 781168509481.2020 - val_loss: 830686036401.2197\n",
      "Epoch 472/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 775739169732.6461 - val_loss: 822566360575.8560\n",
      "Epoch 473/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 774112763705.7693 - val_loss: 820520239099.3912\n",
      "Epoch 474/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 768038905149.5149 - val_loss: 822521667097.7800\n",
      "Epoch 475/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 780256852411.7141 - val_loss: 818804314053.2388\n",
      "Epoch 476/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 771434212417.1165 - val_loss: 826665953239.0977\n",
      "Epoch 477/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 772925486164.7091 - val_loss: 818068497008.4816\n",
      "Epoch 478/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 770863508397.5959 - val_loss: 817309374841.3390\n",
      "Epoch 479/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 771660027854.4424 - val_loss: 817601205847.4216\n",
      "Epoch 480/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 769890049480.9679 - val_loss: 821845841486.2042\n",
      "Epoch 481/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 772170940950.7620 - val_loss: 818025039538.4438\n",
      "Epoch 482/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 776644417099.2009 - val_loss: 823233312428.6830\n",
      "Epoch 483/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 775484832418.2150 - val_loss: 816420142518.6925\n",
      "Epoch 484/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 770793711388.9567 - val_loss: 826801580765.3625\n",
      "Epoch 485/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 783797319773.3528 - val_loss: 817132868018.3718\n",
      "Epoch 486/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 778325131973.9426 - val_loss: 823924375353.8251\n",
      "Epoch 487/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 769796075190.3839 - val_loss: 821680051905.4222\n",
      "Epoch 488/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 770773756949.3213 - val_loss: 824135741973.4594\n",
      "Epoch 489/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 772392605462.0416 - val_loss: 851469820279.6107\n",
      "Epoch 490/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 779640413359.7568 - val_loss: 815999757016.7539\n",
      "Epoch 491/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 767654154726.3567 - val_loss: 815471326348.8540\n",
      "Epoch 492/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 769322127099.5341 - val_loss: 816438579980.0259\n",
      "Epoch 493/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 769771805497.1930 - val_loss: 814593789704.2812\n",
      "Epoch 494/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 769469791158.8159 - val_loss: 814980683328.3781\n",
      "Epoch 495/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 777414470704.9814 - val_loss: 813693934853.2568\n",
      "Epoch 496/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 769062358290.8722 - val_loss: 814368569096.8574\n",
      "Epoch 497/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 768669074689.5846 - val_loss: 816098405219.0155\n",
      "Epoch 498/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 768180182546.7281 - val_loss: 813662617328.0856\n",
      "Epoch 499/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 768512159377.5037 - val_loss: 827270022184.6144\n",
      "Epoch 500/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 782005565348.9521 - val_loss: 814469427908.5907\n",
      "Epoch 501/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 769357332599.8604 - val_loss: 812852878850.7365\n",
      "Epoch 502/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 767134153193.8142 - val_loss: 828894320302.9873\n",
      "Epoch 503/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 79us/step - loss: 785950779755.0388 - val_loss: 812968177538.4124\n",
      "Epoch 504/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 766376980325.5645 - val_loss: 817453805414.7600\n",
      "Epoch 505/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 769639946679.1040 - val_loss: 815433676391.5521\n",
      "Epoch 506/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 770171243545.9314 - val_loss: 819825313264.8777\n",
      "Epoch 507/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 767642336026.0753 - val_loss: 818905989746.7859\n",
      "Epoch 508/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 770110418411.5430 - val_loss: 822549012016.8236\n",
      "Epoch 509/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 771960815348.6189 - val_loss: 811420915112.2903\n",
      "Epoch 510/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 767175725957.2583 - val_loss: 812326261138.1108\n",
      "Epoch 511/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 767633592455.9955 - val_loss: 811200255418.7252\n",
      "Epoch 512/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 770027745724.8666 - val_loss: 812609763956.5142\n",
      "Epoch 513/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 769917437812.5470 - val_loss: 810988914658.6194\n",
      "Epoch 514/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 767155505759.3696 - val_loss: 811574069398.3595\n",
      "Epoch 515/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 769220254725.1863 - val_loss: 810937534771.0560\n",
      "Epoch 516/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 765668715043.4396 - val_loss: 818938978204.9125\n",
      "Epoch 517/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 774382685578.7327 - val_loss: 810133010240.1620\n",
      "Epoch 518/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 767904575232.1442 - val_loss: 811498698191.4644\n",
      "Epoch 519/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 765672494012.5785 - val_loss: 848343583192.9698\n",
      "Epoch 520/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 767230809195.7592 - val_loss: 809363548626.0568\n",
      "Epoch 521/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 765421222774.2758 - val_loss: 809241888618.7927\n",
      "Epoch 522/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 765540757727.5858 - val_loss: 823085687187.2631\n",
      "Epoch 523/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 770572788426.5526 - val_loss: 814125522808.0427\n",
      "Epoch 524/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 764970668536.7969 - val_loss: 814152228156.5615\n",
      "Epoch 525/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 761030272916.8170 - val_loss: 808819750724.7708\n",
      "Epoch 526/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 765145446862.1543 - val_loss: 811933949705.4335\n",
      "Epoch 527/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 765167253394.5121 - val_loss: 813386479239.8132\n",
      "Epoch 528/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 769058026350.2083 - val_loss: 807814390868.3972\n",
      "Epoch 529/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 769970338475.4351 - val_loss: 810343231800.2408\n",
      "Epoch 530/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 767503149276.1284 - val_loss: 808710739598.7263\n",
      "Epoch 531/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 763035892515.2954 - val_loss: 818305701643.1617\n",
      "Epoch 532/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 764651666109.8751 - val_loss: 809407339366.4720\n",
      "Epoch 533/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 763931625110.1136 - val_loss: 810650332122.8422\n",
      "Epoch 534/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 763741531682.8632 - val_loss: 835979178070.4135\n",
      "Epoch 535/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 765921174697.9944 - val_loss: 817827745018.8872\n",
      "Epoch 536/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 764500358717.3707 - val_loss: 811267843764.7483\n",
      "Epoch 537/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 765191119885.2538 - val_loss: 808198025417.6315\n",
      "Epoch 538/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 765339007576.4547 - val_loss: 824342726585.1409\n",
      "Epoch 539/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 761506676155.1379 - val_loss: 810967274994.0298\n",
      "Epoch 540/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 766193298462.5414 - val_loss: 805421198575.3654\n",
      "Epoch 541/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 765080228436.9972 - val_loss: 824458741255.9213\n",
      "Epoch 542/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 765494056260.4299 - val_loss: 811297939066.5631\n",
      "Epoch 543/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 762462608283.1559 - val_loss: 830249147284.5592\n",
      "Epoch 544/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 763600173840.8553 - val_loss: 815086409602.9885\n",
      "Epoch 545/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 766474332227.9978 - val_loss: 809018575574.4495\n",
      "Epoch 546/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 763284812110.2262 - val_loss: 805294046605.2141\n",
      "Epoch 547/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 762872606749.9651 - val_loss: 810498718112.5131\n",
      "Epoch 548/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 760935952377.0850 - val_loss: 810362130870.9806\n",
      "Epoch 549/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 760534299308.5874 - val_loss: 816131793815.4397\n",
      "Epoch 550/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 763615452921.2290 - val_loss: 804447489026.0164\n",
      "Epoch 551/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 760211399195.3719 - val_loss: 807527829094.9761\n",
      "Epoch 552/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 764822460596.9432 - val_loss: 804147916596.0641\n",
      "Epoch 553/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 764512548791.3923 - val_loss: 812944571434.3427\n",
      "Epoch 554/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 763586924827.5160 - val_loss: 804890047734.8546\n",
      "Epoch 555/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 762633385109.2493 - val_loss: 806048753346.5745\n",
      "Epoch 556/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 759256314669.6680 - val_loss: 803509041149.1195\n",
      "Epoch 557/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 761541140272.5492 - val_loss: 802494148574.5868\n",
      "Epoch 558/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 760458450073.8593 - val_loss: 806714643917.1600\n",
      "Epoch 559/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 759711301927.0410 - val_loss: 837807106182.2290\n",
      "Epoch 560/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 763736134920.4998 - val_loss: 802849001182.8029\n",
      "Epoch 561/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 761200723026.4041 - val_loss: 801962969768.3623\n",
      "Epoch 562/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 763172092771.8356 - val_loss: 805409487098.5991\n",
      "Epoch 563/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 759471243688.1215 - val_loss: 801984609829.3019\n",
      "Epoch 564/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 761682443349.8615 - val_loss: 803010585412.4827\n",
      "Epoch 565/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 758541694838.8519 - val_loss: 800953674286.2312\n",
      "Epoch 566/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 758453180135.9415 - val_loss: 811251227927.6917\n",
      "Epoch 567/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 763889441714.2059 - val_loss: 801467605948.3094\n",
      "Epoch 568/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 758527485531.3359 - val_loss: 801208202007.5477\n",
      "Epoch 569/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 758538594086.1768 - val_loss: 815033166741.7114\n",
      "Epoch 570/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 764172679381.2133 - val_loss: 815492520124.3815\n",
      "Epoch 571/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 766644689261.3438 - val_loss: 800807970913.3593\n",
      "Epoch 572/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 757930887277.4879 - val_loss: 805865553159.5612\n",
      "Epoch 573/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 764271341561.0850 - val_loss: 808046978187.7018\n",
      "Epoch 574/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 760052329050.7596 - val_loss: 803016709396.8113\n",
      "Epoch 575/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 760687074936.1487 - val_loss: 808526579230.9648\n",
      "Epoch 576/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 760104330078.0731 - val_loss: 799933296878.5012\n",
      "Epoch 577/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 757547034848.7383 - val_loss: 814017269927.0662\n",
      "Epoch 578/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 755285733571.9258 - val_loss: 820873326941.1106\n",
      "Epoch 579/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 763869643756.9836 - val_loss: 799185682588.6965\n",
      "Epoch 580/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 755964913077.3754 - val_loss: 802429658726.4000\n",
      "Epoch 581/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 759831493864.5177 - val_loss: 799658901610.0006\n",
      "Epoch 582/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 759332423512.8870 - val_loss: 805178141280.9271\n",
      "Epoch 583/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 756168816742.5729 - val_loss: 799628737702.7781\n",
      "Epoch 584/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 758442128560.9094 - val_loss: 802746677917.9927\n",
      "Epoch 585/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 759444788387.0793 - val_loss: 800961123660.1158\n",
      "Epoch 586/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 757676919866.2015 - val_loss: 799705155679.0548\n",
      "Epoch 587/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 754569170397.7130 - val_loss: 823016616649.1995\n",
      "Epoch 588/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 757378079057.6837 - val_loss: 798405153898.2886\n",
      "Epoch 589/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 756497919716.4840 - val_loss: 798909495408.3375\n",
      "Epoch 590/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 756467484753.8278 - val_loss: 797227798764.1969\n",
      "Epoch 591/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 755916615378.0438 - val_loss: 798263282215.3181\n",
      "Epoch 592/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 759527087680.8284 - val_loss: 803541803369.7845\n",
      "Epoch 593/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 755561613928.5897 - val_loss: 807562792753.7598\n",
      "Epoch 594/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 758376758779.6781 - val_loss: 817393929158.1029\n",
      "Epoch 595/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 758862235858.9083 - val_loss: 804605809678.1143\n",
      "Epoch 596/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 760493911203.6556 - val_loss: 799875942236.9666\n",
      "Epoch 597/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 754400581807.4688 - val_loss: 796509437025.6472\n",
      "Epoch 598/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 758004616642.0529 - val_loss: 801980392140.6560\n",
      "Epoch 599/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 754721770351.3607 - val_loss: 807375423725.9252\n",
      "Epoch 600/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 753221685697.4767 - val_loss: 804415907969.6202\n",
      "Epoch 601/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 759922389897.8683 - val_loss: 798632116371.4790\n",
      "Epoch 602/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 754411979166.9016 - val_loss: 797474602459.5623\n",
      "Epoch 603/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 760508815504.0630 - val_loss: 796607306699.8639\n",
      "Epoch 604/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 757838817274.2374 - val_loss: 795955394441.9015\n",
      "Epoch 605/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 754114820511.1897 - val_loss: 820354657749.8014\n",
      "Epoch 606/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 759815860170.9849 - val_loss: 800522422938.5361\n",
      "Epoch 607/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 757295014667.0928 - val_loss: 795639823554.4304\n",
      "Epoch 608/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 755718320775.7074 - val_loss: 801650822381.0610\n",
      "Epoch 609/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 754990954635.4530 - val_loss: 802882833949.2366\n",
      "Epoch 610/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 754338020979.5386 - val_loss: 795317259344.3645\n",
      "Epoch 611/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 752355838974.2712 - val_loss: 794423630989.4301\n",
      "Epoch 612/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 758286385087.4598 - val_loss: 806555953774.7533\n",
      "Epoch 613/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 755016015793.6298 - val_loss: 794951802522.8242\n",
      "Epoch 614/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 752690156246.0776 - val_loss: 798808828199.5342\n",
      "Epoch 615/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 756013141755.5341 - val_loss: 796710295915.5128\n",
      "Epoch 616/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 755385807523.3674 - val_loss: 796173784414.5508\n",
      "Epoch 617/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 754926750229.6094 - val_loss: 805005076056.5739\n",
      "Epoch 618/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 754104739336.3556 - val_loss: 797100568657.8048\n",
      "Epoch 619/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 754652875838.8114 - val_loss: 794186247172.3207\n",
      "Epoch 620/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 752774345148.8666 - val_loss: 795701575633.9128\n",
      "Epoch 621/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 755333204907.2909 - val_loss: 798942277810.5879\n",
      "Epoch 622/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 759191634473.2020 - val_loss: 796926030551.0256\n",
      "Epoch 623/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 753768230932.7451 - val_loss: 798761811488.6931\n",
      "Epoch 624/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 754544309415.6895 - val_loss: 793248271343.5814\n",
      "Epoch 625/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 750504662345.0399 - val_loss: 798919817028.7708\n",
      "Epoch 626/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 752453795941.4204 - val_loss: 794233414058.3066\n",
      "Epoch 627/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 753176912162.4312 - val_loss: 796260266890.7656\n",
      "Epoch 628/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 749411298057.9404 - val_loss: 796424844083.2000\n",
      "Epoch 629/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 74us/step - loss: 750967443499.7952 - val_loss: 796044340624.6707\n",
      "Epoch 630/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 753287903205.4924 - val_loss: 803291191917.6011\n",
      "Epoch 631/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 752839436420.5380 - val_loss: 793189608613.6259\n",
      "Epoch 632/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 752706926344.2117 - val_loss: 795164665732.1406\n",
      "Epoch 633/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 753006655037.3707 - val_loss: 806618254711.3226\n",
      "Epoch 634/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 749433524701.1368 - val_loss: 795365353265.1837\n",
      "Epoch 635/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 753274061353.2020 - val_loss: 806859562669.2590\n",
      "Epoch 636/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 756756899732.8170 - val_loss: 798285854684.8585\n",
      "Epoch 637/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 753010412417.8008 - val_loss: 792564600736.3690\n",
      "Epoch 638/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 748754964551.4553 - val_loss: 801617023779.0695\n",
      "Epoch 639/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 748709866194.6201 - val_loss: 816206827650.1963\n",
      "Epoch 640/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 750082701746.4941 - val_loss: 792823323724.9080\n",
      "Epoch 641/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 751340454791.5632 - val_loss: 791297212757.0453\n",
      "Epoch 642/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 750166083181.7760 - val_loss: 797510375709.1646\n",
      "Epoch 643/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 751147486436.7721 - val_loss: 796173446729.0194\n",
      "Epoch 644/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 750027822243.0793 - val_loss: 797964112748.8090\n",
      "Epoch 645/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 752942088708.3219 - val_loss: 801429248298.9907\n",
      "Epoch 646/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 747971847964.9567 - val_loss: 791286548985.2310\n",
      "Epoch 647/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 752434519930.3094 - val_loss: 791976447084.0168\n",
      "Epoch 648/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 748602290178.3049 - val_loss: 796834098313.9735\n",
      "Epoch 649/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 758294687515.8041 - val_loss: 789283845841.5527\n",
      "Epoch 650/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 750791725699.0973 - val_loss: 793907567268.9058\n",
      "Epoch 651/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 752625901056.2881 - val_loss: 791426558906.5812\n",
      "Epoch 652/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 750192009008.5492 - val_loss: 793613778727.3901\n",
      "Epoch 653/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 748578902013.6951 - val_loss: 792000066419.7220\n",
      "Epoch 654/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 749608970584.5988 - val_loss: 789561087423.3339\n",
      "Epoch 655/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 748218389455.0186 - val_loss: 788915617289.6495\n",
      "Epoch 656/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 747987801204.9792 - val_loss: 790092364731.7333\n",
      "Epoch 657/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 749312711170.8092 - val_loss: 788784011157.4233\n",
      "Epoch 658/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 745953653826.2690 - val_loss: 789079229512.8754\n",
      "Epoch 659/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 750372954393.2111 - val_loss: 814911032119.2327\n",
      "Epoch 660/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 750551201977.5531 - val_loss: 813558062760.6504\n",
      "Epoch 661/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 747079635955.8988 - val_loss: 801495045230.8973\n",
      "Epoch 662/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 752667459109.1683 - val_loss: 788303926147.8527\n",
      "Epoch 663/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 750103351727.0365 - val_loss: 788096289406.8838\n",
      "Epoch 664/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 748105455916.8036 - val_loss: 803662305399.2506\n",
      "Epoch 665/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 759365786306.4851 - val_loss: 788162613737.6765\n",
      "Epoch 666/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 745661461441.1886 - val_loss: 790440282114.8805\n",
      "Epoch 667/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 747644287014.0326 - val_loss: 791637614552.8259\n",
      "Epoch 668/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 751772499119.7568 - val_loss: 786880965903.6265\n",
      "Epoch 669/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 750846177162.4446 - val_loss: 787990701527.5297\n",
      "Epoch 670/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 748288193505.4586 - val_loss: 795784335748.2847\n",
      "Epoch 671/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 746737550280.1035 - val_loss: 816097694964.5502\n",
      "Epoch 672/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 746925472160.0540 - val_loss: 788034492894.7308\n",
      "Epoch 673/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 747143541780.1688 - val_loss: 798353939164.2104\n",
      "Epoch 674/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 746968626804.6910 - val_loss: 793924694664.6774\n",
      "Epoch 675/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 745878585823.4418 - val_loss: 787289383601.0037\n",
      "Epoch 676/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 744798329049.8232 - val_loss: 788962755578.5271\n",
      "Epoch 677/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 744735794816.2161 - val_loss: 786633682260.7572\n",
      "Epoch 678/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 746114769066.5706 - val_loss: 786751151475.8661\n",
      "Epoch 679/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 746938062374.3207 - val_loss: 809644141175.9707\n",
      "Epoch 680/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 744454724376.0585 - val_loss: 786590756346.6711\n",
      "Epoch 681/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 749044120078.6945 - val_loss: 792602547370.8107\n",
      "Epoch 682/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 745259784990.6854 - val_loss: 784531923416.3938\n",
      "Epoch 683/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 743002577408.8643 - val_loss: 822358762345.3524\n",
      "Epoch 684/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 751229563122.0259 - val_loss: 805168667895.4307\n",
      "Epoch 685/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 745918883013.6545 - val_loss: 800234683328.3420\n",
      "Epoch 686/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 747602991450.9038 - val_loss: 783643020575.7570\n",
      "Epoch 687/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 742732601469.6229 - val_loss: 792897041811.5510\n",
      "Epoch 688/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 751065914880.2881 - val_loss: 785039692030.6318\n",
      "Epoch 689/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 743424287969.3146 - val_loss: 789737543347.5961\n",
      "Epoch 690/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 743496829249.5487 - val_loss: 782486912535.4757\n",
      "Epoch 691/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 743211512889.0490 - val_loss: 782720346568.8394\n",
      "Epoch 692/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 742956124887.2302 - val_loss: 786345050227.2180\n",
      "Epoch 693/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 742239279137.4226 - val_loss: 788325226177.4222\n",
      "Epoch 694/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 748585683219.4485 - val_loss: 785871790764.3949\n",
      "Epoch 695/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 745349279156.2229 - val_loss: 790195504698.6172\n",
      "Epoch 696/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 741822806413.6139 - val_loss: 786331057162.0815\n",
      "Epoch 697/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 745123046911.1357 - val_loss: 784513864165.0677\n",
      "Epoch 698/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 738513805740.7316 - val_loss: 786819811957.6664\n",
      "Epoch 699/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 739964865903.0725 - val_loss: 793909343889.0306\n",
      "Epoch 700/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 739173011674.9758 - val_loss: 791258348931.9966\n",
      "Epoch 701/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 741464754442.2285 - val_loss: 807287167413.2523\n",
      "Epoch 702/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 752206542996.6731 - val_loss: 784945917798.7600\n",
      "Epoch 703/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 743082913683.6647 - val_loss: 779369496622.0872\n",
      "Epoch 704/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 738849558544.1351 - val_loss: 783880105388.3229\n",
      "Epoch 705/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 745343632171.3629 - val_loss: 780946121051.9584\n",
      "Epoch 706/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 740888443803.1559 - val_loss: 779018470417.5708\n",
      "Epoch 707/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 738748639189.3573 - val_loss: 781689881437.8307\n",
      "Epoch 708/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 743730662011.0298 - val_loss: 781213582007.9167\n",
      "Epoch 709/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 740409648136.6438 - val_loss: 781070338585.4919\n",
      "Epoch 710/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 744355818249.9404 - val_loss: 782227353981.3716\n",
      "Epoch 711/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 740666028301.6860 - val_loss: 799769603790.6722\n",
      "Epoch 712/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 739182698463.7299 - val_loss: 797832618373.4369\n",
      "Epoch 713/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 742222545808.7833 - val_loss: 783926708853.6664\n",
      "Epoch 714/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 741572127378.0798 - val_loss: 779767303981.1510\n",
      "Epoch 715/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 742294662700.0833 - val_loss: 780870490457.0779\n",
      "Epoch 716/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 740957997156.2678 - val_loss: 778680358570.3787\n",
      "Epoch 717/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 736837127571.9528 - val_loss: 777938183842.3134\n",
      "Epoch 718/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 739763686144.7203 - val_loss: 782247250798.5372\n",
      "Epoch 719/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 736257908639.1897 - val_loss: 776698305857.1702\n",
      "Epoch 720/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 745416389845.7896 - val_loss: 793427427742.7848\n",
      "Epoch 721/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 739930613081.1750 - val_loss: 777037235530.9637\n",
      "Epoch 722/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 740890028099.9978 - val_loss: 776479439259.9044\n",
      "Epoch 723/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 744148675457.8008 - val_loss: 814794079905.7373\n",
      "Epoch 724/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 739860637229.2358 - val_loss: 779569617194.9907\n",
      "Epoch 725/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 737956898238.5953 - val_loss: 775409216691.4520\n",
      "Epoch 726/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 737250325118.4873 - val_loss: 775431079630.0962\n",
      "Epoch 727/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 735130166589.5149 - val_loss: 777662706589.2006\n",
      "Epoch 728/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 735623782741.1414 - val_loss: 781407409484.1158\n",
      "Epoch 729/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 735582209207.8243 - val_loss: 778078831366.2650\n",
      "Epoch 730/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 738068448670.3252 - val_loss: 774288294288.9586\n",
      "Epoch 731/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 739426430111.6218 - val_loss: 797057077885.1555\n",
      "Epoch 732/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 738465741741.5959 - val_loss: 800145172660.3162\n",
      "Epoch 733/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 732132122342.7889 - val_loss: 817367334036.3431\n",
      "Epoch 734/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 746495603427.9077 - val_loss: 778025952474.0500\n",
      "Epoch 735/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 735231582088.1396 - val_loss: 774552062941.1466\n",
      "Epoch 736/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 734516471092.2948 - val_loss: 774163572828.7505\n",
      "Epoch 737/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 745590615245.7220 - val_loss: 785039294975.5680\n",
      "Epoch 738/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 738478292826.0394 - val_loss: 773346075345.2646\n",
      "Epoch 739/1000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 737848537671.1672 - val_loss: 775134020422.7871\n",
      "Epoch 740/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 732463895484.5785 - val_loss: 773754286724.0686\n",
      "Epoch 741/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 735722538324.5649 - val_loss: 772777426957.8262\n",
      "Epoch 742/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 738778838082.2690 - val_loss: 775485433524.7483\n",
      "Epoch 743/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 736112330114.6652 - val_loss: 793791444260.9418\n",
      "Epoch 744/1000\n",
      "3554/3554 [==============================] - 1s 143us/step - loss: 737235163934.6854 - val_loss: 778086672656.4906\n",
      "Epoch 745/1000\n",
      "3554/3554 [==============================] - 1s 147us/step - loss: 735700539766.5638 - val_loss: 779271624376.2048\n",
      "Epoch 746/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 733494186350.4963 - val_loss: 779190466409.6405\n",
      "Epoch 747/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 747392025267.5026 - val_loss: 780998036680.1913\n",
      "Epoch 748/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 734050902642.9623 - val_loss: 779609872835.0785\n",
      "Epoch 749/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 734503799852.3713 - val_loss: 770830253107.5601\n",
      "Epoch 750/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 731634879560.8960 - val_loss: 776456716168.1732\n",
      "Epoch 751/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 735759472673.4226 - val_loss: 779386282889.6135\n",
      "Epoch 752/1000\n",
      "3554/3554 [==============================] - 1s 148us/step - loss: 734253729191.5453 - val_loss: 777628120379.9854\n",
      "Epoch 753/1000\n",
      "3554/3554 [==============================] - 0s 140us/step - loss: 731691439654.3207 - val_loss: 771171430486.4135\n",
      "Epoch 754/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 733890952469.1772 - val_loss: 798731182285.0880\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 121us/step - loss: 733613424178.9983 - val_loss: 769014117595.2023\n",
      "Epoch 756/1000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 732762744807.7974 - val_loss: 770142715581.3896\n",
      "Epoch 757/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 735550515360.1981 - val_loss: 769056341670.3459\n",
      "Epoch 758/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 732911264264.9320 - val_loss: 770621339423.9010\n",
      "Epoch 759/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 731478339927.4463 - val_loss: 770355177643.0988\n",
      "Epoch 760/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 730507447718.3928 - val_loss: 769595546051.0785\n",
      "Epoch 761/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 728735209938.7642 - val_loss: 768108160227.2675\n",
      "Epoch 762/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 731139603182.7124 - val_loss: 771632389703.8672\n",
      "Epoch 763/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 728615080348.5964 - val_loss: 767500396374.0535\n",
      "Epoch 764/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 731045120487.5093 - val_loss: 770620456466.8669\n",
      "Epoch 765/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 734017937313.4946 - val_loss: 766605321023.8740\n",
      "Epoch 766/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 732535087237.6906 - val_loss: 779105653433.0690\n",
      "Epoch 767/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 736449059282.7642 - val_loss: 771618574185.0645\n",
      "Epoch 768/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 730786332358.5188 - val_loss: 770986674234.1851\n",
      "Epoch 769/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 727477470290.4041 - val_loss: 787901239946.9817\n",
      "Epoch 770/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 727398880059.4980 - val_loss: 775785017126.2379\n",
      "Epoch 771/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 731604414374.1046 - val_loss: 772800592283.9044\n",
      "Epoch 772/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 733473500637.7130 - val_loss: 770639354873.9510\n",
      "Epoch 773/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 729652720687.8289 - val_loss: 781886897203.5601\n",
      "Epoch 774/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 736749794582.3297 - val_loss: 764197135250.8309\n",
      "Epoch 775/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 728953620884.5289 - val_loss: 790474470711.3767\n",
      "Epoch 776/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 731446532691.8446 - val_loss: 765549214287.6444\n",
      "Epoch 777/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 727771053194.8768 - val_loss: 764058324965.4999\n",
      "Epoch 778/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 727620909057.1526 - val_loss: 765180326601.1995\n",
      "Epoch 779/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 726012350360.2747 - val_loss: 765360502467.7266\n",
      "Epoch 780/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 736721689559.0861 - val_loss: 765864000457.5594\n",
      "Epoch 781/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 727079906924.6234 - val_loss: 776212884581.1038\n",
      "Epoch 782/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 730517464593.5757 - val_loss: 776094704374.1344\n",
      "Epoch 783/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 728894769187.7277 - val_loss: 764451039227.9674\n",
      "Epoch 784/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 732170796622.6584 - val_loss: 764169317785.8881\n",
      "Epoch 785/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 727314055546.0214 - val_loss: 769997406443.0447\n",
      "Epoch 786/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 725193704613.3844 - val_loss: 762941100701.4166\n",
      "Epoch 787/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 727235277189.5464 - val_loss: 764330057274.0410\n",
      "Epoch 788/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 725679436437.5374 - val_loss: 766027238037.9274\n",
      "Epoch 789/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 727152011534.8385 - val_loss: 771549861795.2495\n",
      "Epoch 790/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 726379891664.1711 - val_loss: 778164501601.6472\n",
      "Epoch 791/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 723519248528.0630 - val_loss: 764813688426.4326\n",
      "Epoch 792/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 724447431761.2516 - val_loss: 762492594061.3580\n",
      "Epoch 793/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 724256379126.6360 - val_loss: 766777946553.2849\n",
      "Epoch 794/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 726072264813.4879 - val_loss: 765517401643.0627\n",
      "Epoch 795/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 723439546407.1852 - val_loss: 761216748790.2784\n",
      "Epoch 796/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 723995919663.6849 - val_loss: 789982290793.3524\n",
      "Epoch 797/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 725439585027.0253 - val_loss: 759938163837.8756\n",
      "Epoch 798/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 720644208069.5104 - val_loss: 762439643738.5902\n",
      "Epoch 799/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 722619167625.8683 - val_loss: 759522525428.2622\n",
      "Epoch 800/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 727536854637.7760 - val_loss: 761039569043.4790\n",
      "Epoch 801/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 724343273039.8109 - val_loss: 759093502470.4810\n",
      "Epoch 802/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 721693584626.6021 - val_loss: 798205322250.6577\n",
      "Epoch 803/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 724740666231.4283 - val_loss: 768130660086.4225\n",
      "Epoch 804/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 725700325811.6466 - val_loss: 760891447106.4664\n",
      "Epoch 805/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 722487840673.4946 - val_loss: 761000823026.5339\n",
      "Epoch 806/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 723247329770.9668 - val_loss: 763786844062.6407\n",
      "Epoch 807/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 724442938396.2363 - val_loss: 767882510760.0022\n",
      "Epoch 808/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 729109024830.2352 - val_loss: 768599452625.6248\n",
      "Epoch 809/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 719772561308.8846 - val_loss: 768115479077.5898\n",
      "Epoch 810/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 719954421025.8547 - val_loss: 758809784927.1989\n",
      "Epoch 811/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 722322579608.1306 - val_loss: 758052977812.3431\n",
      "Epoch 812/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 725006340619.2369 - val_loss: 768944762924.9351\n",
      "Epoch 813/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 719899649982.3073 - val_loss: 757480521446.8680\n",
      "Epoch 814/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 718706651645.4069 - val_loss: 759251245746.7319\n",
      "Epoch 815/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 721168762454.7260 - val_loss: 758283144876.1069\n",
      "Epoch 816/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 724684074291.1425 - val_loss: 757763221504.5760\n",
      "Epoch 817/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 720210585098.6608 - val_loss: 758293940729.2310\n",
      "Epoch 818/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 721501851147.8131 - val_loss: 764434343033.2670\n",
      "Epoch 819/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 720837593704.5897 - val_loss: 755418824081.8228\n",
      "Epoch 820/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 724439459382.4558 - val_loss: 754645387764.6222\n",
      "Epoch 821/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 720614358947.7997 - val_loss: 762479184257.4042\n",
      "Epoch 822/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 725188047818.4086 - val_loss: 772763520449.0621\n",
      "Epoch 823/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 717761704657.4677 - val_loss: 774374965957.7429\n",
      "Epoch 824/1000\n",
      "3554/3554 [==============================] - 0s 130us/step - loss: 719852293062.9510 - val_loss: 757855050308.1227\n",
      "Epoch 825/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 718324962273.4586 - val_loss: 755737261113.8971\n",
      "Epoch 826/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 716811319324.8126 - val_loss: 753526591410.8040\n",
      "Epoch 827/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 722200019801.4631 - val_loss: 753425076358.8051\n",
      "Epoch 828/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 715446868416.3241 - val_loss: 753905632370.3539\n",
      "Epoch 829/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 717128116513.2786 - val_loss: 770051917591.8357\n",
      "Epoch 830/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 715862815358.4873 - val_loss: 760241177829.5719\n",
      "Epoch 831/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 714337663806.9556 - val_loss: 761766121989.3289\n",
      "Epoch 832/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 717064623248.6393 - val_loss: 754982711119.7164\n",
      "Epoch 833/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 718801336523.9933 - val_loss: 755972219098.9142\n",
      "Epoch 834/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 715079804359.8154 - val_loss: 755420304324.3746\n",
      "Epoch 835/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 715127285173.3754 - val_loss: 752765371340.4399\n",
      "Epoch 836/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 714574292050.4041 - val_loss: 752262736029.2726\n",
      "Epoch 837/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 713479451714.2690 - val_loss: 752199889641.4604\n",
      "Epoch 838/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 717105496478.9016 - val_loss: 760080189774.7083\n",
      "Epoch 839/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 713944406731.7052 - val_loss: 754126623799.5927\n",
      "Epoch 840/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 717695730111.1716 - val_loss: 763227050026.6306\n",
      "Epoch 841/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 716521183747.1694 - val_loss: 752177942842.5452\n",
      "Epoch 842/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 713447579569.6298 - val_loss: 750360416826.3291\n",
      "Epoch 843/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 714291718654.5593 - val_loss: 758475830652.5074\n",
      "Epoch 844/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 714911552444.5785 - val_loss: 749333046941.7046\n",
      "Epoch 845/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 713196861588.6731 - val_loss: 752817486392.6008\n",
      "Epoch 846/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 714281130367.7839 - val_loss: 751836495097.7350\n",
      "Epoch 847/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 710709722960.8193 - val_loss: 756117892123.3643\n",
      "Epoch 848/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 717115672713.1481 - val_loss: 749395402035.3440\n",
      "Epoch 849/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 710568861414.7889 - val_loss: 754784284666.2391\n",
      "Epoch 850/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 712375314924.1193 - val_loss: 753807008861.6146\n",
      "Epoch 851/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 713366153993.9404 - val_loss: 752654078704.3735\n",
      "Epoch 852/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 711133844919.6804 - val_loss: 748863565007.1044\n",
      "Epoch 853/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 715587640578.7372 - val_loss: 752083221610.8647\n",
      "Epoch 854/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 714227630642.4221 - val_loss: 752616437757.1195\n",
      "Epoch 855/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 711017283556.3398 - val_loss: 748288715425.4492\n",
      "Epoch 856/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 709728068824.0945 - val_loss: 759219678530.3224\n",
      "Epoch 857/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 712198322306.2330 - val_loss: 756439409782.9626\n",
      "Epoch 858/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 708499986898.7642 - val_loss: 746343528684.7729\n",
      "Epoch 859/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 709042819447.7164 - val_loss: 745617466127.4824\n",
      "Epoch 860/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 709201803661.6139 - val_loss: 754024276698.4822\n",
      "Epoch 861/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 713776673127.5813 - val_loss: 749384595734.2515\n",
      "Epoch 862/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 713924630791.9235 - val_loss: 747766845653.1533\n",
      "Epoch 863/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 718108188035.8176 - val_loss: 745702457562.9142\n",
      "Epoch 864/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 710173948384.5942 - val_loss: 755043810332.5165\n",
      "Epoch 865/1000\n",
      "3554/3554 [==============================] - 0s 65us/step - loss: 709821078072.1846 - val_loss: 744065990637.5651\n",
      "Epoch 866/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 708670439931.1018 - val_loss: 745555066174.8658\n",
      "Epoch 867/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 708956401546.4446 - val_loss: 745756054779.4633\n",
      "Epoch 868/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 708640350479.9910 - val_loss: 744078534739.5331\n",
      "Epoch 869/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 708421608830.0552 - val_loss: 745278262216.4073\n",
      "Epoch 870/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 708733398544.4232 - val_loss: 747606605505.9983\n",
      "Epoch 871/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 706863247353.0850 - val_loss: 745947535221.1624\n",
      "Epoch 872/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 712148822240.7383 - val_loss: 742908118840.0968\n",
      "Epoch 873/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 708040910132.2948 - val_loss: 783417141799.3181\n",
      "Epoch 874/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 710033053360.6212 - val_loss: 742810434169.1229\n",
      "Epoch 875/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 707218158151.1672 - val_loss: 756123692239.1044\n",
      "Epoch 876/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 706979967042.8452 - val_loss: 748954702500.3297\n",
      "Epoch 877/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 709574285428.4030 - val_loss: 743155944069.7969\n",
      "Epoch 878/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 705598682952.7518 - val_loss: 741930878937.4020\n",
      "Epoch 879/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 708995636054.0056 - val_loss: 742767911760.0045\n",
      "Epoch 880/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 705544509611.7231 - val_loss: 758502248861.6327\n",
      "Epoch 881/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 73us/step - loss: 706891632019.9528 - val_loss: 741367973167.3114\n",
      "Epoch 882/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 713022294116.2678 - val_loss: 745468109682.5699\n",
      "Epoch 883/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 704108421124.6100 - val_loss: 740746026524.0844\n",
      "Epoch 884/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 706125117542.5729 - val_loss: 742174134775.5027\n",
      "Epoch 885/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 707863100047.7749 - val_loss: 741179414131.0740\n",
      "Epoch 886/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 703577768018.9803 - val_loss: 755231058194.7949\n",
      "Epoch 887/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 709499264622.9286 - val_loss: 746833469333.9994\n",
      "Epoch 888/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 709141076628.3850 - val_loss: 742203851639.1786\n",
      "Epoch 889/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 708111646637.0197 - val_loss: 758422289565.5607\n",
      "Epoch 890/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 705843975755.2009 - val_loss: 742800347413.6754\n",
      "Epoch 891/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 703549452078.2443 - val_loss: 755690202074.2661\n",
      "Epoch 892/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 705328245311.0996 - val_loss: 738644172341.4324\n",
      "Epoch 893/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 704207547173.6003 - val_loss: 741648690634.2797\n",
      "Epoch 894/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 704532018463.5498 - val_loss: 739470523849.1274\n",
      "Epoch 895/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 707960660650.2825 - val_loss: 751858020099.6726\n",
      "Epoch 896/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 703200595283.4126 - val_loss: 746555991064.4838\n",
      "Epoch 897/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 707655674273.2065 - val_loss: 739937331306.5767\n",
      "Epoch 898/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 703583554318.5503 - val_loss: 745942620733.2096\n",
      "Epoch 899/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 706725365473.0265 - val_loss: 742147306611.5061\n",
      "Epoch 900/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 701339646073.0129 - val_loss: 742482142559.7030\n",
      "Epoch 901/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 703377807965.0647 - val_loss: 739174627878.1660\n",
      "Epoch 902/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 700138681262.7484 - val_loss: 738792951249.1926\n",
      "Epoch 903/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 702423186967.3381 - val_loss: 756164019185.3097\n",
      "Epoch 904/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 707064708778.8586 - val_loss: 757377910238.4427\n",
      "Epoch 905/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 705546417365.5015 - val_loss: 737424809573.8239\n",
      "Epoch 906/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 704806661602.3230 - val_loss: 746820796222.4337\n",
      "Epoch 907/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 700481358901.5914 - val_loss: 740890264164.9597\n",
      "Epoch 908/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 705457196998.3748 - val_loss: 755856658105.9331\n",
      "Epoch 909/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 701738540673.3685 - val_loss: 745240538286.8433\n",
      "Epoch 910/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 702028290984.9860 - val_loss: 737352867835.3912\n",
      "Epoch 911/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 701228986268.8846 - val_loss: 735470284904.8484\n",
      "Epoch 912/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 701127519151.9010 - val_loss: 736653712624.5176\n",
      "Epoch 913/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 699421482809.7693 - val_loss: 753181345336.3127\n",
      "Epoch 914/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 701155614991.9910 - val_loss: 741513744263.0211\n",
      "Epoch 915/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 702327982927.0906 - val_loss: 738594805194.8557\n",
      "Epoch 916/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 700944920715.4530 - val_loss: 738599492302.3843\n",
      "Epoch 917/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 703519161095.6354 - val_loss: 734663457772.1249\n",
      "Epoch 918/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 697700812374.7260 - val_loss: 734868604589.8352\n",
      "Epoch 919/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 701170707191.5002 - val_loss: 745761677968.7427\n",
      "Epoch 920/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 699077737784.9049 - val_loss: 735190420650.2346\n",
      "Epoch 921/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 698676571074.3409 - val_loss: 747757449933.8081\n",
      "Epoch 922/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 704655254407.5632 - val_loss: 744103452985.3929\n",
      "Epoch 923/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 697891663662.2443 - val_loss: 736502252063.8290\n",
      "Epoch 924/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 702244436448.5942 - val_loss: 737929812306.1648\n",
      "Epoch 925/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 700766664732.8126 - val_loss: 734129513858.2683\n",
      "Epoch 926/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 701241962341.5645 - val_loss: 732113967635.7311\n",
      "Epoch 927/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 699756938267.0839 - val_loss: 741641703618.7184\n",
      "Epoch 928/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 701494755309.5599 - val_loss: 733092568995.8256\n",
      "Epoch 929/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 701321005893.8705 - val_loss: 758107679582.4067\n",
      "Epoch 930/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 697717634713.5712 - val_loss: 731799954647.7457\n",
      "Epoch 931/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 697369637852.8485 - val_loss: 734512144236.5210\n",
      "Epoch 932/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 699543236767.0455 - val_loss: 732282088390.1029\n",
      "Epoch 933/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 696204928889.1571 - val_loss: 735044007027.5061\n",
      "Epoch 934/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 694989863162.0934 - val_loss: 738109550593.1522\n",
      "Epoch 935/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 698552165306.2734 - val_loss: 731384017565.4166\n",
      "Epoch 936/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 697316079206.8610 - val_loss: 737027398785.9083\n",
      "Epoch 937/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 695921411200.5043 - val_loss: 740532191434.7837\n",
      "Epoch 938/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 696036129580.5155 - val_loss: 737798186600.7043\n",
      "Epoch 939/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 696616363371.6150 - val_loss: 745936110116.4377\n",
      "Epoch 940/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 697567494448.2611 - val_loss: 748604178328.3038\n",
      "Epoch 941/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 698009538364.0742 - val_loss: 758046777883.2202\n",
      "Epoch 942/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 697947748657.4137 - val_loss: 741875778994.6599\n",
      "Epoch 943/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 693520930690.3771 - val_loss: 754330648110.5193\n",
      "Epoch 944/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 696138578617.2650 - val_loss: 738788550792.5333\n",
      "Epoch 945/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 694764414103.5543 - val_loss: 737169384004.6987\n",
      "Epoch 946/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 695655555178.0304 - val_loss: 731244109284.7798\n",
      "Epoch 947/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 696591698315.3090 - val_loss: 731634168067.2405\n",
      "Epoch 948/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 694442730758.1947 - val_loss: 735584859968.1620\n",
      "Epoch 949/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 693946003873.2065 - val_loss: 731433915952.2476\n",
      "Epoch 950/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 695070718303.5138 - val_loss: 757645192048.8417\n",
      "Epoch 951/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 694515397882.6697 - val_loss: 792877854474.5857\n",
      "Epoch 952/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 698667172614.4828 - val_loss: 734660242367.1898\n",
      "Epoch 953/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 693091187159.3741 - val_loss: 734658157213.9927\n",
      "Epoch 954/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 693878132254.2532 - val_loss: 728406161951.2529\n",
      "Epoch 955/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 689899154176.7203 - val_loss: 730155611486.5508\n",
      "Epoch 956/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 692804054452.2229 - val_loss: 739989194141.9207\n",
      "Epoch 957/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 695841386387.6647 - val_loss: 735292797120.4141\n",
      "Epoch 958/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 692151938991.3247 - val_loss: 746141301682.5159\n",
      "Epoch 959/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 698123217139.1785 - val_loss: 736789219098.1401\n",
      "Epoch 960/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 695902898396.1284 - val_loss: 782384997723.0942\n",
      "Epoch 961/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 694170299192.0405 - val_loss: 728131904107.8728\n",
      "Epoch 962/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 694159044997.5464 - val_loss: 728230106997.1624\n",
      "Epoch 963/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 691795706713.4631 - val_loss: 727552681712.6616\n",
      "Epoch 964/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 692483125969.4677 - val_loss: 728785393366.1615\n",
      "Epoch 965/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 694759460106.8047 - val_loss: 730421405797.9679\n",
      "Epoch 966/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 689091568021.6815 - val_loss: 727582389358.8973\n",
      "Epoch 967/1000\n",
      "3554/3554 [==============================] - 1s 147us/step - loss: 692889774890.2104 - val_loss: 740168437509.9769\n",
      "Epoch 968/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 694898866140.2723 - val_loss: 726919090879.9821\n",
      "Epoch 969/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 692191908197.2762 - val_loss: 726949965862.0220\n",
      "Epoch 970/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 696072047840.1621 - val_loss: 729473498126.6903\n",
      "Epoch 971/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 690757572217.3010 - val_loss: 741865740771.3395\n",
      "Epoch 972/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 689547643026.3679 - val_loss: 739276424785.6608\n",
      "Epoch 973/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 699071098919.1852 - val_loss: 728256628289.5303\n",
      "Epoch 974/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 690591719963.3719 - val_loss: 727654810772.9193\n",
      "Epoch 975/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 690579021911.5902 - val_loss: 731040705220.0146\n",
      "Epoch 976/1000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 692562507048.7698 - val_loss: 732038985787.3373\n",
      "Epoch 977/1000\n",
      "3554/3554 [==============================] - 1s 141us/step - loss: 692098382276.9342 - val_loss: 733738138332.2104\n",
      "Epoch 978/1000\n",
      "3554/3554 [==============================] - 1s 164us/step - loss: 686593447047.4193 - val_loss: 728735997587.9111\n",
      "Epoch 979/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 693571462124.9836 - val_loss: 729575366925.6101\n",
      "Epoch 980/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 691587876789.0872 - val_loss: 728348933219.9517\n",
      "Epoch 981/1000\n",
      "3554/3554 [==============================] - 1s 161us/step - loss: 691479918344.2117 - val_loss: 725802096305.2917\n",
      "Epoch 982/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 691192038390.2037 - val_loss: 728790085458.0209\n",
      "Epoch 983/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 687288979094.1136 - val_loss: 728852715056.2476\n",
      "Epoch 984/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 688372055178.8768 - val_loss: 733342534655.7119\n",
      "Epoch 985/1000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 693694820575.5858 - val_loss: 755212074517.1713\n",
      "Epoch 986/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 690933450630.9871 - val_loss: 738525632707.2946\n",
      "Epoch 987/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 690374006863.5228 - val_loss: 727871918696.4163\n",
      "Epoch 988/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 687203643523.3855 - val_loss: 723808350024.8033\n",
      "Epoch 989/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 688114180672.2521 - val_loss: 732204191440.9767\n",
      "Epoch 990/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 689070130931.4666 - val_loss: 726980712993.5573\n",
      "Epoch 991/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 698043800101.7445 - val_loss: 724046376352.8011\n",
      "Epoch 992/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 689977752216.9950 - val_loss: 730299721071.8335\n",
      "Epoch 993/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 689314349876.5830 - val_loss: 723649646775.1967\n",
      "Epoch 994/1000\n",
      "3554/3554 [==============================] - 1s 145us/step - loss: 685076684260.6281 - val_loss: 728071305171.6411\n",
      "Epoch 995/1000\n",
      "3554/3554 [==============================] - 0s 138us/step - loss: 688525495865.9133 - val_loss: 730601307692.2150\n",
      "Epoch 996/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 694446874610.1700 - val_loss: 726495769678.6362\n",
      "Epoch 997/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 686195566770.6382 - val_loss: 724660692688.4005\n",
      "Epoch 998/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 688548836270.7484 - val_loss: 744847507485.9567\n",
      "Epoch 999/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 687148565549.5239 - val_loss: 723813076940.1519\n",
      "Epoch 1000/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 683449689884.9567 - val_loss: 723737742211.2765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09fa84aba8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model22.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(\"model22 loss:\",model22.loss)\n",
    "\n",
    "model_train22 = model22.fit(predictors,target, epochs=1000, validation_split=0.5, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model33 loss: mean_squared_error\n",
      "Train on 3554 samples, validate on 3555 samples\n",
      "Epoch 1/1000\n",
      "3554/3554 [==============================] - 1s 166us/step - loss: 131146985535599.7812 - val_loss: 123269451087724.5156\n",
      "Epoch 2/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 107353625783289.6719 - val_loss: 83431910072419.0938\n",
      "Epoch 3/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 59655186075064.8359 - val_loss: 34897331161602.4492\n",
      "Epoch 4/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 22357837083050.4258 - val_loss: 12427679661231.9961\n",
      "Epoch 5/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 10297644014661.7246 - val_loss: 8298259047780.3115\n",
      "Epoch 6/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 8292149668866.8809 - val_loss: 7665499389016.4297\n",
      "Epoch 7/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 7776494958454.8516 - val_loss: 7378635972651.7832\n",
      "Epoch 8/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 7470617067446.2393 - val_loss: 7120656260500.1270\n",
      "Epoch 9/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 7207557289879.1230 - val_loss: 6907396275740.0840\n",
      "Epoch 10/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 6983931811513.8418 - val_loss: 6721105777157.3291\n",
      "Epoch 11/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 6786445296897.5850 - val_loss: 6562684371985.5703\n",
      "Epoch 12/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 6612944417471.0283 - val_loss: 6413223577999.2305\n",
      "Epoch 13/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 6455344002141.9287 - val_loss: 6286438095040.9902\n",
      "Epoch 14/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 6312120663114.3359 - val_loss: 6179189431170.1240\n",
      "Epoch 15/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 6179973879453.0293 - val_loss: 6058800056041.7480\n",
      "Epoch 16/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 6059717097860.3936 - val_loss: 5964444016245.9541\n",
      "Epoch 17/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 5950955487287.3203 - val_loss: 5871706132223.0635\n",
      "Epoch 18/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 5846697732947.7012 - val_loss: 5769967401634.6016\n",
      "Epoch 19/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 5751065709206.6895 - val_loss: 5690872925015.4941\n",
      "Epoch 20/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 5663004848517.5469 - val_loss: 5620747761509.0322\n",
      "Epoch 21/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 5581496869919.6943 - val_loss: 5547313264162.4209\n",
      "Epoch 22/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 5503466847380.0967 - val_loss: 5479383450065.4805\n",
      "Epoch 23/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 5431296832034.2871 - val_loss: 5418920513956.5459\n",
      "Epoch 24/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 5362406295550.8467 - val_loss: 5362262844223.8740\n",
      "Epoch 25/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 5298369308334.3164 - val_loss: 5301562304738.4033\n",
      "Epoch 26/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 5236535950198.2754 - val_loss: 5256524108261.0674\n",
      "Epoch 27/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 5178097665917.7676 - val_loss: 5206074356498.9385\n",
      "Epoch 28/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 5122922213654.9053 - val_loss: 5156973909457.7686\n",
      "Epoch 29/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 5070470083060.1865 - val_loss: 5109649953644.8086\n",
      "Epoch 30/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 5020318521312.8828 - val_loss: 5068294566273.9805\n",
      "Epoch 31/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 4971964545218.7734 - val_loss: 5026086529771.1885\n",
      "Epoch 32/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 4926873526350.3701 - val_loss: 4986794377944.1777\n",
      "Epoch 33/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 4881375572956.8486 - val_loss: 4968134826149.3379\n",
      "Epoch 34/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 4841680155391.5684 - val_loss: 4923374443320.3848\n",
      "Epoch 35/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 4801725257899.7236 - val_loss: 4885367973570.5742\n",
      "Epoch 36/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 4763401178515.9531 - val_loss: 4851268843355.8145\n",
      "Epoch 37/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 4727303326294.1494 - val_loss: 4815157665508.5635\n",
      "Epoch 38/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 4691461976444.3262 - val_loss: 4787928355766.5488\n",
      "Epoch 39/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 4657359275357.7852 - val_loss: 4756419499211.0713\n",
      "Epoch 40/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 4624952540769.0986 - val_loss: 4726162121724.8311\n",
      "Epoch 41/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 4593274290637.0020 - val_loss: 4697200962587.3643\n",
      "Epoch 42/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 4563326680673.0986 - val_loss: 4675803493171.2002\n",
      "Epoch 43/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 4533188407491.9258 - val_loss: 4644131337937.5527\n",
      "Epoch 44/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 4505931774432.5947 - val_loss: 4622988378305.5664\n",
      "Epoch 45/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 4476545901000.3916 - val_loss: 4610107242626.7725\n",
      "Epoch 46/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 4450516763785.7246 - val_loss: 4582612208462.2764\n",
      "Epoch 47/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 4426197456311.6807 - val_loss: 4553062926053.1396\n",
      "Epoch 48/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 4401115095229.0107 - val_loss: 4532953798793.6855\n",
      "Epoch 49/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 4377633262696.3013 - val_loss: 4515471883089.7324\n",
      "Epoch 50/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 4355205674668.0112 - val_loss: 4502519258824.9111\n",
      "Epoch 51/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 4332154040626.5664 - val_loss: 4476521443709.3721\n",
      "Epoch 52/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 4310684070664.7881 - val_loss: 4454886437945.8975\n",
      "Epoch 53/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 4290035469476.2319 - val_loss: 4439563263813.3467\n",
      "Epoch 54/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 4270144656408.7788 - val_loss: 4418268267297.9170\n",
      "Epoch 55/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 4250330305671.4189 - val_loss: 4403678955227.9219\n",
      "Epoch 56/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 4231661492644.6641 - val_loss: 4392605002530.4932\n",
      "Epoch 57/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 4213451991707.2998 - val_loss: 4375084670305.4312\n",
      "Epoch 58/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 4195092121147.0659 - val_loss: 4357344347425.4854\n",
      "Epoch 59/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 4178061240295.7974 - val_loss: 4349992790638.7534\n",
      "Epoch 60/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 4160975138117.5825 - val_loss: 4335383232571.9136\n",
      "Epoch 61/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 4143899568168.3374 - val_loss: 4308970441845.2344\n",
      "Epoch 62/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4126824644064.0181 - val_loss: 4294773952927.9370\n",
      "Epoch 63/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 4111323598310.9331 - val_loss: 4282249541587.6411\n",
      "Epoch 64/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 4095815133159.7974 - val_loss: 4276226347866.3740\n",
      "Epoch 65/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 4081087697142.6362 - val_loss: 4259665294444.5928\n",
      "Epoch 66/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 4065578444158.0552 - val_loss: 4239721521536.8281\n",
      "Epoch 67/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 4051197564426.6606 - val_loss: 4227678878476.6021\n",
      "Epoch 68/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 4038704450880.9727 - val_loss: 4217598909904.0405\n",
      "Epoch 69/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 4025036984253.1548 - val_loss: 4206579937025.3682\n",
      "Epoch 70/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 4010985400885.8799 - val_loss: 4194910235431.1021\n",
      "Epoch 71/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 3998132100442.3271 - val_loss: 4181325037010.3447\n",
      "Epoch 72/1000\n",
      "3554/3554 [==============================] - 0s 132us/step - loss: 3985164917276.5249 - val_loss: 4176795980247.2417\n",
      "Epoch 73/1000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 3972504876116.7090 - val_loss: 4161156291052.5571\n",
      "Epoch 74/1000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 3960677002086.1406 - val_loss: 4155564210136.2500\n",
      "Epoch 75/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3948279913191.3652 - val_loss: 4141120987341.0879\n",
      "Epoch 76/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 3937179890311.7075 - val_loss: 4133648249568.2432\n",
      "Epoch 77/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 3924700651616.8105 - val_loss: 4116518109543.4800\n",
      "Epoch 78/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 3913451995136.0000 - val_loss: 4110900552249.1768\n",
      "Epoch 79/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 3902618747755.9033 - val_loss: 4099712353104.0044\n",
      "Epoch 80/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3891678244774.1045 - val_loss: 4103038295517.8667\n",
      "Epoch 81/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3881461237177.9854 - val_loss: 4081428165302.7646\n",
      "Epoch 82/1000\n",
      "3554/3554 [==============================] - 0s 137us/step - loss: 3870282729793.5488 - val_loss: 4071645186968.0156\n",
      "Epoch 83/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 3859727002276.5200 - val_loss: 4065209478544.6704\n",
      "Epoch 84/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 3849342179112.4814 - val_loss: 4051187100621.0161\n",
      "Epoch 85/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 3839489875520.8286 - val_loss: 4047625636054.8813\n",
      "Epoch 86/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3829680958051.9800 - val_loss: 4033604176077.6641\n",
      "Epoch 87/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 3819911124638.1812 - val_loss: 4025273501901.6641\n",
      "Epoch 88/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 3810691710939.1196 - val_loss: 4014874414008.2769\n",
      "Epoch 89/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 3801715054740.6733 - val_loss: 4007836668476.6333\n",
      "Epoch 90/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 3792391793676.1011 - val_loss: 4003192637530.4463\n",
      "Epoch 91/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 3783367140872.3555 - val_loss: 3992574482864.6436\n",
      "Epoch 92/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 3774075703293.6953 - val_loss: 3985048739869.9565\n",
      "Epoch 93/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 3765007822066.6025 - val_loss: 3971794557535.7749\n",
      "Epoch 94/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 3756997509249.0806 - val_loss: 3966714069439.0459\n",
      "Epoch 95/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 3748956337052.3081 - val_loss: 3959498912738.3315\n",
      "Epoch 96/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3740336727228.4341 - val_loss: 3954664584494.1592\n",
      "Epoch 97/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 3731750469272.4185 - val_loss: 3945756981811.9922\n",
      "Epoch 98/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3723896980839.0049 - val_loss: 3937779390680.0337\n",
      "Epoch 99/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 3715672376881.8457 - val_loss: 3929237509774.7261\n",
      "Epoch 100/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 3708377740640.0898 - val_loss: 3920267976365.2593\n",
      "Epoch 101/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 3700721076453.3481 - val_loss: 3917321790239.6128\n",
      "Epoch 102/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 3692889356279.9326 - val_loss: 3913461182647.1968\n",
      "Epoch 103/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3685473420536.3643 - val_loss: 3906000919389.2544\n",
      "Epoch 104/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 3677955087540.3667 - val_loss: 3900984314716.1025\n",
      "Epoch 105/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3670622527750.7710 - val_loss: 3887713650935.1426\n",
      "Epoch 106/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3663393572669.2266 - val_loss: 3880152762873.2310\n",
      "Epoch 107/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 3656670849663.6401 - val_loss: 3876658834196.6670\n",
      "Epoch 108/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 3649297403966.2349 - val_loss: 3872288593891.7715\n",
      "Epoch 109/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3642606098557.6230 - val_loss: 3869206194867.8843\n",
      "Epoch 110/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3635714861216.1978 - val_loss: 3861818337626.2300\n",
      "Epoch 111/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3628487585360.9634 - val_loss: 3852025193634.7456\n",
      "Epoch 112/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 3621587307370.7510 - val_loss: 3844653154552.8711\n",
      "Epoch 113/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 3615096723424.8823 - val_loss: 3836120090534.7061\n",
      "Epoch 114/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 3608635666650.9756 - val_loss: 3834443419427.9336\n",
      "Epoch 115/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 3602179789221.8164 - val_loss: 3824466518760.3081\n",
      "Epoch 116/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 3596005997079.9146 - val_loss: 3819150681467.3555\n",
      "Epoch 117/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 3589971482325.5015 - val_loss: 3812249394460.3003\n",
      "Epoch 118/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3583534751047.8877 - val_loss: 3814701578734.8613\n",
      "Epoch 119/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 3577263140543.6040 - val_loss: 3806807049275.0493\n",
      "Epoch 120/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3570731610354.0259 - val_loss: 3799612710568.6504\n",
      "Epoch 121/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 3565021934722.8096 - val_loss: 3795350503382.5215\n",
      "Epoch 122/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 3558893013239.7886 - val_loss: 3787696381281.4312\n",
      "Epoch 123/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3553366809190.2847 - val_loss: 3781794933759.7119\n",
      "Epoch 124/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3547319654516.4028 - val_loss: 3774073831101.3896\n",
      "Epoch 125/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 3541690811511.2842 - val_loss: 3767744966083.3667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 3536026848291.1514 - val_loss: 3761601824743.2280\n",
      "Epoch 127/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 3530576971820.9478 - val_loss: 3757954730157.4033\n",
      "Epoch 128/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 3524611472190.9556 - val_loss: 3751190034922.2524\n",
      "Epoch 129/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3519443016819.8267 - val_loss: 3745706793798.2109\n",
      "Epoch 130/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3513974855842.5034 - val_loss: 3741074032389.4009\n",
      "Epoch 131/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 3508382066414.8564 - val_loss: 3737140453862.2202\n",
      "Epoch 132/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3502822708534.6001 - val_loss: 3733711153673.3613\n",
      "Epoch 133/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3497604914107.4258 - val_loss: 3733838022963.0562\n",
      "Epoch 134/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 3492323228829.3164 - val_loss: 3720304495853.0610\n",
      "Epoch 135/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3487015084851.4302 - val_loss: 3717620686670.5645\n",
      "Epoch 136/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3481514151896.8149 - val_loss: 3714499476619.1260\n",
      "Epoch 137/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3476765742934.5820 - val_loss: 3711672133309.9658\n",
      "Epoch 138/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 3471235049881.1392 - val_loss: 3709570084858.2393\n",
      "Epoch 139/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 3466225721007.4683 - val_loss: 3696546908093.7495\n",
      "Epoch 140/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 3461752288820.1509 - val_loss: 3698529361364.6494\n",
      "Epoch 141/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 3456017390643.2861 - val_loss: 3691770742929.7510\n",
      "Epoch 142/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 3451247966605.6143 - val_loss: 3687048870258.4258\n",
      "Epoch 143/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3446282304254.4150 - val_loss: 3689779344612.4199\n",
      "Epoch 144/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 3441565379842.7373 - val_loss: 3677241121933.4302\n",
      "Epoch 145/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 3436357585521.8101 - val_loss: 3670206433273.6631\n",
      "Epoch 146/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3431554182783.0635 - val_loss: 3663662762890.1895\n",
      "Epoch 147/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 3427607373698.3770 - val_loss: 3661633623505.7686\n",
      "Epoch 148/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3422581980722.9985 - val_loss: 3657763071032.7446\n",
      "Epoch 149/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3417718166405.2583 - val_loss: 3652005090534.7241\n",
      "Epoch 150/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 3413313096416.4497 - val_loss: 3648364889266.8760\n",
      "Epoch 151/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 3408552883192.5088 - val_loss: 3643223334522.5630\n",
      "Epoch 152/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 3403651258106.3813 - val_loss: 3636058453501.5518\n",
      "Epoch 153/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 3399450247380.0605 - val_loss: 3636815373396.3970\n",
      "Epoch 154/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 3394850440246.7441 - val_loss: 3632576522761.3613\n",
      "Epoch 155/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3390605947546.1475 - val_loss: 3627699599767.2954\n",
      "Epoch 156/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 3385892687823.5952 - val_loss: 3619900112660.9551\n",
      "Epoch 157/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3381442700061.5332 - val_loss: 3615457669270.9355\n",
      "Epoch 158/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3377198496630.2754 - val_loss: 3610096894892.1787\n",
      "Epoch 159/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3372938474294.8882 - val_loss: 3608320325020.7686\n",
      "Epoch 160/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3368440263580.3081 - val_loss: 3604498962929.1655\n",
      "Epoch 161/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 3364096495724.3354 - val_loss: 3602761377741.8804\n",
      "Epoch 162/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 3360028806658.5933 - val_loss: 3602116561682.3628\n",
      "Epoch 163/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 3356082904235.1470 - val_loss: 3594707893230.4292\n",
      "Epoch 164/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 3351483772893.4248 - val_loss: 3588798331854.1685\n",
      "Epoch 165/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 3347046936421.5645 - val_loss: 3585910554656.2612\n",
      "Epoch 166/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 3343248429883.4976 - val_loss: 3586802826614.1704\n",
      "Epoch 167/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3339369056614.4287 - val_loss: 3580619893879.5386\n",
      "Epoch 168/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 3334834199188.3848 - val_loss: 3571432078654.8657\n",
      "Epoch 169/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3330702915225.5713 - val_loss: 3569050267324.2373\n",
      "Epoch 170/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3326622194518.0059 - val_loss: 3563781072785.9668\n",
      "Epoch 171/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 3322661260194.0708 - val_loss: 3558861848525.3042\n",
      "Epoch 172/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 3318725767557.5469 - val_loss: 3557217187007.5498\n",
      "Epoch 173/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3314463100805.8345 - val_loss: 3553918453997.9253\n",
      "Epoch 174/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3310140030293.7178 - val_loss: 3549768208477.3267\n",
      "Epoch 175/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3306610608818.3496 - val_loss: 3541463949567.7842\n",
      "Epoch 176/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 3302605039352.0767 - val_loss: 3537861968157.7407\n",
      "Epoch 177/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3299092320963.6377 - val_loss: 3538793540994.8442\n",
      "Epoch 178/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 3294867955818.0303 - val_loss: 3536470802321.3906\n",
      "Epoch 179/1000\n",
      "3554/3554 [==============================] - 0s 140us/step - loss: 3291041884845.7402 - val_loss: 3529827285085.9028\n",
      "Epoch 180/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 3287000380777.3101 - val_loss: 3523395513394.4077\n",
      "Epoch 181/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3283338944424.4097 - val_loss: 3519230663245.3398\n",
      "Epoch 182/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3279818158727.7075 - val_loss: 3520335830272.6479\n",
      "Epoch 183/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 3275912238796.2812 - val_loss: 3515126265419.8999\n",
      "Epoch 184/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 3271972141896.1753 - val_loss: 3508821128895.1177\n",
      "Epoch 185/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 3268007579894.6362 - val_loss: 3512866595127.0884\n",
      "Epoch 186/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3265219406574.2803 - val_loss: 3502497696783.8423\n",
      "Epoch 187/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 3260483262285.3618 - val_loss: 3497633921399.0347\n",
      "Epoch 188/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3257290337301.3213 - val_loss: 3495344540789.2344\n",
      "Epoch 189/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 3253795907181.7764 - val_loss: 3487161619377.9399\n",
      "Epoch 190/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 3250191459636.2949 - val_loss: 3488287361722.7974\n",
      "Epoch 191/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3246512048348.1284 - val_loss: 3485209938789.3198\n",
      "Epoch 192/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3242485719201.3506 - val_loss: 3481569033456.5176\n",
      "Epoch 193/1000\n",
      "3554/3554 [==============================] - 0s 132us/step - loss: 3238911963968.1079 - val_loss: 3479439941477.6079\n",
      "Epoch 194/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 3235555570256.3867 - val_loss: 3475660756032.8101\n",
      "Epoch 195/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 3231700318872.9951 - val_loss: 3467192276848.5537\n",
      "Epoch 196/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 3228514413506.3408 - val_loss: 3471231074520.6099\n",
      "Epoch 197/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 3224893829260.6060 - val_loss: 3464789885112.3486\n",
      "Epoch 198/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 3221343375726.4961 - val_loss: 3459523344606.9468\n",
      "Epoch 199/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3217689902366.3970 - val_loss: 3454899959989.4683\n",
      "Epoch 200/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 3214453639259.0479 - val_loss: 3449771956062.1187\n",
      "Epoch 201/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 3211052958369.0625 - val_loss: 3450719741270.4854\n",
      "Epoch 202/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3207280309747.6108 - val_loss: 3445160152826.4551\n",
      "Epoch 203/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 3204000842133.6816 - val_loss: 3440329580283.6074\n",
      "Epoch 204/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 3200850545053.7490 - val_loss: 3441119937180.2646\n",
      "Epoch 205/1000\n",
      "3554/3554 [==============================] - 0s 130us/step - loss: 3197275475523.1333 - val_loss: 3435382438793.0376\n",
      "Epoch 206/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 3193825004348.6509 - val_loss: 3433613398132.3701\n",
      "Epoch 207/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 3190554419831.5728 - val_loss: 3428325819888.8774\n",
      "Epoch 208/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3187381539200.9365 - val_loss: 3421222179024.5444\n",
      "Epoch 209/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 3184302098375.5273 - val_loss: 3417965440967.5430\n",
      "Epoch 210/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 3181121801075.9707 - val_loss: 3416230217878.3594\n",
      "Epoch 211/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 3177370141160.6621 - val_loss: 3414931229541.3198\n",
      "Epoch 212/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 3174098307660.9297 - val_loss: 3412142817639.4800\n",
      "Epoch 213/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 3170613865272.0405 - val_loss: 3409673241824.9634\n",
      "Epoch 214/1000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 3167364008866.0708 - val_loss: 3406847355387.5352\n",
      "Epoch 215/1000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 3163968207586.7554 - val_loss: 3403665097492.0913\n",
      "Epoch 216/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 3160753179895.2119 - val_loss: 3398089818826.3516\n",
      "Epoch 217/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3157794987086.9468 - val_loss: 3396600574448.5894\n",
      "Epoch 218/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3154605247366.4106 - val_loss: 3395166119272.9204\n",
      "Epoch 219/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 3151351980433.0713 - val_loss: 3388511596819.3711\n",
      "Epoch 220/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 3148157800786.8364 - val_loss: 3383982105730.4844\n",
      "Epoch 221/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 3145064950344.3193 - val_loss: 3384376615637.8735\n",
      "Epoch 222/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 3141779416390.7349 - val_loss: 3380012120563.4702\n",
      "Epoch 223/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 3138822873894.7529 - val_loss: 3373864682237.0474\n",
      "Epoch 224/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3135682037528.3467 - val_loss: 3370832133931.9990\n",
      "Epoch 225/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 3132823754370.5215 - val_loss: 3368808196944.5806\n",
      "Epoch 226/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 3129419394797.7041 - val_loss: 3365224888556.7729\n",
      "Epoch 227/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3126312842647.4102 - val_loss: 3363776308981.2705\n",
      "Epoch 228/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3123517965919.9458 - val_loss: 3360735493406.6050\n",
      "Epoch 229/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 3120091889073.9180 - val_loss: 3356661804921.1948\n",
      "Epoch 230/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 3116760507949.2358 - val_loss: 3355786906732.0171\n",
      "Epoch 231/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3114097342944.5947 - val_loss: 3351154654199.6465\n",
      "Epoch 232/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 3110754119313.5039 - val_loss: 3348761652527.8877\n",
      "Epoch 233/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 3107928043098.1836 - val_loss: 3348108953602.8804\n",
      "Epoch 234/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3105032247716.6641 - val_loss: 3344452316791.1064\n",
      "Epoch 235/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3101996031810.9893 - val_loss: 3342279303303.9570\n",
      "Epoch 236/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 3098823800344.4902 - val_loss: 3336666811730.4531\n",
      "Epoch 237/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3095633162592.6665 - val_loss: 3329746753222.0308\n",
      "Epoch 238/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3092631064647.4551 - val_loss: 3331062298367.6401\n",
      "Epoch 239/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3089679519778.5752 - val_loss: 3329124749220.9775\n",
      "Epoch 240/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3087191198183.5098 - val_loss: 3322783201226.4238\n",
      "Epoch 241/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3084115623576.4185 - val_loss: 3317384523664.2388\n",
      "Epoch 242/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 3081168590807.6626 - val_loss: 3318312903211.3506\n",
      "Epoch 243/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 3078480697119.2617 - val_loss: 3312451548162.5923\n",
      "Epoch 244/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 3075493864378.2734 - val_loss: 3309538718330.8511\n",
      "Epoch 245/1000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 3072593882548.2227 - val_loss: 3308555369067.2969\n",
      "Epoch 246/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3069596214165.3931 - val_loss: 3308286369798.9131\n",
      "Epoch 247/1000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 3066593280958.8838 - val_loss: 3306175305105.2466\n",
      "Epoch 248/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 3063846584587.3809 - val_loss: 3300294474932.6040\n",
      "Epoch 249/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 3060854558828.3354 - val_loss: 3296312336091.3462\n",
      "Epoch 250/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 79us/step - loss: 3057423944619.2910 - val_loss: 3293849718817.4131\n",
      "Epoch 251/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 3055318969457.5220 - val_loss: 3291976436023.6646\n",
      "Epoch 252/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3052358422073.9136 - val_loss: 3286575208327.5972\n",
      "Epoch 253/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 3049467094573.2358 - val_loss: 3283501948616.9116\n",
      "Epoch 254/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3046774700733.8750 - val_loss: 3280065846731.4316\n",
      "Epoch 255/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 3044466043740.9209 - val_loss: 3277486497840.3916\n",
      "Epoch 256/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3041447442414.7124 - val_loss: 3274583582877.2725\n",
      "Epoch 257/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3038683312397.6860 - val_loss: 3273243742808.2856\n",
      "Epoch 258/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3035512819829.5557 - val_loss: 3269349443628.3589\n",
      "Epoch 259/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3033188095710.7217 - val_loss: 3269560277530.6440\n",
      "Epoch 260/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 3029944501846.7261 - val_loss: 3270724694890.5049\n",
      "Epoch 261/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3027482808373.0151 - val_loss: 3267433318614.8813\n",
      "Epoch 262/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3024796898872.7607 - val_loss: 3262305909470.5146\n",
      "Epoch 263/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 3021763864321.2964 - val_loss: 3260604310423.4395\n",
      "Epoch 264/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 3019213367391.6582 - val_loss: 3255359881008.6074\n",
      "Epoch 265/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3016172116536.1846 - val_loss: 3251902443919.8066\n",
      "Epoch 266/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3013780125411.3311 - val_loss: 3254228862859.0537\n",
      "Epoch 267/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 3011144192772.1777 - val_loss: 3247729282257.4087\n",
      "Epoch 268/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 3008511148806.4824 - val_loss: 3244679418806.5483\n",
      "Epoch 269/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3005594091602.9805 - val_loss: 3240900803073.2964\n",
      "Epoch 270/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 3002732927608.7251 - val_loss: 3240752806710.0806\n",
      "Epoch 271/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 3000003060749.2534 - val_loss: 3233014441224.1372\n",
      "Epoch 272/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2997821622667.3086 - val_loss: 3232609162028.2871\n",
      "Epoch 273/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2995136551231.2437 - val_loss: 3229785918120.3623\n",
      "Epoch 274/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2992539397390.2622 - val_loss: 3225732403956.9824\n",
      "Epoch 275/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2990011821013.6455 - val_loss: 3223572356642.9976\n",
      "Epoch 276/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2987218196301.3618 - val_loss: 3222865011533.7002\n",
      "Epoch 277/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2984536164526.0283 - val_loss: 3217659133214.6050\n",
      "Epoch 278/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2982213918789.1504 - val_loss: 3216773642048.1621\n",
      "Epoch 279/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 2979598856212.7451 - val_loss: 3213522531570.2461\n",
      "Epoch 280/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2976866284020.1865 - val_loss: 3212875822838.4224\n",
      "Epoch 281/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2974395510316.0830 - val_loss: 3209160025914.4014\n",
      "Epoch 282/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2971743845348.9160 - val_loss: 3205596908624.3647\n",
      "Epoch 283/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2968840685483.8672 - val_loss: 3205769977678.5645\n",
      "Epoch 284/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2966856066909.4966 - val_loss: 3202585335268.2036\n",
      "Epoch 285/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2964339588216.4365 - val_loss: 3196591240526.4204\n",
      "Epoch 286/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2961475127009.0264 - val_loss: 3195071184421.3018\n",
      "Epoch 287/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2959096157522.2598 - val_loss: 3191075102321.3457\n",
      "Epoch 288/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2956737785074.6025 - val_loss: 3191252894244.4375\n",
      "Epoch 289/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2954061116924.8306 - val_loss: 3189855180449.1611\n",
      "Epoch 290/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2951840668953.7876 - val_loss: 3186214955031.6196\n",
      "Epoch 291/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 2949086195963.2456 - val_loss: 3185883278091.4497\n",
      "Epoch 292/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2946923530386.3677 - val_loss: 3182207783392.4590\n",
      "Epoch 293/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2944420833099.0566 - val_loss: 3178994905271.1968\n",
      "Epoch 294/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2941584840073.5806 - val_loss: 3177884706738.2280\n",
      "Epoch 295/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 2939361482333.6411 - val_loss: 3172820341124.2847\n",
      "Epoch 296/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2936758322193.8638 - val_loss: 3169215577669.5630\n",
      "Epoch 297/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2934264245582.8027 - val_loss: 3168266398968.5830\n",
      "Epoch 298/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2931491578681.1929 - val_loss: 3160430877533.2544\n",
      "Epoch 299/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2929776865420.6060 - val_loss: 3158904108993.2061\n",
      "Epoch 300/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2927308014655.9639 - val_loss: 3157848151814.5532\n",
      "Epoch 301/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 2924639064891.4976 - val_loss: 3156396840742.8140\n",
      "Epoch 302/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2922128460023.7886 - val_loss: 3157882377423.6807\n",
      "Epoch 303/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2919847675045.3843 - val_loss: 3151940689802.4775\n",
      "Epoch 304/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2917176829236.2949 - val_loss: 3147270390073.3931\n",
      "Epoch 305/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2914968749903.6670 - val_loss: 3147541202522.8784\n",
      "Epoch 306/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2912585237761.5850 - val_loss: 3144420321607.7954\n",
      "Epoch 307/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2910201076091.1738 - val_loss: 3142527984226.6553\n",
      "Epoch 308/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2907754694691.7280 - val_loss: 3138495226263.8716\n",
      "Epoch 309/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 2905370341959.1670 - val_loss: 3135789843146.3516\n",
      "Epoch 310/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2903081937913.6616 - val_loss: 3130664821613.9609\n",
      "Epoch 311/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2901204854012.9746 - val_loss: 3130111429214.6226\n",
      "Epoch 312/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2898425100077.0918 - val_loss: 3127918176735.8828\n",
      "Epoch 313/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2895933030722.7012 - val_loss: 3127881741780.0732\n",
      "Epoch 314/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2893275009316.7363 - val_loss: 3126638367163.5894\n",
      "Epoch 315/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2891584540319.3335 - val_loss: 3121395892694.0894\n",
      "Epoch 316/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2888933427742.8296 - val_loss: 3123217675658.3335\n",
      "Epoch 317/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2886698564138.3545 - val_loss: 3118376042555.9136\n",
      "Epoch 318/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2884644933500.6147 - val_loss: 3116020376531.3530\n",
      "Epoch 319/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2882218274975.0454 - val_loss: 3113717956598.7827\n",
      "Epoch 320/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2879898466904.4546 - val_loss: 3108281645924.1675\n",
      "Epoch 321/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2877727179047.0410 - val_loss: 3109241785083.3193\n",
      "Epoch 322/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2875441908485.3301 - val_loss: 3105546539622.1118\n",
      "Epoch 323/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2873130876264.1577 - val_loss: 3103274870139.3555\n",
      "Epoch 324/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2870558254498.9355 - val_loss: 3098522351838.9468\n",
      "Epoch 325/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2868434310039.6987 - val_loss: 3097092573924.2759\n",
      "Epoch 326/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2865973116808.1396 - val_loss: 3090790351106.0884\n",
      "Epoch 327/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2864624413514.4805 - val_loss: 3088142686087.5972\n",
      "Epoch 328/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2862362018446.6226 - val_loss: 3088787272632.8530\n",
      "Epoch 329/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2859626368634.4531 - val_loss: 3085432227036.3545\n",
      "Epoch 330/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2857422745528.5449 - val_loss: 3083538917694.5776\n",
      "Epoch 331/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2855084822378.1743 - val_loss: 3081821764659.5601\n",
      "Epoch 332/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2852711308348.5068 - val_loss: 3081271829132.1338\n",
      "Epoch 333/1000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 2850665793995.8491 - val_loss: 3076693035651.4927\n",
      "Epoch 334/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 2848623326606.1899 - val_loss: 3074876180736.3599\n",
      "Epoch 335/1000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 2846348585027.9976 - val_loss: 3073533568356.3115\n",
      "Epoch 336/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2844288452090.5259 - val_loss: 3071838767798.1885\n",
      "Epoch 337/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2842035864151.3022 - val_loss: 3071909103503.6626\n",
      "Epoch 338/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2839841476716.3354 - val_loss: 3073845084542.5239\n",
      "Epoch 339/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2837969699313.8818 - val_loss: 3067470530996.1001\n",
      "Epoch 340/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2835423625916.7227 - val_loss: 3063491810866.5518\n",
      "Epoch 341/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2833242463845.7085 - val_loss: 3058689188220.2192\n",
      "Epoch 342/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2831113671640.2383 - val_loss: 3056778090951.6870\n",
      "Epoch 343/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2829107375918.2441 - val_loss: 3055354887320.9517\n",
      "Epoch 344/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2826941281964.0112 - val_loss: 3051530975206.0762\n",
      "Epoch 345/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2824753822462.4150 - val_loss: 3050178561036.9619\n",
      "Epoch 346/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2822512792941.9199 - val_loss: 3047654740881.6787\n",
      "Epoch 347/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2820410969851.5342 - val_loss: 3046344743049.6855\n",
      "Epoch 348/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2818263318325.1592 - val_loss: 3043595591175.3452\n",
      "Epoch 349/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2816231766127.7930 - val_loss: 3042065925556.9644\n",
      "Epoch 350/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2813815533763.9258 - val_loss: 3043281297995.6118\n",
      "Epoch 351/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2811918134754.3228 - val_loss: 3037134564139.7109\n",
      "Epoch 352/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2809789421546.1025 - val_loss: 3036130119178.8018\n",
      "Epoch 353/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2807820764067.2231 - val_loss: 3032783682081.5571\n",
      "Epoch 354/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2805820719011.7998 - val_loss: 3030661788775.9844\n",
      "Epoch 355/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2803601318362.8320 - val_loss: 3028949753595.6074\n",
      "Epoch 356/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2801421436106.2642 - val_loss: 3026930437421.5830\n",
      "Epoch 357/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2799410494224.8555 - val_loss: 3025763324845.3311\n",
      "Epoch 358/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2797318114025.6704 - val_loss: 3022489207562.8735\n",
      "Epoch 359/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2795251909926.4644 - val_loss: 3021451600759.1787\n",
      "Epoch 360/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2793318518154.7329 - val_loss: 3017575092566.1973\n",
      "Epoch 361/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 2791235785254.8970 - val_loss: 3014433952012.4580\n",
      "Epoch 362/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2789323837032.0137 - val_loss: 3015228878252.3228\n",
      "Epoch 363/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2787390154146.9355 - val_loss: 3011932817452.3589\n",
      "Epoch 364/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2784927353476.2495 - val_loss: 3010390827600.2207\n",
      "Epoch 365/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2783139082555.7861 - val_loss: 3008064891028.0552\n",
      "Epoch 366/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2781061287251.4121 - val_loss: 3005966364855.1968\n",
      "Epoch 367/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2779080436978.6025 - val_loss: 3004293763174.5439\n",
      "Epoch 368/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2777218218261.7534 - val_loss: 3001461372985.0327\n",
      "Epoch 369/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2775206013198.8384 - val_loss: 2998111201865.8833\n",
      "Epoch 370/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2772687529332.8354 - val_loss: 2997498309562.5811\n",
      "Epoch 371/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2770947870570.1743 - val_loss: 2993325740360.3711\n",
      "Epoch 372/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2769168556331.6514 - val_loss: 2992419864271.8247\n",
      "Epoch 373/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2767136440900.8623 - val_loss: 2989085730908.7505\n",
      "Epoch 374/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 92us/step - loss: 2765273123008.4678 - val_loss: 2988343087206.5439\n",
      "Epoch 375/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2763220914321.2153 - val_loss: 2990934711515.7783\n",
      "Epoch 376/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2761371611728.3867 - val_loss: 2988258689226.7837\n",
      "Epoch 377/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2759519272849.9360 - val_loss: 2984333629015.4219\n",
      "Epoch 378/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2757474601447.5098 - val_loss: 2982083895344.3916\n",
      "Epoch 379/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2755337640587.1646 - val_loss: 2977965830635.9810\n",
      "Epoch 380/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2753414923512.3643 - val_loss: 2977425280601.1499\n",
      "Epoch 381/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2751576185455.5049 - val_loss: 2972844506882.5205\n",
      "Epoch 382/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2749613024591.9551 - val_loss: 2971812353595.7695\n",
      "Epoch 383/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2747578599229.2266 - val_loss: 2971341670356.7935\n",
      "Epoch 384/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2745790781261.3618 - val_loss: 2966314546593.3774\n",
      "Epoch 385/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 2743736407857.7021 - val_loss: 2961686189314.3765\n",
      "Epoch 386/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2742341821877.3750 - val_loss: 2962168929997.5200\n",
      "Epoch 387/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2739914522411.9395 - val_loss: 2960327763383.2686\n",
      "Epoch 388/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2738053929334.5640 - val_loss: 2957190278355.7129\n",
      "Epoch 389/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2736311216859.8403 - val_loss: 2956497571721.3257\n",
      "Epoch 390/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2734295771065.6973 - val_loss: 2955797320169.6763\n",
      "Epoch 391/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2732220186494.9194 - val_loss: 2956569336830.5596\n",
      "Epoch 392/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2730404248512.0361 - val_loss: 2953159898492.5073\n",
      "Epoch 393/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2728460047792.3330 - val_loss: 2951398432687.3472\n",
      "Epoch 394/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2726524727104.6846 - val_loss: 2947879084633.4380\n",
      "Epoch 395/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2724892780777.9585 - val_loss: 2944226250421.3242\n",
      "Epoch 396/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2722794180347.5342 - val_loss: 2942879453728.9810\n",
      "Epoch 397/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2720961204959.2974 - val_loss: 2944008576035.4297\n",
      "Epoch 398/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2719214320419.2954 - val_loss: 2938250979683.4478\n",
      "Epoch 399/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2717220672960.9004 - val_loss: 2939505417882.8242\n",
      "Epoch 400/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2715654750392.4004 - val_loss: 2936741788617.5596\n",
      "Epoch 401/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2713763569816.1304 - val_loss: 2935800984071.6333\n",
      "Epoch 402/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2711702185498.7959 - val_loss: 2936275218937.8071\n",
      "Epoch 403/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2710167620928.9727 - val_loss: 2933540629291.7109\n",
      "Epoch 404/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2708316848589.0015 - val_loss: 2929091662719.2437\n",
      "Epoch 405/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2706259032793.5356 - val_loss: 2926568465921.0083\n",
      "Epoch 406/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2704540653426.2417 - val_loss: 2924490660811.8638\n",
      "Epoch 407/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 2702505675211.8491 - val_loss: 2923203272254.6499\n",
      "Epoch 408/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2700844085868.0474 - val_loss: 2921508984370.5518\n",
      "Epoch 409/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2699006558069.6997 - val_loss: 2920150737921.4404\n",
      "Epoch 410/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2697356373096.3013 - val_loss: 2916963755461.0947\n",
      "Epoch 411/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2695555514331.1196 - val_loss: 2911856094608.6704\n",
      "Epoch 412/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 2694049192587.1646 - val_loss: 2911881352164.6357\n",
      "Epoch 413/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 2692150469562.2734 - val_loss: 2912571379915.9360\n",
      "Epoch 414/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 2690387430020.2495 - val_loss: 2910311944114.8037\n",
      "Epoch 415/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2688517795362.8633 - val_loss: 2907944769415.5972\n",
      "Epoch 416/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2686839217774.9287 - val_loss: 2905465564989.2817\n",
      "Epoch 417/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2684969276099.6377 - val_loss: 2902711131189.8643\n",
      "Epoch 418/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2682933247686.5190 - val_loss: 2901217686240.2432\n",
      "Epoch 419/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2681379119090.1699 - val_loss: 2895747539116.8271\n",
      "Epoch 420/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2679798620459.6514 - val_loss: 2896664414303.0547\n",
      "Epoch 421/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2678004796801.5127 - val_loss: 2895252807355.0854\n",
      "Epoch 422/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2676102856667.1196 - val_loss: 2891803357325.7183\n",
      "Epoch 423/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2674479148680.2837 - val_loss: 2890057404767.4150\n",
      "Epoch 424/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2672741486993.0713 - val_loss: 2889070017380.1675\n",
      "Epoch 425/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2671033457945.7876 - val_loss: 2887877205474.4756\n",
      "Epoch 426/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2669338874969.3188 - val_loss: 2888665015002.4819\n",
      "Epoch 427/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2667293316212.9790 - val_loss: 2883597322998.7104\n",
      "Epoch 428/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2665439064010.9849 - val_loss: 2882537206726.6792\n",
      "Epoch 429/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2663870526198.3477 - val_loss: 2880847262505.9824\n",
      "Epoch 430/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2662047637305.7695 - val_loss: 2880237939896.0605\n",
      "Epoch 431/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2660444290398.9375 - val_loss: 2875089452235.6479\n",
      "Epoch 432/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2658963238462.5234 - val_loss: 2876541306730.7925\n",
      "Epoch 433/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2657354879840.3779 - val_loss: 2875270049394.4980\n",
      "Epoch 434/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 2655658260269.0918 - val_loss: 2874432324220.0034\n",
      "Epoch 435/1000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 2654051052196.5200 - val_loss: 2871416430833.9580\n",
      "Epoch 436/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2652374005466.1113 - val_loss: 2866687729809.1748\n",
      "Epoch 437/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2650532417054.2529 - val_loss: 2866134085515.6299\n",
      "Epoch 438/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2648719079932.2544 - val_loss: 2866402123073.1704\n",
      "Epoch 439/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2647158845960.9321 - val_loss: 2863756441352.8574\n",
      "Epoch 440/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 2645549606509.7764 - val_loss: 2860845818250.0454\n",
      "Epoch 441/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2643825113264.3330 - val_loss: 2859265351373.5200\n",
      "Epoch 442/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2642050810298.5620 - val_loss: 2857333817159.6514\n",
      "Epoch 443/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2640494553902.8208 - val_loss: 2855191231843.7354\n",
      "Epoch 444/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2638726087070.9014 - val_loss: 2850641690388.9551\n",
      "Epoch 445/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2637179023841.7471 - val_loss: 2851068484417.6021\n",
      "Epoch 446/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2635478950648.6528 - val_loss: 2850924933810.1558\n",
      "Epoch 447/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2633723936472.9590 - val_loss: 2848133006339.7446\n",
      "Epoch 448/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 2632276496519.9956 - val_loss: 2846463623241.7397\n",
      "Epoch 449/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 2630762680031.8740 - val_loss: 2844590020897.7734\n",
      "Epoch 450/1000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 2628912195947.0386 - val_loss: 2841232460488.6235\n",
      "Epoch 451/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 2627262442581.8618 - val_loss: 2840508866359.5205\n",
      "Epoch 452/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2625770815082.3184 - val_loss: 2839130652575.2168\n",
      "Epoch 453/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2624127035893.3394 - val_loss: 2836654134553.4199\n",
      "Epoch 454/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2622501615045.5107 - val_loss: 2834868495458.2231\n",
      "Epoch 455/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2620830807438.1899 - val_loss: 2831119906679.4668\n",
      "Epoch 456/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2619365676250.3994 - val_loss: 2830909461900.6382\n",
      "Epoch 457/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2617842918966.4556 - val_loss: 2828315915202.0703\n",
      "Epoch 458/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2616160719823.0186 - val_loss: 2826518101343.1270\n",
      "Epoch 459/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2614783902728.6440 - val_loss: 2823490996637.0566\n",
      "Epoch 460/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2613287996801.5127 - val_loss: 2823578495363.7085\n",
      "Epoch 461/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 2611475985335.9683 - val_loss: 2820338737711.9595\n",
      "Epoch 462/1000\n",
      "3554/3554 [==============================] - 0s 64us/step - loss: 2610138310972.9385 - val_loss: 2818723115887.1133\n",
      "Epoch 463/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2608442282221.4155 - val_loss: 2819029695175.4712\n",
      "Epoch 464/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2606703634712.0586 - val_loss: 2817936394901.9272\n",
      "Epoch 465/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2604971767836.8125 - val_loss: 2814398349381.1309\n",
      "Epoch 466/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2603527708446.1089 - val_loss: 2814104172674.7725\n",
      "Epoch 467/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2601830467739.5884 - val_loss: 2813601257632.7290\n",
      "Epoch 468/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2600321995791.5591 - val_loss: 2810848736931.4653\n",
      "Epoch 469/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2598805415008.2339 - val_loss: 2810193494287.9146\n",
      "Epoch 470/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2596993034483.1782 - val_loss: 2807195324564.6313\n",
      "Epoch 471/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2595473513873.6479 - val_loss: 2809865550595.3848\n",
      "Epoch 472/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2594261700200.0137 - val_loss: 2807102581741.2769\n",
      "Epoch 473/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2592741905303.6987 - val_loss: 2804046370866.1196\n",
      "Epoch 474/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2590767571612.4521 - val_loss: 2801648169009.5439\n",
      "Epoch 475/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2589468930483.6470 - val_loss: 2800047305965.6372\n",
      "Epoch 476/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2587962482867.7910 - val_loss: 2798354033106.0566\n",
      "Epoch 477/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2586599881276.7949 - val_loss: 2796089929740.6738\n",
      "Epoch 478/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2584958370031.7212 - val_loss: 2794433613986.4575\n",
      "Epoch 479/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2583452393209.2290 - val_loss: 2793150674171.4634\n",
      "Epoch 480/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2581926863154.5664 - val_loss: 2791931878707.3442\n",
      "Epoch 481/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2580389542322.4937 - val_loss: 2790514804453.1396\n",
      "Epoch 482/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2578895587797.0693 - val_loss: 2787733912950.1704\n",
      "Epoch 483/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2577598019105.7109 - val_loss: 2785464198375.8765\n",
      "Epoch 484/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2576003597634.1250 - val_loss: 2785294866514.3809\n",
      "Epoch 485/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2574449691528.7158 - val_loss: 2782823658069.9814\n",
      "Epoch 486/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2572912933003.4526 - val_loss: 2781225243657.5054\n",
      "Epoch 487/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2571463570396.8486 - val_loss: 2780231583396.6177\n",
      "Epoch 488/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2570060468166.3745 - val_loss: 2778919405418.7925\n",
      "Epoch 489/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2568349931802.3633 - val_loss: 2780327180345.0327\n",
      "Epoch 490/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2567250973112.2563 - val_loss: 2775458578229.2163\n",
      "Epoch 491/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2565323027688.2295 - val_loss: 2773797423245.1421\n",
      "Epoch 492/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2563785613685.4111 - val_loss: 2771254437708.8359\n",
      "Epoch 493/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2562469536772.0337 - val_loss: 2770664684314.4282\n",
      "Epoch 494/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2560923306867.9707 - val_loss: 2767671636648.9385\n",
      "Epoch 495/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2559486516256.8467 - val_loss: 2766343938861.4390\n",
      "Epoch 496/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2558185619797.1411 - val_loss: 2765580905591.5386\n",
      "Epoch 497/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2556623987826.6743 - val_loss: 2762609109361.2739\n",
      "Epoch 498/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 101us/step - loss: 2555004286387.0703 - val_loss: 2759294566192.6074\n",
      "Epoch 499/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2553950244603.5342 - val_loss: 2760662409103.0864\n",
      "Epoch 500/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2552191291610.9756 - val_loss: 2759003946717.6504\n",
      "Epoch 501/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2550751189163.7231 - val_loss: 2757453422769.4355\n",
      "Epoch 502/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2549370828939.4526 - val_loss: 2757469210548.5322\n",
      "Epoch 503/1000\n",
      "3554/3554 [==============================] - 0s 65us/step - loss: 2547734724672.5405 - val_loss: 2755005283697.5615\n",
      "Epoch 504/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2546573742529.4766 - val_loss: 2753170381808.7334\n",
      "Epoch 505/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2545068676655.5410 - val_loss: 2750554365866.7388\n",
      "Epoch 506/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2543712823811.1694 - val_loss: 2749070064912.7788\n",
      "Epoch 507/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2542345288824.4365 - val_loss: 2748137376359.8403\n",
      "Epoch 508/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2540579284811.0566 - val_loss: 2746675484846.5552\n",
      "Epoch 509/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2539271155197.9829 - val_loss: 2744594888106.5947\n",
      "Epoch 510/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2537846788508.5967 - val_loss: 2743961231380.7393\n",
      "Epoch 511/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2536533788615.5273 - val_loss: 2745499419849.9194\n",
      "Epoch 512/1000\n",
      "3554/3554 [==============================] - 0s 128us/step - loss: 2535239736113.1255 - val_loss: 2742437902715.6436\n",
      "Epoch 513/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2533620827965.2266 - val_loss: 2740153783715.6816\n",
      "Epoch 514/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2532221092301.0015 - val_loss: 2739980689774.3931\n",
      "Epoch 515/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2531018791341.8843 - val_loss: 2734856723307.0806\n",
      "Epoch 516/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2529493473507.0435 - val_loss: 2733105592943.9053\n",
      "Epoch 517/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 2528102780261.8525 - val_loss: 2731444393273.6812\n",
      "Epoch 518/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2526663332953.3193 - val_loss: 2729226883736.8081\n",
      "Epoch 519/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2525360659645.5869 - val_loss: 2727731695695.5005\n",
      "Epoch 520/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2524053254800.9272 - val_loss: 2727940837791.9370\n",
      "Epoch 521/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 2522302280438.3477 - val_loss: 2725593287096.7090\n",
      "Epoch 522/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 2521102973629.8750 - val_loss: 2723216886919.6694\n",
      "Epoch 523/1000\n",
      "3554/3554 [==============================] - 0s 128us/step - loss: 2519768735176.9678 - val_loss: 2722589794559.7842\n",
      "Epoch 524/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2518410429245.8032 - val_loss: 2721702596286.5420\n",
      "Epoch 525/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2516849741222.3931 - val_loss: 2718961585218.5386\n",
      "Epoch 526/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2515711988391.9775 - val_loss: 2716779035257.1230\n",
      "Epoch 527/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2514602821575.5273 - val_loss: 2715560348853.7563\n",
      "Epoch 528/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2513040901681.8457 - val_loss: 2715359152080.1846\n",
      "Epoch 529/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2511519766349.3618 - val_loss: 2712756026175.2979\n",
      "Epoch 530/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2510371777160.8599 - val_loss: 2712790894313.7485\n",
      "Epoch 531/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2508902925154.6831 - val_loss: 2713687511741.6777\n",
      "Epoch 532/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2507635281594.9937 - val_loss: 2711213526266.8872\n",
      "Epoch 533/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2506246572611.7095 - val_loss: 2709819596599.5205\n",
      "Epoch 534/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2504819244631.8784 - val_loss: 2708590508867.9067\n",
      "Epoch 535/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2503503604665.1211 - val_loss: 2705663505820.7686\n",
      "Epoch 536/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2501971211274.9487 - val_loss: 2703430624174.4834\n",
      "Epoch 537/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2500637659815.9775 - val_loss: 2703678708517.9497\n",
      "Epoch 538/1000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 2494456018701.763 - 0s 85us/step - loss: 2499609645818.9580 - val_loss: 2703192643915.2520\n",
      "Epoch 539/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2498304623855.1445 - val_loss: 2699648917022.3887\n",
      "Epoch 540/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2497016925265.2515 - val_loss: 2698088545345.3862\n",
      "Epoch 541/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2495445285759.4956 - val_loss: 2695565225254.6699\n",
      "Epoch 542/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2494326315981.8662 - val_loss: 2691988709124.8247\n",
      "Epoch 543/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2493073061377.4404 - val_loss: 2693033212712.2544\n",
      "Epoch 544/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2491714814293.1411 - val_loss: 2693794244986.4912\n",
      "Epoch 545/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2490357953039.8472 - val_loss: 2691223293625.9331\n",
      "Epoch 546/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2489069568682.2827 - val_loss: 2689563266045.4077\n",
      "Epoch 547/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2487723316811.2007 - val_loss: 2688349000658.2007\n",
      "Epoch 548/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2486303578394.3638 - val_loss: 2689523780227.4927\n",
      "Epoch 549/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2485279539928.3828 - val_loss: 2688861425517.6733\n",
      "Epoch 550/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2484073697959.9775 - val_loss: 2685453100462.0513\n",
      "Epoch 551/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2482774232449.5127 - val_loss: 2684049333352.5605\n",
      "Epoch 552/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2481272008143.8828 - val_loss: 2682606839647.2710\n",
      "Epoch 553/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2479974374838.5278 - val_loss: 2682167068347.6611\n",
      "Epoch 554/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2478813565801.5981 - val_loss: 2677292164083.6143\n",
      "Epoch 555/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2477467019184.4771 - val_loss: 2676520068189.6147\n",
      "Epoch 556/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2475905352620.4434 - val_loss: 2674603721146.4370\n",
      "Epoch 557/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2474847951536.0449 - val_loss: 2673693928530.6689\n",
      "Epoch 558/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2473491053918.9375 - val_loss: 2675222093844.7393\n",
      "Epoch 559/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2472380511881.4360 - val_loss: 2672948637542.7603\n",
      "Epoch 560/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2471177522035.9707 - val_loss: 2671196868742.8052\n",
      "Epoch 561/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2469755675329.3325 - val_loss: 2669308078711.9707\n",
      "Epoch 562/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2468509458685.5508 - val_loss: 2666801033744.8506\n",
      "Epoch 563/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 2467254113590.5996 - val_loss: 2665345331890.1558\n",
      "Epoch 564/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2466134673250.1069 - val_loss: 2665336911869.1196\n",
      "Epoch 565/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2464746429878.5278 - val_loss: 2663352341684.0283\n",
      "Epoch 566/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2463295046093.0015 - val_loss: 2662247765752.7271\n",
      "Epoch 567/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2462272486594.1968 - val_loss: 2660335680853.9092\n",
      "Epoch 568/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2461250101464.6709 - val_loss: 2660825754711.2778\n",
      "Epoch 569/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2459932105899.1470 - val_loss: 2660106942258.3359\n",
      "Epoch 570/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2458833081636.1597 - val_loss: 2656564219624.0200\n",
      "Epoch 571/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2457375882775.3384 - val_loss: 2654805282457.6719\n",
      "Epoch 572/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2456119294736.8555 - val_loss: 2654121378958.0063\n",
      "Epoch 573/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2454932562357.9517 - val_loss: 2651556013587.4429\n",
      "Epoch 574/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2453698844686.9824 - val_loss: 2651799107055.7256\n",
      "Epoch 575/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2452469330613.8076 - val_loss: 2648557217926.2290\n",
      "Epoch 576/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2451268842387.0884 - val_loss: 2649297551627.3057\n",
      "Epoch 577/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2450156150714.8496 - val_loss: 2646572719637.1714\n",
      "Epoch 578/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2448728911616.1440 - val_loss: 2644396644816.9048\n",
      "Epoch 579/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2447600200467.1602 - val_loss: 2645311697862.6792\n",
      "Epoch 580/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2446433196673.9448 - val_loss: 2643758304930.8896\n",
      "Epoch 581/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2445115307101.9292 - val_loss: 2641827953425.2104\n",
      "Epoch 582/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2444003812337.5938 - val_loss: 2639048096082.1650\n",
      "Epoch 583/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2442963706229.4116 - val_loss: 2638821127235.1143\n",
      "Epoch 584/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2441783363020.4253 - val_loss: 2637178328893.8574\n",
      "Epoch 585/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2440715112519.4551 - val_loss: 2636308249066.2524\n",
      "Epoch 586/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2439327992258.6289 - val_loss: 2635904120317.5518\n",
      "Epoch 587/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2438290805766.3389 - val_loss: 2635131880705.2241\n",
      "Epoch 588/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2436992192887.7163 - val_loss: 2633755227765.3784\n",
      "Epoch 589/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2435972896358.8608 - val_loss: 2631403414593.9624\n",
      "Epoch 590/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2434668205523.3403 - val_loss: 2629557085770.7476\n",
      "Epoch 591/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2433528698894.9824 - val_loss: 2630892953676.0439\n",
      "Epoch 592/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2432455767401.3101 - val_loss: 2628014235673.3481\n",
      "Epoch 593/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2431146402347.5068 - val_loss: 2625833226978.5474\n",
      "Epoch 594/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2429949910629.7085 - val_loss: 2622835289166.0601\n",
      "Epoch 595/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2428913506254.4424 - val_loss: 2624718460650.9009\n",
      "Epoch 596/1000\n",
      "3554/3554 [==============================] - 1s 145us/step - loss: 2427676537525.8076 - val_loss: 2622511014899.6143\n",
      "Epoch 597/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2426468196633.2109 - val_loss: 2622704252782.2490\n",
      "Epoch 598/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2425163313725.9473 - val_loss: 2620564313719.6826\n",
      "Epoch 599/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2424217395684.0518 - val_loss: 2619003832576.3599\n",
      "Epoch 600/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2422962942710.3477 - val_loss: 2617303693903.6445\n",
      "Epoch 601/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2421856948618.1562 - val_loss: 2615454218172.5977\n",
      "Epoch 602/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2420703599916.2271 - val_loss: 2613150033468.9214\n",
      "Epoch 603/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2419637950478.9824 - val_loss: 2610788813199.2305\n",
      "Epoch 604/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2418759335843.7998 - val_loss: 2610994837404.3364\n",
      "Epoch 605/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2417486937992.1396 - val_loss: 2609875128795.8506\n",
      "Epoch 606/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2416231571718.7710 - val_loss: 2609216670851.0605\n",
      "Epoch 607/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2415166880390.5547 - val_loss: 2609052817135.5093\n",
      "Epoch 608/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2413949166227.2324 - val_loss: 2607026227663.7524\n",
      "Epoch 609/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2412800356326.6450 - val_loss: 2605402550681.0239\n",
      "Epoch 610/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2411715872407.2661 - val_loss: 2604139187566.1050\n",
      "Epoch 611/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2410622978009.9673 - val_loss: 2604362309569.7822\n",
      "Epoch 612/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2409558153691.9844 - val_loss: 2602698212231.3091\n",
      "Epoch 613/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2408353073630.2896 - val_loss: 2600296152027.9941\n",
      "Epoch 614/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2407370486665.8682 - val_loss: 2597018391423.2437\n",
      "Epoch 615/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2406087310963.5386 - val_loss: 2596177110049.7012\n",
      "Epoch 616/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2405204377812.0605 - val_loss: 2596554514526.4790\n",
      "Epoch 617/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2403982969550.0103 - val_loss: 2599203954528.9990\n",
      "Epoch 618/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2403193957563.2822 - val_loss: 2595168736615.7681\n",
      "Epoch 619/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2401745979473.8281 - val_loss: 2591989530356.6943\n",
      "Epoch 620/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2400639447927.4282 - val_loss: 2590291136139.8457\n",
      "Epoch 621/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2399443031120.0991 - val_loss: 2588802421654.2876\n",
      "Epoch 622/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 122us/step - loss: 2398572378094.7124 - val_loss: 2587398004235.9541\n",
      "Epoch 623/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 2397472947117.5957 - val_loss: 2586066876196.5098\n",
      "Epoch 624/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2396475015066.5796 - val_loss: 2585935833876.6670\n",
      "Epoch 625/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2395223069905.1792 - val_loss: 2583385840541.7769\n",
      "Epoch 626/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2394354762087.5811 - val_loss: 2583013720054.7827\n",
      "Epoch 627/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2393178776275.7729 - val_loss: 2582708783424.8823\n",
      "Epoch 628/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2392012079703.3022 - val_loss: 2583042465895.6963\n",
      "Epoch 629/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2390968553773.9561 - val_loss: 2580761573626.5991\n",
      "Epoch 630/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2389951442134.9419 - val_loss: 2578294422662.5171\n",
      "Epoch 631/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2388765639642.5435 - val_loss: 2577979125629.8037\n",
      "Epoch 632/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2387602845725.9653 - val_loss: 2576571944085.4951\n",
      "Epoch 633/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2386721975575.4824 - val_loss: 2574133846087.4351\n",
      "Epoch 634/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2385728831672.4009 - val_loss: 2572627979014.8413\n",
      "Epoch 635/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2384819811431.1489 - val_loss: 2572978757072.0405\n",
      "Epoch 636/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2383677336320.7202 - val_loss: 2571819770443.6118\n",
      "Epoch 637/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2382551094444.2993 - val_loss: 2569036001218.9346\n",
      "Epoch 638/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2381523810231.3921 - val_loss: 2568676292936.9473\n",
      "Epoch 639/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2380471623518.6494 - val_loss: 2566743887859.3262\n",
      "Epoch 640/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2379335325314.5210 - val_loss: 2567820109064.1372\n",
      "Epoch 641/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2378152299979.2729 - val_loss: 2567990286681.0781\n",
      "Epoch 642/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2377348541986.8633 - val_loss: 2566430280715.2339\n",
      "Epoch 643/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2376325762903.7344 - val_loss: 2565689426818.7002\n",
      "Epoch 644/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2375226730600.8779 - val_loss: 2562806912963.5107\n",
      "Epoch 645/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2374154876154.6694 - val_loss: 2560623768624.6797\n",
      "Epoch 646/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2373092289096.3198 - val_loss: 2559258204081.0757\n",
      "Epoch 647/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2372213640383.8921 - val_loss: 2559049484254.0107\n",
      "Epoch 648/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2370802397379.9258 - val_loss: 2556045938219.0625\n",
      "Epoch 649/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2370270888765.8032 - val_loss: 2555295182358.3237\n",
      "Epoch 650/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2369147929507.2231 - val_loss: 2554578251885.4570\n",
      "Epoch 651/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2367977009495.4463 - val_loss: 2555560071348.8921\n",
      "Epoch 652/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2366857519781.6724 - val_loss: 2552754244052.3613\n",
      "Epoch 653/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2366273585483.9214 - val_loss: 2552505827143.6514\n",
      "Epoch 654/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2365047500138.4624 - val_loss: 2552263180721.2197\n",
      "Epoch 655/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2364066716797.0469 - val_loss: 2550057180338.2998\n",
      "Epoch 656/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2363023089631.7300 - val_loss: 2548814303684.8066\n",
      "Epoch 657/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2361759086288.8916 - val_loss: 2547578619473.6606\n",
      "Epoch 658/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2361004584556.6235 - val_loss: 2546054062971.7871\n",
      "Epoch 659/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2360176537786.7056 - val_loss: 2544916151614.8657\n",
      "Epoch 660/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2359160791616.2524 - val_loss: 2544346835239.5342\n",
      "Epoch 661/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2357966712105.9224 - val_loss: 2543434130723.7896\n",
      "Epoch 662/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2357119131348.3491 - val_loss: 2543007034510.5825\n",
      "Epoch 663/1000\n",
      "3554/3554 [==============================] - 0s 65us/step - loss: 2356113596586.5708 - val_loss: 2542484970936.9971\n",
      "Epoch 664/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2355125215789.8120 - val_loss: 2541887165666.1152\n",
      "Epoch 665/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2354248059542.6899 - val_loss: 2539789768666.5542\n",
      "Epoch 666/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2353253322085.8525 - val_loss: 2539805894320.1396\n",
      "Epoch 667/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2352312112088.2383 - val_loss: 2535984972158.5239\n",
      "Epoch 668/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2351243809641.5981 - val_loss: 2536650508065.0532\n",
      "Epoch 669/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2350138444724.5107 - val_loss: 2534501926472.4434\n",
      "Epoch 670/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2349330937940.1328 - val_loss: 2537547161588.4780\n",
      "Epoch 671/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2348495230391.6802 - val_loss: 2532759843358.6768\n",
      "Epoch 672/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2347496665920.1079 - val_loss: 2532478936300.4849\n",
      "Epoch 673/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2346387538639.1626 - val_loss: 2530227441213.4976\n",
      "Epoch 674/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2345447798130.5303 - val_loss: 2529375076163.0425\n",
      "Epoch 675/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2344445329380.3398 - val_loss: 2528146437099.5488\n",
      "Epoch 676/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2343592049480.1753 - val_loss: 2526508505677.3398\n",
      "Epoch 677/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2342558411852.6416 - val_loss: 2526482241281.9443\n",
      "Epoch 678/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2341631353242.2915 - val_loss: 2527944457875.9111\n",
      "Epoch 679/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2340806850542.7124 - val_loss: 2525197045000.7134\n",
      "Epoch 680/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2339847233042.7280 - val_loss: 2524078596969.3525\n",
      "Epoch 681/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2338696302866.2959 - val_loss: 2522553127719.6782\n",
      "Epoch 682/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2337881001815.7344 - val_loss: 2520977769885.9209\n",
      "Epoch 683/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2336974988356.5737 - val_loss: 2518014412627.7490\n",
      "Epoch 684/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2336049938754.7012 - val_loss: 2518084007693.4663\n",
      "Epoch 685/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2335071236374.9062 - val_loss: 2517736898140.6064\n",
      "Epoch 686/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 2334110915201.9448 - val_loss: 2517790125613.3672\n",
      "Epoch 687/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2333228223615.3516 - val_loss: 2516039527612.6694\n",
      "Epoch 688/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2332168488866.6470 - val_loss: 2516656597849.7979\n",
      "Epoch 689/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2331245361383.6533 - val_loss: 2512747873228.4399\n",
      "Epoch 690/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2330334853545.2744 - val_loss: 2513482908781.7451\n",
      "Epoch 691/1000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 2329466223837.8569 - val_loss: 2510723454058.2886\n",
      "Epoch 692/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2328523563971.4937 - val_loss: 2511776495133.8125\n",
      "Epoch 693/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2327660923524.2495 - val_loss: 2513623701125.5088\n",
      "Epoch 694/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2327000314743.4282 - val_loss: 2509375897000.8662\n",
      "Epoch 695/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2325800274868.5107 - val_loss: 2507641948130.6196\n",
      "Epoch 696/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2324929591785.8145 - val_loss: 2506620764925.0474\n",
      "Epoch 697/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2323907473004.6235 - val_loss: 2507406927259.6162\n",
      "Epoch 698/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2323240367622.0508 - val_loss: 2506030316019.4702\n",
      "Epoch 699/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2322281342546.1157 - val_loss: 2505182505564.3184\n",
      "Epoch 700/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2321385339037.8931 - val_loss: 2502481998251.4590\n",
      "Epoch 701/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2320296341450.9849 - val_loss: 2500721163914.6938\n",
      "Epoch 702/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 2319530230700.4434 - val_loss: 2498256617909.8286\n",
      "Epoch 703/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2318681930946.1968 - val_loss: 2496764109894.5708\n",
      "Epoch 704/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2317931437605.1680 - val_loss: 2497857841212.4893\n",
      "Epoch 705/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2316805470264.4727 - val_loss: 2498640910358.4673\n",
      "Epoch 706/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2315926044266.3188 - val_loss: 2497914101702.3911\n",
      "Epoch 707/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2315109521911.6440 - val_loss: 2495286933726.9468\n",
      "Epoch 708/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2314243949272.3828 - val_loss: 2499148910109.2363\n",
      "Epoch 709/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2313497571016.8237 - val_loss: 2495288365416.6323\n",
      "Epoch 710/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2312492514313.7964 - val_loss: 2492375696779.4858\n",
      "Epoch 711/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2311402582848.1079 - val_loss: 2490115856848.9048\n",
      "Epoch 712/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 2310608444670.1270 - val_loss: 2489855120425.7666\n",
      "Epoch 713/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2309444256675.2231 - val_loss: 2487476462049.6113\n",
      "Epoch 714/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 2308983309717.1050 - val_loss: 2487230914530.0435\n",
      "Epoch 715/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2307920034106.6338 - val_loss: 2487155805296.6255\n",
      "Epoch 716/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2307067422105.7153 - val_loss: 2486334354913.8994\n",
      "Epoch 717/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2306209903940.4302 - val_loss: 2485826961818.1763\n",
      "Epoch 718/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2305428709878.4917 - val_loss: 2484568200749.9434\n",
      "Epoch 719/1000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 2304351980413.1904 - val_loss: 2483076363874.9434\n",
      "Epoch 720/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2303738298303.4600 - val_loss: 2481912668941.1782\n",
      "Epoch 721/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2302611171783.2393 - val_loss: 2481090940044.2778\n",
      "Epoch 722/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2302010836479.1357 - val_loss: 2479938985253.5181\n",
      "Epoch 723/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2301196232357.0962 - val_loss: 2480055461178.8330\n",
      "Epoch 724/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2300290441063.8696 - val_loss: 2479791292291.5645\n",
      "Epoch 725/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2299487320636.7949 - val_loss: 2478136259457.5483\n",
      "Epoch 726/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2298607603285.5732 - val_loss: 2476427219576.5469\n",
      "Epoch 727/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2297693168081.6118 - val_loss: 2475821286108.7866\n",
      "Epoch 728/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2297031209561.6069 - val_loss: 2474432078683.5264\n",
      "Epoch 729/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2296120452030.8838 - val_loss: 2472935689757.2363\n",
      "Epoch 730/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2295274336608.6660 - val_loss: 2472419223308.3140\n",
      "Epoch 731/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 2294410339728.4951 - val_loss: 2471217527965.5605\n",
      "Epoch 732/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2293612107117.9199 - val_loss: 2470759401092.3564\n",
      "Epoch 733/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2292708318323.8267 - val_loss: 2469730303387.0400\n",
      "Epoch 734/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2291839238309.9604 - val_loss: 2469409890380.0439\n",
      "Epoch 735/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2291077951923.6465 - val_loss: 2467776899358.3169\n",
      "Epoch 736/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2290202322366.5952 - val_loss: 2464918909756.4175\n",
      "Epoch 737/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2289531526554.8677 - val_loss: 2464673376173.6191\n",
      "Epoch 738/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2288653654390.5640 - val_loss: 2465832678517.8105\n",
      "Epoch 739/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2287717199975.7251 - val_loss: 2464263404342.0806\n",
      "Epoch 740/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2286976348650.9668 - val_loss: 2463248875464.6953\n",
      "Epoch 741/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2286112645687.0322 - val_loss: 2461617096536.3579\n",
      "Epoch 742/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2285226546228.4390 - val_loss: 2460564227015.8311\n",
      "Epoch 743/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2284400450528.3062 - val_loss: 2458722745386.3428\n",
      "Epoch 744/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2283757260696.2749 - val_loss: 2458213475804.1382\n",
      "Epoch 745/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2282862643825.8101 - val_loss: 2459011506625.3501\n",
      "Epoch 746/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 101us/step - loss: 2282093708351.9639 - val_loss: 2458079487959.0977\n",
      "Epoch 747/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2281106810468.5562 - val_loss: 2457253662831.7612\n",
      "Epoch 748/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2280380186024.6978 - val_loss: 2455100308780.4312\n",
      "Epoch 749/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2279596256710.6626 - val_loss: 2453223433987.0967\n",
      "Epoch 750/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2279049600171.1470 - val_loss: 2450451880525.3398\n",
      "Epoch 751/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2278592039251.9888 - val_loss: 2450581282398.9106\n",
      "Epoch 752/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2277564517383.4912 - val_loss: 2449370935566.7622\n",
      "Epoch 753/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2276864197400.9229 - val_loss: 2449016896452.9507\n",
      "Epoch 754/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2276110892097.1162 - val_loss: 2448253844526.9512\n",
      "Epoch 755/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2275166422006.2036 - val_loss: 2447333241512.9385\n",
      "Epoch 756/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2274366624645.2583 - val_loss: 2449079116755.3530\n",
      "Epoch 757/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2273272083310.2080 - val_loss: 2448272330877.5874\n",
      "Epoch 758/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2272690884393.6343 - val_loss: 2447342629291.4590\n",
      "Epoch 759/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2271765975397.2764 - val_loss: 2445755685798.9941\n",
      "Epoch 760/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2271098858065.5396 - val_loss: 2444028148727.6465\n",
      "Epoch 761/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2270473791555.9976 - val_loss: 2444906784551.9663\n",
      "Epoch 762/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2269626867430.7891 - val_loss: 2442447737635.0693\n",
      "Epoch 763/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2268776132481.2246 - val_loss: 2441356763437.8711\n",
      "Epoch 764/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2268290252707.7998 - val_loss: 2441984247686.4448\n",
      "Epoch 765/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2267340989010.6919 - val_loss: 2440198226878.6138\n",
      "Epoch 766/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2266561072697.9136 - val_loss: 2441903907002.6533\n",
      "Epoch 767/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2265814370561.0083 - val_loss: 2441882916347.2471\n",
      "Epoch 768/1000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 2265117637466.6157 - val_loss: 2438188523978.5679\n",
      "Epoch 769/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2264249255298.6650 - val_loss: 2438094481864.8394\n",
      "Epoch 770/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2263342770197.8975 - val_loss: 2434211400825.8433\n",
      "Epoch 771/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2262761196579.1514 - val_loss: 2434903543979.6748\n",
      "Epoch 772/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2261855026397.8569 - val_loss: 2435154697177.6899\n",
      "Epoch 773/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2261185062491.3359 - val_loss: 2434293885596.8403\n",
      "Epoch 774/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2260570768669.2446 - val_loss: 2431826627784.1914\n",
      "Epoch 775/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2259724834849.4229 - val_loss: 2432857668857.7349\n",
      "Epoch 776/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2258937752873.9224 - val_loss: 2434738705030.3730\n",
      "Epoch 777/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2258223195518.0552 - val_loss: 2431775136987.7783\n",
      "Epoch 778/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2257404728271.0186 - val_loss: 2432878388594.1377\n",
      "Epoch 779/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2256814038915.5293 - val_loss: 2429884212160.9180\n",
      "Epoch 780/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2255956415125.5376 - val_loss: 2428154323794.3091\n",
      "Epoch 781/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2255059689393.6299 - val_loss: 2427477871525.8418\n",
      "Epoch 782/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2254339970238.7393 - val_loss: 2425482182342.6069\n",
      "Epoch 783/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2253717276523.3271 - val_loss: 2427810885400.4116\n",
      "Epoch 784/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2253097759994.6694 - val_loss: 2424670959189.1172\n",
      "Epoch 785/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2252165779988.4570 - val_loss: 2426233934129.3276\n",
      "Epoch 786/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2251658357864.3018 - val_loss: 2425294620633.4019\n",
      "Epoch 787/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2250777842426.9580 - val_loss: 2423327333148.7324\n",
      "Epoch 788/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2249980335451.4800 - val_loss: 2422319217655.6465\n",
      "Epoch 789/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2249116383565.0737 - val_loss: 2420767246569.3164\n",
      "Epoch 790/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2248490547662.1543 - val_loss: 2420792576242.5337\n",
      "Epoch 791/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2247663147142.8433 - val_loss: 2421346359977.5146\n",
      "Epoch 792/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2246799083417.4272 - val_loss: 2417073791135.0010\n",
      "Epoch 793/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2246300592555.5791 - val_loss: 2416620970004.7393\n",
      "Epoch 794/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2245497382191.6846 - val_loss: 2415435234991.5635\n",
      "Epoch 795/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2244932858522.1475 - val_loss: 2415028420833.8271\n",
      "Epoch 796/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2244111309405.0645 - val_loss: 2414345164988.3813\n",
      "Epoch 797/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2243469765957.5825 - val_loss: 2414189196608.5942\n",
      "Epoch 798/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2242778550389.5552 - val_loss: 2414099613781.8374\n",
      "Epoch 799/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2242125712400.7114 - val_loss: 2412901954866.7681\n",
      "Epoch 800/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2241372103573.3936 - val_loss: 2412817691572.2441\n",
      "Epoch 801/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2240683747838.5596 - val_loss: 2410577138471.6782\n",
      "Epoch 802/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2239848664136.0317 - val_loss: 2409790390571.5669\n",
      "Epoch 803/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2239238647247.8828 - val_loss: 2408894194048.8281\n",
      "Epoch 804/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2238557871081.5264 - val_loss: 2409225013730.1875\n",
      "Epoch 805/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2237928403186.0259 - val_loss: 2407841170965.7476\n",
      "Epoch 806/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2237144107984.1709 - val_loss: 2405842467347.7310\n",
      "Epoch 807/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2236479520721.8999 - val_loss: 2405010042017.0171\n",
      "Epoch 808/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2235670286029.4341 - val_loss: 2404205060213.5225\n",
      "Epoch 809/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2234990508282.0933 - val_loss: 2404513163255.3589\n",
      "Epoch 810/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2234395967755.3809 - val_loss: 2402456872573.4434\n",
      "Epoch 811/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2233619865291.1289 - val_loss: 2402188457248.3330\n",
      "Epoch 812/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2232868264218.3638 - val_loss: 2401762294135.0347\n",
      "Epoch 813/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2232216646961.9897 - val_loss: 2403089413989.0317\n",
      "Epoch 814/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2231757627309.0195 - val_loss: 2401044709555.7402\n",
      "Epoch 815/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2230798787588.0337 - val_loss: 2400148131062.2783\n",
      "Epoch 816/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2230298284988.5786 - val_loss: 2399610216662.3057\n",
      "Epoch 817/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2229695620542.5952 - val_loss: 2398749100131.6636\n",
      "Epoch 818/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2228868072318.3433 - val_loss: 2394867450081.8271\n",
      "Epoch 819/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2228257596327.8335 - val_loss: 2394800210662.0039\n",
      "Epoch 820/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2227609221425.4136 - val_loss: 2394927993658.1133\n",
      "Epoch 821/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2226898581200.8916 - val_loss: 2393302741931.0269\n",
      "Epoch 822/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2226122603539.0161 - val_loss: 2394585034976.6753\n",
      "Epoch 823/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2225453820128.7383 - val_loss: 2393069295030.1167\n",
      "Epoch 824/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2224961783373.5059 - val_loss: 2391814522127.0503\n",
      "Epoch 825/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2224216680054.9961 - val_loss: 2390274756516.9775\n",
      "Epoch 826/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2223411374904.0405 - val_loss: 2390798439447.3315\n",
      "Epoch 827/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2222694131770.7778 - val_loss: 2393477318552.8799\n",
      "Epoch 828/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2222300227359.2617 - val_loss: 2390222278864.5444\n",
      "Epoch 829/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2221390606052.4839 - val_loss: 2391275580977.9756\n",
      "Epoch 830/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2220910660698.4717 - val_loss: 2389153174887.7681\n",
      "Epoch 831/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2220045606686.1089 - val_loss: 2388643067108.4199\n",
      "Epoch 832/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2219616332848.9814 - val_loss: 2384587661673.7847\n",
      "Epoch 833/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2219017556357.5464 - val_loss: 2384405991199.3247\n",
      "Epoch 834/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2217949207907.5474 - val_loss: 2385296354630.9312\n",
      "Epoch 835/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2217647599028.2227 - val_loss: 2384471699786.6758\n",
      "Epoch 836/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2217127046764.6235 - val_loss: 2382853754982.5439\n",
      "Epoch 837/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2216292194392.7427 - val_loss: 2382117472383.3159\n",
      "Epoch 838/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2215695497705.8145 - val_loss: 2381920974513.2915\n",
      "Epoch 839/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2214874862773.5195 - val_loss: 2383519200779.3779\n",
      "Epoch 840/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2214453527735.8247 - val_loss: 2380691989058.3945\n",
      "Epoch 841/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2213551469578.9487 - val_loss: 2379491408030.4248\n",
      "Epoch 842/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2213010279560.5718 - val_loss: 2376799308682.1895\n",
      "Epoch 843/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2212491563522.0171 - val_loss: 2376455869956.4648\n",
      "Epoch 844/1000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2211757591817.6523 - val_loss: 2375752758823.0303\n",
      "Epoch 845/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2211181896412.9927 - val_loss: 2374297460765.9565\n",
      "Epoch 846/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2210809729224.5356 - val_loss: 2374437030213.4907\n",
      "Epoch 847/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2209870534919.9233 - val_loss: 2373933698285.0610\n",
      "Epoch 848/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2209258119108.0698 - val_loss: 2371004798067.5059\n",
      "Epoch 849/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2209129567201.4585 - val_loss: 2370275627829.5044\n",
      "Epoch 850/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2208598836610.6650 - val_loss: 2369988034661.3916\n",
      "Epoch 851/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2207678220296.0674 - val_loss: 2370586377972.6943\n",
      "Epoch 852/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2206827382229.6455 - val_loss: 2371397463749.4551\n",
      "Epoch 853/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2206306065809.6475 - val_loss: 2371339821565.2637\n",
      "Epoch 854/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2205589617848.9771 - val_loss: 2370390816041.5503\n",
      "Epoch 855/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2204939298625.8369 - val_loss: 2369823106450.3989\n",
      "Epoch 856/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2204216906915.0791 - val_loss: 2369355472414.3887\n",
      "Epoch 857/1000\n",
      "3554/3554 [==============================] - 0s 65us/step - loss: 2203789710628.1597 - val_loss: 2367364128959.2617\n",
      "Epoch 858/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2203007123419.1196 - val_loss: 2366661787482.0859\n",
      "Epoch 859/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2202441799187.8809 - val_loss: 2366975540704.1709\n",
      "Epoch 860/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2201846196144.4771 - val_loss: 2366329546471.4443\n",
      "Epoch 861/1000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 2201120423484.2183 - val_loss: 2364858447583.3789\n",
      "Epoch 862/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2200705239905.5308 - val_loss: 2364267930948.0508\n",
      "Epoch 863/1000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2199974495139.2231 - val_loss: 2365040832446.3257\n",
      "Epoch 864/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2199377539738.1475 - val_loss: 2363473201408.3599\n",
      "Epoch 865/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2198765440394.7327 - val_loss: 2361332226821.6890\n",
      "Epoch 866/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2198248486083.3496 - val_loss: 2363820274828.8540\n",
      "Epoch 867/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2197745326159.5227 - val_loss: 2361785440061.8574\n",
      "Epoch 868/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2197018382628.7358 - val_loss: 2361039010019.5557\n",
      "Epoch 869/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2196451452968.3376 - val_loss: 2361149265942.4673\n",
      "Epoch 870/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 86us/step - loss: 2195859149037.4160 - val_loss: 2359374095131.0044\n",
      "Epoch 871/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2195308027438.3884 - val_loss: 2357985142844.4893\n",
      "Epoch 872/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2194585118981.6184 - val_loss: 2357471468579.4297\n",
      "Epoch 873/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2193941375855.3608 - val_loss: 2356201358593.2241\n",
      "Epoch 874/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2193490198813.8210 - val_loss: 2357679241812.2529\n",
      "Epoch 875/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2193006147336.2114 - val_loss: 2355597906349.7632\n",
      "Epoch 876/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2192276687944.0315 - val_loss: 2354857852416.4321\n",
      "Epoch 877/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2191704216536.2385 - val_loss: 2354389644170.7656\n",
      "Epoch 878/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2191104511684.2136 - val_loss: 2351620736778.0098\n",
      "Epoch 879/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2190545473423.0544 - val_loss: 2351864703248.4907\n",
      "Epoch 880/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2189896652741.2222 - val_loss: 2350034944064.5220\n",
      "Epoch 881/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2189417191955.3044 - val_loss: 2351733791078.9043\n",
      "Epoch 882/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2188843410887.8154 - val_loss: 2351584434553.3389\n",
      "Epoch 883/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2188244526840.0764 - val_loss: 2350500963610.5723\n",
      "Epoch 884/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2187606620474.6335 - val_loss: 2348992908833.8452\n",
      "Epoch 885/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2187041997371.6421 - val_loss: 2347775382537.5054\n",
      "Epoch 886/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2186367579635.6106 - val_loss: 2348574413598.4609\n",
      "Epoch 887/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 2185913113476.1057 - val_loss: 2348435081887.1450\n",
      "Epoch 888/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2185334613595.3359 - val_loss: 2347744204730.2930\n",
      "Epoch 889/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2184809064958.5593 - val_loss: 2346750146202.2480\n",
      "Epoch 890/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2184248821039.6848 - val_loss: 2344501074601.5146\n",
      "Epoch 891/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2183598800398.1182 - val_loss: 2343310549727.0908\n",
      "Epoch 892/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2183186719840.2339 - val_loss: 2343142354560.3242\n",
      "Epoch 893/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2182508768323.4216 - val_loss: 2344002976667.4722\n",
      "Epoch 894/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2182012151636.2771 - val_loss: 2343275649954.3853\n",
      "Epoch 895/1000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 2198456652059.881 - 0s 99us/step - loss: 2181249335843.4395 - val_loss: 2342355213259.8638\n",
      "Epoch 896/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2180726394176.9724 - val_loss: 2342400264789.4053\n",
      "Epoch 897/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2180198262367.9460 - val_loss: 2340736396190.9287\n",
      "Epoch 898/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2179691197994.9309 - val_loss: 2338271930709.6216\n",
      "Epoch 899/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2179245773295.5767 - val_loss: 2337271268475.5713\n",
      "Epoch 900/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2178539584761.5171 - val_loss: 2338497448289.1431\n",
      "Epoch 901/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2177941845067.4890 - val_loss: 2337071651608.1235\n",
      "Epoch 902/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2177458084279.1040 - val_loss: 2337607365841.9849\n",
      "Epoch 903/1000\n",
      "3554/3554 [==============================] - 0s 64us/step - loss: 2176888138081.8188 - val_loss: 2336783191115.7559\n",
      "Epoch 904/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2176354677826.2690 - val_loss: 2336023561035.3960\n",
      "Epoch 905/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2175880887782.9331 - val_loss: 2335294472648.2632\n",
      "Epoch 906/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2175109402165.3035 - val_loss: 2334155399930.7432\n",
      "Epoch 907/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2174730617320.6616 - val_loss: 2332984028012.2329\n",
      "Epoch 908/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2174146060357.1501 - val_loss: 2333195956282.7612\n",
      "Epoch 909/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2173628159258.9399 - val_loss: 2332490514279.9121\n",
      "Epoch 910/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2173141374225.7197 - val_loss: 2331658734317.4932\n",
      "Epoch 911/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2172537161072.2249 - val_loss: 2329949937481.9556\n",
      "Epoch 912/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2172113463345.5576 - val_loss: 2330250972185.0601\n",
      "Epoch 913/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2171378494328.5808 - val_loss: 2329915896553.4604\n",
      "Epoch 914/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2170867284171.9932 - val_loss: 2328725697275.8955\n",
      "Epoch 915/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2170351922197.3215 - val_loss: 2325766340054.9536\n",
      "Epoch 916/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2170258935326.2534 - val_loss: 2326243804490.9639\n",
      "Epoch 917/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2169507864765.0107 - val_loss: 2326134893392.5806\n",
      "Epoch 918/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2168753290858.3186 - val_loss: 2326297036019.3979\n",
      "Epoch 919/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2168264827705.1929 - val_loss: 2327822870132.2261\n",
      "Epoch 920/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2167844495327.7297 - val_loss: 2326078180178.8848\n",
      "Epoch 921/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2167153015845.4563 - val_loss: 2324632008862.4248\n",
      "Epoch 922/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2166657095676.5425 - val_loss: 2323981462578.4077\n",
      "Epoch 923/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2166141071408.4053 - val_loss: 2326420339597.6460\n",
      "Epoch 924/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 2165770863596.9836 - val_loss: 2323411964287.6758\n",
      "Epoch 925/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2165047002286.6042 - val_loss: 2321381441918.5239\n",
      "Epoch 926/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2164624022422.5459 - val_loss: 2321657133195.9897\n",
      "Epoch 927/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2164106849531.8223 - val_loss: 2322837271158.2427\n",
      "Epoch 928/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2163671370667.2910 - val_loss: 2320542493515.9717\n",
      "Epoch 929/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2163054683250.6741 - val_loss: 2322426735477.4502\n",
      "Epoch 930/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2162608073223.7795 - val_loss: 2320251914855.8403\n",
      "Epoch 931/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2162002435049.5261 - val_loss: 2319211860674.2861\n",
      "Epoch 932/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2161420644400.4053 - val_loss: 2318447243442.5879\n",
      "Epoch 933/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2160923090681.2290 - val_loss: 2318412732498.3809\n",
      "Epoch 934/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2160522015801.0488 - val_loss: 2318615124036.5547\n",
      "Epoch 935/1000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 2159887226212.7000 - val_loss: 2316994156501.9453\n",
      "Epoch 936/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2159433741581.1096 - val_loss: 2316457482290.1196\n",
      "Epoch 937/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2158797309167.1445 - val_loss: 2315644517925.3018\n",
      "Epoch 938/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2158417898618.7415 - val_loss: 2315602753076.8564\n",
      "Epoch 939/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2157792840591.6309 - val_loss: 2313838370099.3442\n",
      "Epoch 940/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2157285885738.7869 - val_loss: 2312782749725.9565\n",
      "Epoch 941/1000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 2156936550461.0828 - val_loss: 2311698759509.4775\n",
      "Epoch 942/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 2156387167723.5430 - val_loss: 2311478399714.5474\n",
      "Epoch 943/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 2155801839282.9263 - val_loss: 2312801159092.5322\n",
      "Epoch 944/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2155384247136.3782 - val_loss: 2311928114346.2344\n",
      "Epoch 945/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2154775861180.0022 - val_loss: 2311062693123.5283\n",
      "Epoch 946/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2154379493883.1018 - val_loss: 2312597049644.7188\n",
      "Epoch 947/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2153883657653.3755 - val_loss: 2310861051331.9424\n",
      "Epoch 948/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2153538758760.8779 - val_loss: 2308980414200.7271\n",
      "Epoch 949/1000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 2152870442087.1489 - val_loss: 2307698009808.4004\n",
      "Epoch 950/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2152308547394.9893 - val_loss: 2308238246155.3057\n",
      "Epoch 951/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2151872418370.5571 - val_loss: 2306389763179.4409\n",
      "Epoch 952/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2151272804833.7466 - val_loss: 2305338727192.1235\n",
      "Epoch 953/1000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2151015163898.2373 - val_loss: 2306418206245.8779\n",
      "Epoch 954/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2150407445214.1450 - val_loss: 2304872194972.0483\n",
      "Epoch 955/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2149892581324.1372 - val_loss: 2305139275980.5117\n",
      "Epoch 956/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2149465678215.8516 - val_loss: 2305055518207.2798\n",
      "Epoch 957/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2148873697760.5942 - val_loss: 2306424370685.5518\n",
      "Epoch 958/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 2148664843969.9089 - val_loss: 2304475998641.7959\n",
      "Epoch 959/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2148003957961.1118 - val_loss: 2303560934535.0933\n",
      "Epoch 960/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2147354877025.3867 - val_loss: 2302478444844.1431\n",
      "Epoch 961/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2146949375957.3574 - val_loss: 2300948793125.6621\n",
      "Epoch 962/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2146421293434.0215 - val_loss: 2300694758887.6602\n",
      "Epoch 963/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2145938309562.5615 - val_loss: 2299987186734.3755\n",
      "Epoch 964/1000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 2145454792404.3489 - val_loss: 2299465510588.2373\n",
      "Epoch 965/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 2144987784417.3147 - val_loss: 2297899069521.8047\n",
      "Epoch 966/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2144545674356.9792 - val_loss: 2297427090052.6445\n",
      "Epoch 967/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2144081684935.2390 - val_loss: 2295811676246.9897\n",
      "Epoch 968/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2143603860668.4346 - val_loss: 2297410477496.9971\n",
      "Epoch 969/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2142993561481.2922 - val_loss: 2296610002431.8560\n",
      "Epoch 970/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2142464100230.9871 - val_loss: 2296364540929.4404\n",
      "Epoch 971/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 2142169208736.3423 - val_loss: 2295176187683.9336\n",
      "Epoch 972/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2141717014485.3574 - val_loss: 2294974260483.5283\n",
      "Epoch 973/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2141158707197.6948 - val_loss: 2295627925211.9224\n",
      "Epoch 974/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2140752036660.5830 - val_loss: 2293998379521.8721\n",
      "Epoch 975/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2140287346294.9961 - val_loss: 2293179052284.3276\n",
      "Epoch 976/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2139756415592.5896 - val_loss: 2293105988990.8120\n",
      "Epoch 977/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2139266144649.0039 - val_loss: 2292448365888.0181\n",
      "Epoch 978/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2138928030078.6313 - val_loss: 2292435839239.2729\n",
      "Epoch 979/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 2138339714181.1143 - val_loss: 2292507116856.5288\n",
      "Epoch 980/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2137981463361.2607 - val_loss: 2291334582618.8062\n",
      "Epoch 981/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2137602513915.9663 - val_loss: 2291718906849.1792\n",
      "Epoch 982/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2137069636311.2300 - val_loss: 2289875121744.5088\n",
      "Epoch 983/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2136604673770.2463 - val_loss: 2288872631249.6245\n",
      "Epoch 984/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2136353963054.6763 - val_loss: 2287456267881.2803\n",
      "Epoch 985/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2135735373766.3748 - val_loss: 2286573352989.3804\n",
      "Epoch 986/1000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 2135371321562.9758 - val_loss: 2286799746336.3330\n",
      "Epoch 987/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2134823762584.9949 - val_loss: 2286213328807.2822\n",
      "Epoch 988/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2134396193734.9512 - val_loss: 2285791493392.7788\n",
      "Epoch 989/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2133877684667.7141 - val_loss: 2285549272668.6064\n",
      "Epoch 990/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2133309333462.5098 - val_loss: 2283950359482.0049\n",
      "Epoch 991/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2133170801404.1104 - val_loss: 2284040435810.5112\n",
      "Epoch 992/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 2132576636040.5718 - val_loss: 2282555190621.9746\n",
      "Epoch 993/1000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2132328576269.1096 - val_loss: 2283173813702.8232\n",
      "Epoch 994/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 79us/step - loss: 2131591071208.0854 - val_loss: 2282950853446.7871\n",
      "Epoch 995/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2131100557501.0107 - val_loss: 2282826255850.5405\n",
      "Epoch 996/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2130743099410.4402 - val_loss: 2283922758410.2979\n",
      "Epoch 997/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2130300605944.2205 - val_loss: 2281070382303.2349\n",
      "Epoch 998/1000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2129757286024.2837 - val_loss: 2282003242189.9521\n",
      "Epoch 999/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2129375734178.3591 - val_loss: 2280362947747.8975\n",
      "Epoch 1000/1000\n",
      "3554/3554 [==============================] - 0s 66us/step - loss: 2128952113051.7319 - val_loss: 2280046858605.2412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09eed84f28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model33.compile(loss='mean_squared_error', optimizer='adagrad')\n",
    "print(\"model33 loss:\",model33.loss)\n",
    "\n",
    "model_train33 = model33.fit(predictors,target, epochs=1000, validation_split=0.5, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model44 loss: mean_squared_error\n",
      "Train on 3554 samples, validate on 3555 samples\n",
      "Epoch 1/1000\n",
      "3554/3554 [==============================] - 1s 150us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/1000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/1000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/1000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/1000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/1000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/1000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/1000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/1000\n",
      "3554/3554 [==============================] - 0s 133us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/1000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/1000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/1000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/1000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/1000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/1000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/1000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/1000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/1000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/1000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/1000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/1000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/1000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: nan - val_loss: nan\n",
      "Epoch 51/1000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: nan - val_loss: nan\n",
      "Epoch 52/1000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: nan - val_loss: nan\n",
      "Epoch 53/1000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: nan - val_loss: nan\n",
      "Epoch 54/1000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: nan - val_loss: nan\n",
      "Epoch 55/1000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: nan - val_loss: nan\n",
      "Epoch 56/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: nan - val_loss: nan\n",
      "Epoch 57/1000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: nan - val_loss: nan\n",
      "Epoch 58/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: nan - val_loss: nan\n",
      "Epoch 59/1000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: nan - val_loss: nan\n",
      "Epoch 60/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: nan - val_loss: nan\n",
      "Epoch 61/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: nan - val_loss: nan\n",
      "Epoch 62/1000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: nan - val_loss: nan\n",
      "Epoch 63/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: nan - val_loss: nan\n",
      "Epoch 64/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: nan - val_loss: nan\n",
      "Epoch 65/1000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: nan - val_loss: nan\n",
      "Epoch 66/1000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: nan - val_loss: nan\n",
      "Epoch 67/1000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: nan - val_loss: nan\n",
      "Epoch 68/1000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: nan - val_loss: nan\n",
      "Epoch 69/1000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: nan - val_loss: nan\n",
      "Epoch 70/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: nan - val_loss: nan\n",
      "Epoch 71/1000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: nan - val_loss: nan\n",
      "Epoch 72/1000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: nan - val_loss: nan\n",
      "Epoch 73/1000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: nan - val_loss: nan\n",
      "Epoch 74/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: nan - val_loss: nan\n",
      "Epoch 75/1000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: nan - val_loss: nan\n",
      "Epoch 76/1000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: nan - val_loss: nan\n",
      "Epoch 77/1000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: nan - val_loss: nan\n",
      "Epoch 78/1000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: nan - val_loss: nan\n",
      "Epoch 79/1000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: nan - val_loss: nan\n",
      "Epoch 80/1000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09f08ef7f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model44.compile(loss='mean_squared_error', optimizer='adadelta')\n",
    "print(\"model44 loss:\",model44.loss)\n",
    "\n",
    "model_train44 = model44.fit(predictors,target, epochs=1000, validation_split=0.5, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/matplotlib/scale.py:111: RuntimeWarning: invalid value encountered in less_equal\n",
      "  out[a <= 0] = -1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09edd222e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(model_train11.history['val_loss'], 'r',\n",
    "         model_train22.history['val_loss'], 'y',\n",
    "         model_train33.history['val_loss'], 'g',\n",
    "         model_train44.history['val_loss'], 'b',\n",
    "         )\n",
    "plt.title(\"red:nadam ,yellow:adam ,green: adagrad, blue:adadelta\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.savefig(\"Adam and Nadam neural network output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[723649646775.19666,\n",
       " 723737742211.27649,\n",
       " 723808350024.80334,\n",
       " 723813076940.15186,\n",
       " 724046376352.80115,\n",
       " 724660692688.40051,\n",
       " 725802096305.29175,\n",
       " 726495769678.63623,\n",
       " 726919090879.98206,\n",
       " 726949965862.02197,\n",
       " 726980712993.55725,\n",
       " 727552681712.66162,\n",
       " 727582389358.89734,\n",
       " 727654810772.91931,\n",
       " 727871918696.41626,\n",
       " 728071305171.64111,\n",
       " 728131904107.8728,\n",
       " 728230106997.16235,\n",
       " 728256628289.53027,\n",
       " 728348933219.95166,\n",
       " 728406161951.25293,\n",
       " 728735997587.91113,\n",
       " 728785393366.1615,\n",
       " 728790085458.02087,\n",
       " 728852715056.24756,\n",
       " 729473498126.69031,\n",
       " 729575366925.61011,\n",
       " 730155611486.55078,\n",
       " 730299721071.8335,\n",
       " 730421405797.9679,\n",
       " 730601307692.21497,\n",
       " 731040705220.01465,\n",
       " 731244109284.77979,\n",
       " 731384017565.41663,\n",
       " 731433915952.24756,\n",
       " 731634168067.24048,\n",
       " 731799954647.74573,\n",
       " 732038985787.33728,\n",
       " 732113967635.73108,\n",
       " 732204191440.97668,\n",
       " 732282088390.10291,\n",
       " 733092568995.82556,\n",
       " 733342534655.71191,\n",
       " 733738138332.21045,\n",
       " 734129513858.26831,\n",
       " 734512144236.521,\n",
       " 734658157213.99268,\n",
       " 734660242367.18982,\n",
       " 734663457772.12488,\n",
       " 734868604589.83521,\n",
       " 735044007027.5061,\n",
       " 735190420650.23462,\n",
       " 735292797120.41406,\n",
       " 735470284904.84839,\n",
       " 735584859968.16199,\n",
       " 736502252063.82898,\n",
       " 736653712624.51758,\n",
       " 736789219098.14014,\n",
       " 737027398785.90833,\n",
       " 737169384004.69873,\n",
       " 737352867835.39124,\n",
       " 737424809573.82385,\n",
       " 737798186600.70435,\n",
       " 737929812306.16479,\n",
       " 738109550593.15222,\n",
       " 738525632707.29456,\n",
       " 738594805194.85571,\n",
       " 738599492302.38428,\n",
       " 738644172341.43237,\n",
       " 738788550792.53333,\n",
       " 738792951249.19263,\n",
       " 739174627878.16602,\n",
       " 739276424785.66077,\n",
       " 739470523849.12744,\n",
       " 739937331306.57666,\n",
       " 739989194141.92065,\n",
       " 740168437509.97693,\n",
       " 740532191434.78369,\n",
       " 740746026524.08435,\n",
       " 740890264164.95972,\n",
       " 741179414131.07397,\n",
       " 741367973167.3114,\n",
       " 741513744263.02112,\n",
       " 741641703618.71838,\n",
       " 741648690634.27966,\n",
       " 741865740771.33948,\n",
       " 741875778994.65991,\n",
       " 741930878937.40198,\n",
       " 742147306611.5061,\n",
       " 742174134775.50269,\n",
       " 742203851639.17859,\n",
       " 742482142559.703,\n",
       " 742767911760.00452,\n",
       " 742800347413.67542,\n",
       " 742810434169.12292,\n",
       " 742908118840.0968,\n",
       " 743155944069.79688,\n",
       " 744065990637.56506,\n",
       " 744078534739.53308,\n",
       " 744103452985.39294,\n",
       " 744847507485.95667,\n",
       " 745240538286.84326,\n",
       " 745278262216.40735,\n",
       " 745468109682.56995,\n",
       " 745555066174.86584,\n",
       " 745617466127.48242,\n",
       " 745702457562.91418,\n",
       " 745756054779.46326,\n",
       " 745761677968.74268,\n",
       " 745936110116.43774,\n",
       " 745942620733.20959,\n",
       " 745947535221.16235,\n",
       " 746141301682.51587,\n",
       " 746343528684.77295,\n",
       " 746555991064.48376,\n",
       " 746820796222.43372,\n",
       " 746833469333.99939,\n",
       " 747606605505.99829,\n",
       " 747757449933.80811,\n",
       " 747766845653.15332,\n",
       " 748288715425.44922,\n",
       " 748604178328.30383,\n",
       " 748863565007.10437,\n",
       " 748954702500.32971,\n",
       " 749333046941.70459,\n",
       " 749384595734.25146,\n",
       " 749395402035.34399,\n",
       " 750360416826.3291,\n",
       " 751836495097.73499,\n",
       " 751858020099.67261,\n",
       " 752083221610.86475,\n",
       " 752177942842.54517,\n",
       " 752199889641.46045,\n",
       " 752262736029.27258,\n",
       " 752616437757.11951,\n",
       " 752654078704.37354,\n",
       " 752765371340.43994,\n",
       " 752817486392.60083,\n",
       " 753181345336.31274,\n",
       " 753425076358.80505,\n",
       " 753526591410.80396,\n",
       " 753807008861.61462,\n",
       " 753905632370.35388,\n",
       " 754024276698.48218,\n",
       " 754126623799.59265,\n",
       " 754330648110.51929,\n",
       " 754645387764.62219,\n",
       " 754784284666.23914,\n",
       " 754982711119.71643,\n",
       " 755043810332.51648,\n",
       " 755212074517.17126,\n",
       " 755231058194.79492,\n",
       " 755418824081.82275,\n",
       " 755420304324.37463,\n",
       " 755690202074.26611,\n",
       " 755737261113.89709,\n",
       " 755856658105.93311,\n",
       " 755972219098.91418,\n",
       " 756117892123.36426,\n",
       " 756123692239.10437,\n",
       " 756164019185.30969,\n",
       " 756439409782.96265,\n",
       " 757377910238.44275,\n",
       " 757480521446.86804,\n",
       " 757645192048.84167,\n",
       " 757763221504.57605,\n",
       " 757855050308.12268,\n",
       " 758046777883.22021,\n",
       " 758052977812.34314,\n",
       " 758107679582.40674,\n",
       " 758283144876.10693,\n",
       " 758293940729.23096,\n",
       " 758422289565.56067,\n",
       " 758475830652.50745,\n",
       " 758502248861.63269,\n",
       " 758809784927.19885,\n",
       " 759093502470.48096,\n",
       " 759219678530.32239,\n",
       " 759251245746.73193,\n",
       " 759522525428.26221,\n",
       " 759938163837.87561,\n",
       " 760080189774.70825,\n",
       " 760241177829.5719,\n",
       " 760891447106.46643,\n",
       " 761000823026.53394,\n",
       " 761039569043.479,\n",
       " 761216748790.27844,\n",
       " 761766121989.32886,\n",
       " 762439643738.59021,\n",
       " 762479184257.40417,\n",
       " 762492594061.35803,\n",
       " 762941100701.41663,\n",
       " 763227050026.63062,\n",
       " 763786844062.64075,\n",
       " 764058324965.49988,\n",
       " 764169317785.88806,\n",
       " 764197135250.83093,\n",
       " 764330057274.04102,\n",
       " 764434343033.26697,\n",
       " 764451039227.96741,\n",
       " 764813688426.43262,\n",
       " 765180326601.19946,\n",
       " 765360502467.72656,\n",
       " 765517401643.06274,\n",
       " 765549214287.64441,\n",
       " 765864000457.55945,\n",
       " 766027238037.92737,\n",
       " 766605321023.87402,\n",
       " 766777946553.28491,\n",
       " 767500396374.05347,\n",
       " 767882510760.0022,\n",
       " 768108160227.26746,\n",
       " 768115479077.58984,\n",
       " 768130660086.42249,\n",
       " 768599452625.62476,\n",
       " 768944762924.93506,\n",
       " 769014117595.20227,\n",
       " 769056341670.34595,\n",
       " 769595546051.07849,\n",
       " 769997406443.04468,\n",
       " 770051917591.83569,\n",
       " 770142715581.38965,\n",
       " 770355177643.09875,\n",
       " 770620456466.86694,\n",
       " 770621339423.901,\n",
       " 770639354873.95105,\n",
       " 770830253107.56006,\n",
       " 770986674234.18506,\n",
       " 771171430486.41345,\n",
       " 771549861795.24951,\n",
       " 771618574185.06445,\n",
       " 771632389703.86719,\n",
       " 772763520449.06213,\n",
       " 772777426957.82617,\n",
       " 772800592283.90442,\n",
       " 773346075345.26465,\n",
       " 773754286724.0686,\n",
       " 774163572828.75049,\n",
       " 774288294288.95862,\n",
       " 774374965957.74292,\n",
       " 774552062941.14661,\n",
       " 775134020422.78711,\n",
       " 775409216691.45203,\n",
       " 775431079630.09619,\n",
       " 775485433524.74829,\n",
       " 775785017126.23792,\n",
       " 776094704374.1344,\n",
       " 776212884581.10376,\n",
       " 776456716168.17322,\n",
       " 776479439259.90442,\n",
       " 776698305857.17017,\n",
       " 777037235530.96375,\n",
       " 777628120379.98535,\n",
       " 777662706589.20056,\n",
       " 777938183842.31335,\n",
       " 778025952474.05005,\n",
       " 778078831366.26501,\n",
       " 778086672656.4906,\n",
       " 778164501601.64722,\n",
       " 778680358570.37866,\n",
       " 779018470417.5708,\n",
       " 779105653433.06897,\n",
       " 779190466409.6405,\n",
       " 779271624376.20483,\n",
       " 779369496622.08716,\n",
       " 779386282889.61353,\n",
       " 779569617194.99072,\n",
       " 779609872835.07849,\n",
       " 779767303981.151,\n",
       " 780870490457.07788,\n",
       " 780946121051.95837,\n",
       " 780998036680.19128,\n",
       " 781070338585.49194,\n",
       " 781213582007.91675,\n",
       " 781407409484.11584,\n",
       " 781689881437.83069,\n",
       " 781886897203.56006,\n",
       " 782227353981.37158,\n",
       " 782247250798.53723,\n",
       " 782384997723.09424,\n",
       " 782486912535.47571,\n",
       " 782720346568.83936,\n",
       " 783417141799.31812,\n",
       " 783643020575.75696,\n",
       " 783880105388.32288,\n",
       " 783926708853.66638,\n",
       " 784513864165.06775,\n",
       " 784531923416.3938,\n",
       " 784945917798.76001,\n",
       " 785039294975.56799,\n",
       " 785039692030.63184,\n",
       " 785871790764.3949,\n",
       " 786331057162.08154,\n",
       " 786345050227.21802,\n",
       " 786590756346.67114,\n",
       " 786633682260.7572,\n",
       " 786751151475.86609,\n",
       " 786819811957.66638,\n",
       " 786880965903.62646,\n",
       " 787289383601.00366,\n",
       " 787901239946.98169,\n",
       " 787990701527.52966,\n",
       " 788034492894.73083,\n",
       " 788096289406.88379,\n",
       " 788162613737.67651,\n",
       " 788303926147.85266,\n",
       " 788325226177.42224,\n",
       " 788784011157.42334,\n",
       " 788915617289.64954,\n",
       " 788962755578.5271,\n",
       " 789079229512.87537,\n",
       " 789283845841.55273,\n",
       " 789561087423.33386,\n",
       " 789737543347.59607,\n",
       " 789982290793.35242,\n",
       " 790092364731.73328,\n",
       " 790195504698.61719,\n",
       " 790440282114.88049,\n",
       " 790474470711.37671,\n",
       " 791258348931.99658,\n",
       " 791286548985.23096,\n",
       " 791297212757.04529,\n",
       " 791426558906.58118,\n",
       " 791637614552.82593,\n",
       " 791976447084.01685,\n",
       " 792000066419.72205,\n",
       " 792564600736.36902,\n",
       " 792602547370.81067,\n",
       " 792823323724.90796,\n",
       " 792877854474.58569,\n",
       " 792897041811.55103,\n",
       " 793189608613.62585,\n",
       " 793248271343.58142,\n",
       " 793427427742.78479,\n",
       " 793613778727.39014,\n",
       " 793791444260.94177,\n",
       " 793907567268.90576,\n",
       " 793909343889.03064,\n",
       " 793924694664.67737,\n",
       " 794186247172.32068,\n",
       " 794233414058.30664,\n",
       " 794423630989.43005,\n",
       " 794951802522.82422,\n",
       " 795164665732.14062,\n",
       " 795317259344.3645,\n",
       " 795365353265.18372,\n",
       " 795639823554.43042,\n",
       " 795701575633.91284,\n",
       " 795784335748.28467,\n",
       " 795955394441.90149,\n",
       " 796044340624.67065,\n",
       " 796173446729.01941,\n",
       " 796173784414.55078,\n",
       " 796260266890.76562,\n",
       " 796424844083.19995,\n",
       " 796509437025.64722,\n",
       " 796607306699.86389,\n",
       " 796710295915.51282,\n",
       " 796834098313.97351,\n",
       " 796926030551.02563,\n",
       " 797057077885.15552,\n",
       " 797100568657.80481,\n",
       " 797227798764.1969,\n",
       " 797474602459.56226,\n",
       " 797510375709.16455,\n",
       " 797832618373.43689,\n",
       " 797964112748.80896,\n",
       " 798205322250.65771,\n",
       " 798263282215.31812,\n",
       " 798285854684.85852,\n",
       " 798353939164.21045,\n",
       " 798405153898.28857,\n",
       " 798632116371.479,\n",
       " 798731182285.08801,\n",
       " 798761811488.69312,\n",
       " 798808828199.53418,\n",
       " 798909495408.33752,\n",
       " 798919817028.77075,\n",
       " 798942277810.58789,\n",
       " 799185682588.69653,\n",
       " 799628737702.77808,\n",
       " 799658901610.00061,\n",
       " 799705155679.05481,\n",
       " 799769603790.67224,\n",
       " 799875942236.96655,\n",
       " 799933296878.50122,\n",
       " 800145172660.31616,\n",
       " 800234683328.34204,\n",
       " 800522422938.53613,\n",
       " 800807970913.35925,\n",
       " 800953674286.2312,\n",
       " 800961123660.11584,\n",
       " 801208202007.54773,\n",
       " 801429248298.99072,\n",
       " 801467605948.30945,\n",
       " 801495045230.89734,\n",
       " 801617023779.06946,\n",
       " 801650822381.06104,\n",
       " 801962969768.3623,\n",
       " 801980392140.65601,\n",
       " 801984609829.30188,\n",
       " 802429658726.40002,\n",
       " 802494148574.58679,\n",
       " 802746677917.99268,\n",
       " 802849001182.80286,\n",
       " 802882833949.23657,\n",
       " 803010585412.48267,\n",
       " 803016709396.81128,\n",
       " 803291191917.60107,\n",
       " 803509041149.11951,\n",
       " 803541803369.78455,\n",
       " 803662305399.25061,\n",
       " 804147916596.06409,\n",
       " 804415907969.62024,\n",
       " 804447489026.01636,\n",
       " 804605809678.11426,\n",
       " 804890047734.85461,\n",
       " 805005076056.57385,\n",
       " 805168667895.43066,\n",
       " 805178141280.92712,\n",
       " 805294046605.21411,\n",
       " 805409487098.59912,\n",
       " 805421198575.36536,\n",
       " 805865553159.56116,\n",
       " 806048753346.57446,\n",
       " 806555953774.7533,\n",
       " 806618254711.32263,\n",
       " 806714643917.16003,\n",
       " 806859562669.25903,\n",
       " 807287167413.25232,\n",
       " 807375423725.92517,\n",
       " 807527829094.97607,\n",
       " 807562792753.75977,\n",
       " 807814390868.39722,\n",
       " 808046978187.70178,\n",
       " 808198025417.63147,\n",
       " 808526579230.96484,\n",
       " 808710739598.72632,\n",
       " 808819750724.77075,\n",
       " 809018575574.44946,\n",
       " 809241888618.79272,\n",
       " 809363548626.05676,\n",
       " 809407339366.47205,\n",
       " 809644141175.9707,\n",
       " 810133010240.16199,\n",
       " 810343231800.24084,\n",
       " 810362130870.98059,\n",
       " 810498718112.51306,\n",
       " 810650332122.84216,\n",
       " 810937534771.05603,\n",
       " 810967274994.02979,\n",
       " 810988914658.61938,\n",
       " 811200255418.72522,\n",
       " 811251227927.69165,\n",
       " 811267843764.74829,\n",
       " 811297939066.56311,\n",
       " 811420915112.29028,\n",
       " 811498698191.46436,\n",
       " 811574069398.3595,\n",
       " 811933949705.43347,\n",
       " 812326261138.11084,\n",
       " 812609763956.51416,\n",
       " 812852878850.73645,\n",
       " 812944571434.34265,\n",
       " 812968177538.41235,\n",
       " 813386479239.81323,\n",
       " 813558062760.65039,\n",
       " 813662617328.08557,\n",
       " 813693934853.25684,\n",
       " 814017269927.06616,\n",
       " 814125522808.04272,\n",
       " 814152228156.56152,\n",
       " 814368569096.85742,\n",
       " 814469427908.5907,\n",
       " 814593789704.28125,\n",
       " 814794079905.7373,\n",
       " 814911032119.23267,\n",
       " 814980683328.37805,\n",
       " 815033166741.71143,\n",
       " 815086409602.98853,\n",
       " 815433676391.55212,\n",
       " 815471326348.854,\n",
       " 815492520124.38147,\n",
       " 815999757016.75391,\n",
       " 816097694964.55017,\n",
       " 816098405219.0155,\n",
       " 816131793815.4397,\n",
       " 816206827650.19629,\n",
       " 816420142518.6925,\n",
       " 816438579980.02588,\n",
       " 817132868018.37183,\n",
       " 817309374841.33899,\n",
       " 817367334036.34314,\n",
       " 817393929158.10291,\n",
       " 817453805414.76001,\n",
       " 817601205847.42163,\n",
       " 817827745018.88721,\n",
       " 818025039538.44385,\n",
       " 818068497008.48157,\n",
       " 818305701643.16174,\n",
       " 818804314053.23877,\n",
       " 818905989746.78589,\n",
       " 818938978204.91248,\n",
       " 819825313264.87769,\n",
       " 820346127414.44055,\n",
       " 820354657749.80139,\n",
       " 820520239099.39124,\n",
       " 820783766627.95166,\n",
       " 820827475812.45569,\n",
       " 820873326941.1106,\n",
       " 821680051905.42224,\n",
       " 821845841486.20422,\n",
       " 821857221282.60144,\n",
       " 822358762345.35242,\n",
       " 822521667097.78003,\n",
       " 822549012016.82361,\n",
       " 822566360575.85596,\n",
       " 823016616649.19946,\n",
       " 823020130005.58533,\n",
       " 823085687187.26306,\n",
       " 823233312428.68298,\n",
       " 823270799094.42249,\n",
       " 823822127902.46082,\n",
       " 823924375353.82507,\n",
       " 823929991848.93835,\n",
       " 824135741973.45935,\n",
       " 824326617790.25378,\n",
       " 824342726585.14087,\n",
       " 824458741255.92126,\n",
       " 824506531236.25769,\n",
       " 824689128602.68018,\n",
       " 824740991069.32654,\n",
       " 825015075461.22083,\n",
       " 825582802943.42395,\n",
       " 825919996171.30579,\n",
       " 826021100595.56006,\n",
       " 826351553726.97388,\n",
       " 826358912885.16235,\n",
       " 826556622636.57495,\n",
       " 826665953239.09766,\n",
       " 826801580765.36255,\n",
       " 826816350008.0968,\n",
       " 827270022184.61438,\n",
       " 827273516101.41882,\n",
       " 827404050060.99805,\n",
       " 828152941740.53894,\n",
       " 828245742099.15503,\n",
       " 828576260941.7002,\n",
       " 828787195857.91284,\n",
       " 828894320302.9873,\n",
       " 829381633933.35803,\n",
       " 829528898118.427,\n",
       " 830249147284.5592,\n",
       " 830686036401.21973,\n",
       " 830890854660.96875,\n",
       " 831208200988.73254,\n",
       " 831545490557.29956,\n",
       " 831930729701.5719,\n",
       " 831968493047.2146,\n",
       " 832932489013.79236,\n",
       " 833053011782.49902,\n",
       " 833970999108.1947,\n",
       " 834505494824.11023,\n",
       " 834520377690.2301,\n",
       " 834786107412.73926,\n",
       " 835033726542.78027,\n",
       " 835827858027.29675,\n",
       " 835979178070.41345,\n",
       " 836343532297.14539,\n",
       " 836395353257.65857,\n",
       " 836996072566.96265,\n",
       " 837009964281.73499,\n",
       " 837185983414.54858,\n",
       " 837480694764.70093,\n",
       " 837497931373.31311,\n",
       " 837589409499.34631,\n",
       " 837642227699.32605,\n",
       " 837807106182.229,\n",
       " 837859633917.62366,\n",
       " 838348026933.28833,\n",
       " 838436810414.41125,\n",
       " 838609572666.11304,\n",
       " 838694432946.58789,\n",
       " 838952715198.61377,\n",
       " 839577969259.8728,\n",
       " 839653712192.30603,\n",
       " 839971634149.21179,\n",
       " 840657771827.34399,\n",
       " 840859642782.92883,\n",
       " 840921472321.17017,\n",
       " 841309725964.74597,\n",
       " 841692121694.6228,\n",
       " 841728653590.25146,\n",
       " 842260756790.51257,\n",
       " 842481812529.83179,\n",
       " 843096592288.36902,\n",
       " 843202982877.72266,\n",
       " 843488347268.7887,\n",
       " 843666159691.46777,\n",
       " 843968957322.18958,\n",
       " 844312283530.6217,\n",
       " 844427744459.07178,\n",
       " 844578894071.71875,\n",
       " 844596799926.40454,\n",
       " 845144832513.29614,\n",
       " 845176568777.55945,\n",
       " 846622400772.3927,\n",
       " 846696566130.71387,\n",
       " 847227268409.10498,\n",
       " 847637613528.53784,\n",
       " 847675456263.99329,\n",
       " 848044275718.33704,\n",
       " 848240428690.18286,\n",
       " 848343583192.96985,\n",
       " 848406678783.49597,\n",
       " 848610524382.08264,\n",
       " 848688655221.45032,\n",
       " 849169629437.47961,\n",
       " 849473540166.28296,\n",
       " 849655252285.13757,\n",
       " 849665049806.52832,\n",
       " 851469820279.61072,\n",
       " 851595893034.99072,\n",
       " 851627840461.59216,\n",
       " 851816804493.71814,\n",
       " 851976900719.76147,\n",
       " 852186292977.52576,\n",
       " 852449997397.98145,\n",
       " 852492007448.77185,\n",
       " 852782334776.96094,\n",
       " 852795158420.84729,\n",
       " 853882831096.87085,\n",
       " 853910085242.56311,\n",
       " 854185553430.32349,\n",
       " 854493496245.39636,\n",
       " 854773107737.05994,\n",
       " 855204425030.93103,\n",
       " 856938429528.42981,\n",
       " 857036563982.83435,\n",
       " 857380412995.25854,\n",
       " 857858698373.36487,\n",
       " 857942747897.01489,\n",
       " 857951922513.0127,\n",
       " 857990092421.50891,\n",
       " 859466577142.85461,\n",
       " 859495124864.10803,\n",
       " 860302812078.48328,\n",
       " 861608902443.9989,\n",
       " 862603498874.20312,\n",
       " 863372988242.02087,\n",
       " 864922464443.8053,\n",
       " 865281263701.8374,\n",
       " 865396089815.96179,\n",
       " 865525982038.9176,\n",
       " 865894568669.93872,\n",
       " 866002171269.72485,\n",
       " 866187867728.50854,\n",
       " 866381022995.22705,\n",
       " 866411909853.93872,\n",
       " 867097084742.78711,\n",
       " 867612320322.39441,\n",
       " 868451438491.76038,\n",
       " 868644532232.06531,\n",
       " 868864766190.50122,\n",
       " 869936969531.55334,\n",
       " 870644608087.27759,\n",
       " 870931165436.32739,\n",
       " 870967807664.71558,\n",
       " 870992827696.46362,\n",
       " 871406958713.55505,\n",
       " 871602968391.07507,\n",
       " 872747055838.51477,\n",
       " 873653025603.90662,\n",
       " 874833903579.13025,\n",
       " 876706893433.41101,\n",
       " 876752968135.68726,\n",
       " 877703623433.72156,\n",
       " 878026830320.5896,\n",
       " 880062237131.1438,\n",
       " 880270307395.97864,\n",
       " 880671153795.78064,\n",
       " 881736619805.59668,\n",
       " 882857568122.92322,\n",
       " 883509502369.08911,\n",
       " 883832401015.5387,\n",
       " 884753883373.34912,\n",
       " 885082680748.32288,\n",
       " 885226164999.41711,\n",
       " 885489322005.31531,\n",
       " 886197254071.98877,\n",
       " 888323361102.70825,\n",
       " 888890175606.67456,\n",
       " 890731614292.10913,\n",
       " 891010652210.11987,\n",
       " 891172361875.33496,\n",
       " 891220209349.45483,\n",
       " 891262044831.1449,\n",
       " 892320545239.81775,\n",
       " 894566088846.87036,\n",
       " 895345044415.18982,\n",
       " 895418222392.0968,\n",
       " 896932480856.35779,\n",
       " 899943869660.93054,\n",
       " 899997832316.72351,\n",
       " 900544234688.70215,\n",
       " 901517447754.45959,\n",
       " 902436062695.66016,\n",
       " 903528860139.40479,\n",
       " 903562681222.73303,\n",
       " 903766054925.25012,\n",
       " 904647823195.23828,\n",
       " 907200539103.88293,\n",
       " 908093927127.88977,\n",
       " 909540838171.58032,\n",
       " 909762330264.51978,\n",
       " 909936400028.55249,\n",
       " 911828495073.97131,\n",
       " 911835789691.64331,\n",
       " 911971700525.151,\n",
       " 912960085774.04224,\n",
       " 915878988829.66858,\n",
       " 916401707468.2959,\n",
       " 917047554056.06531,\n",
       " 917816072974.90637,\n",
       " 918436880109.78113,\n",
       " 919483274728.81238,\n",
       " 920217918411.86389,\n",
       " 920804941130.38757,\n",
       " 921392839671.64673,\n",
       " 924747016893.10156,\n",
       " 925575747726.29419,\n",
       " 926086572216.92493,\n",
       " 927120044528.5896,\n",
       " 927364328456.92944,\n",
       " 927541279048.94739,\n",
       " 928134389988.13159,\n",
       " 929750151106.93445,\n",
       " 930620597862.40002,\n",
       " 931827865368.98792,\n",
       " 934546634126.94238,\n",
       " 934738508657.99377,\n",
       " 935358815838.91089,\n",
       " 939474903424.82812,\n",
       " 939619903474.74988,\n",
       " 939682001443.28552,\n",
       " 942070502644.55017,\n",
       " 942196516537.35693,\n",
       " 944560611616.04504,\n",
       " 945374538281.62256,\n",
       " 945565141082.44617,\n",
       " 950248103546.56311,\n",
       " 950605527673.12292,\n",
       " 951523488120.76282,\n",
       " 951801981647.53638,\n",
       " 952877322033.75977,\n",
       " 953149848189.73169,\n",
       " 953408449079.44861,\n",
       " 954570194303.38794,\n",
       " 955093917716.73926,\n",
       " 956663285302.00842,\n",
       " 959330843768.69092,\n",
       " 960862947455.31592,\n",
       " 961780819175.87622,\n",
       " 963070721310.60474,\n",
       " 965318143966.58679,\n",
       " 965463468802.2323,\n",
       " 966265734226.09277,\n",
       " 966705003691.38672,\n",
       " 967383793797.65283,\n",
       " 969208528038.77808,\n",
       " 970068254763.78284,\n",
       " 971486930640.40051,\n",
       " 972213744937.55054,\n",
       " 972821975981.33105,\n",
       " 974045734944.83716,\n",
       " 976518859381.66638,\n",
       " 978563247564.58398,\n",
       " 980417133453.64612,\n",
       " 981482511980.73694,\n",
       " 983839502662.06689,\n",
       " 983853607060.05518,\n",
       " 986655804547.06042,\n",
       " 987284288445.46155,\n",
       " 987414910837.16235,\n",
       " 987590273798.84106,\n",
       " 988841519740.57947,\n",
       " 992828893276.75049,\n",
       " 992964965738.07263,\n",
       " 993241515380.44214,\n",
       " 993587983141.37378,\n",
       " 996159357766.78711,\n",
       " 996386799708.17444,\n",
       " 999146580858.05908,\n",
       " 999275985423.12231,\n",
       " 999974830651.48132,\n",
       " 1002233589523.8031,\n",
       " 1004600066720.8732,\n",
       " 1004825112219.9763,\n",
       " 1005494480424.7584,\n",
       " 1006217241020.7415,\n",
       " 1006410361067.6208,\n",
       " 1009214852130.2773,\n",
       " 1010033690548.5322,\n",
       " 1011189454133.0723,\n",
       " 1015647754027.4227,\n",
       " 1016153813838.8523,\n",
       " 1017183105958.418,\n",
       " 1017871010675.146,\n",
       " 1022558083564.557,\n",
       " 1024372982170.1761,\n",
       " 1026620658840.3759,\n",
       " 1026978848346.3021,\n",
       " 1029094502329.717,\n",
       " 1029930344454.9131,\n",
       " 1030200467624.2183,\n",
       " 1030584524180.1272,\n",
       " 1031063929845.9185,\n",
       " 1031887647350.2424,\n",
       " 1032204104969.2894,\n",
       " 1035233095784.8484,\n",
       " 1036628199606.6205,\n",
       " 1036633180674.4484,\n",
       " 1037644500186.6262,\n",
       " 1040886088975.0503,\n",
       " 1042080685192.8214,\n",
       " 1044320441943.4216,\n",
       " 1045016733129.7035,\n",
       " 1046010517826.0343,\n",
       " 1048537723399.6332,\n",
       " 1051622546462.8208,\n",
       " 1052273135773.8486,\n",
       " 1054103113890.4574,\n",
       " 1056653672365.0431,\n",
       " 1057947035630.1412,\n",
       " 1059876248745.0824,\n",
       " 1060251420905.3164,\n",
       " 1062692611133.3536,\n",
       " 1069017546682.8691,\n",
       " 1070032454581.3964,\n",
       " 1070949433814.0895,\n",
       " 1071319111126.3774,\n",
       " 1073192031674.7252,\n",
       " 1074800132437.0453,\n",
       " 1076525132315.7964,\n",
       " 1076652601107.8031,\n",
       " 1078910940918.4225,\n",
       " 1080869444477.5156,\n",
       " 1089338334390.9086,\n",
       " 1089568495974.9041,\n",
       " 1091831991910.9761,\n",
       " 1093001943974.9941,\n",
       " 1096913540529.5077,\n",
       " 1097452599071.901,\n",
       " 1102376313258.3066,\n",
       " 1104633369337.5911,\n",
       " 1104794014794.0276,\n",
       " 1106088987239.8403,\n",
       " 1108805729340.7776,\n",
       " 1109838300914.9661,\n",
       " 1114750834973.1646,\n",
       " 1118945321146.6531,\n",
       " 1122245791311.9324,\n",
       " 1126133493496.1509,\n",
       " 1127255217073.9397,\n",
       " 1128869565712.2026,\n",
       " 1138830262857.3074,\n",
       " 1145315498209.2512,\n",
       " 1150845968980.8293,\n",
       " 1153655697651.1101,\n",
       " 1162511637096.9924,\n",
       " 1163069214546.0208,\n",
       " 1170135765057.6743,\n",
       " 1171203809250.0432,\n",
       " 1175445344298.6306,\n",
       " 1190857692704.1169,\n",
       " 1192585951611.6433,\n",
       " 1193379296028.1565,\n",
       " 1195996862638.5552,\n",
       " 1205622836516.3657,\n",
       " 1207236364581.2297,\n",
       " 1222876929875.1731,\n",
       " 1223475075306.4688,\n",
       " 1234641994405.4819,\n",
       " 1236298846246.022,\n",
       " 1253560523567.1675,\n",
       " 1255068255622.877,\n",
       " 1266829830725.2749,\n",
       " 1286976505043.137,\n",
       " 1287584982819.3574,\n",
       " 1291457875563.5847,\n",
       " 1298430750151.9753,\n",
       " 1320863200544.333,\n",
       " 1321389051711.5859,\n",
       " 1334351262187.981,\n",
       " 1344831686248.7043,\n",
       " 1355267017559.4937,\n",
       " 1363061009227.9719,\n",
       " 1373108715395.5645,\n",
       " 1386260878363.9404,\n",
       " 1396996815877.4729,\n",
       " 1420159738084.1316,\n",
       " 1420709041000.2002,\n",
       " 1432407891200.6482,\n",
       " 1449826598123.9089,\n",
       " 1463562141401.042,\n",
       " 1486561264533.1353,\n",
       " 1490581280638.3796,\n",
       " 1501361341079.6558,\n",
       " 1513716969733.5449,\n",
       " 1538914316494.2402,\n",
       " 1551800572276.7302,\n",
       " 1565920923294.2808,\n",
       " 1571296422791.8853,\n",
       " 1579687017334.6025,\n",
       " 1596832330408.9385,\n",
       " 1609419473287.7412,\n",
       " 1651413515596.6919,\n",
       " 1654895091871.0007,\n",
       " 1659580203289.1318,\n",
       " 1672952984140.7639,\n",
       " 1692137444343.0706,\n",
       " 1718832874416.7876,\n",
       " 1733775730402.8354,\n",
       " 1737434656937.3704,\n",
       " 1750243139240.6504,\n",
       " 1766188272614.364,\n",
       " 1783099959416.4028,\n",
       " 1814685011609.6721,\n",
       " 1822855854044.8584,\n",
       " 1832219048767.2979,\n",
       " 1848766264709.7249,\n",
       " 1900633764004.7617,\n",
       " 1913307231087.1133,\n",
       " 1914116422384.9497,\n",
       " 1952618858554.1851,\n",
       " 1953940671964.7146,\n",
       " 1975744896030.8208,\n",
       " 2012086266285.187,\n",
       " 2014208731108.9238,\n",
       " 2036065711415.3767,\n",
       " 2041226487840.8372,\n",
       " 2067915659715.0784,\n",
       " 2084443969765.8599,\n",
       " 2105245158696.9744,\n",
       " 2171835594297.4651,\n",
       " 2192271394496.27,\n",
       " 2193719482819.0784,\n",
       " 2208344862032.1484,\n",
       " 2242772784165.1577,\n",
       " 2276102805895.165,\n",
       " 2327871247682.8984,\n",
       " 2334812071203.7896,\n",
       " 2387745424148.0913,\n",
       " 2411277100411.3555,\n",
       " 2452078284506.1943,\n",
       " 2482642627712.7563,\n",
       " 2525052158970.5273,\n",
       " 2565307664815.4912,\n",
       " 2619654179247.4912,\n",
       " 2656390277198.9243,\n",
       " 2703997217681.1025,\n",
       " 2753863009504.9634,\n",
       " 2783512823548.1836,\n",
       " 2831354202064.1846,\n",
       " 2878106936441.2671,\n",
       " 2926767086714.9951,\n",
       " 2987382929022.5957,\n",
       " 3062403328438.4043,\n",
       " 3111745408794.7163,\n",
       " 3131213985567.9009,\n",
       " 3188916416202.0635,\n",
       " 3234845115921.1387,\n",
       " 3290952201734.769,\n",
       " 3342830057708.7729,\n",
       " 3435989107333.2207,\n",
       " 3472475076807.9033,\n",
       " 3529670765023.8828,\n",
       " 3571640150790.8413,\n",
       " 3673080200478.605,\n",
       " 3708939334907.4634,\n",
       " 3751189346195.1191,\n",
       " 3857611676062.2085,\n",
       " 3885827863393.2871,\n",
       " 3965694398225.4985,\n",
       " 4046929886190.4292,\n",
       " 4151608436234.2256,\n",
       " 4224997952030.1006,\n",
       " 4341025335503.9683,\n",
       " 4474286502628.8516,\n",
       " 4630857307726.2041,\n",
       " 4842817674303.082,\n",
       " 5038690293058.8984,\n",
       " 5336898414104.0518,\n",
       " 5660149628821.999,\n",
       " 6131061439631.7344,\n",
       " 6694279874807.1426,\n",
       " 7633188115318.0264,\n",
       " 27363463878968.816,\n",
       " 117877849025853.72,\n",
       " 132174084957752.31]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(model_train22.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model11.save(\"model_train11.h5\")\n",
    "model22.save(\"model_train22.h5\")\n",
    "model33.save(\"model_train33.h5\")\n",
    "model44.save(\"model_train44.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
