{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing pandas to visualise the dataset in tabular format\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "\n",
    "import numpy as np\n",
    "#introducing stable analysis through random seed\n",
    "np.random.seed(42)\n",
    "#importing time function to create animated behaviour\n",
    "import time\n",
    "#importing regExp for data cleaning\n",
    "import re\n",
    "from numpy import NaN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/scaled_train.csv\")\n",
    "test = pd.read_csv(\"../data/scaled_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PRT_ID  AREA  INT_SQFT  DIST_MAINROAD  N_BEDROOM  N_BATHROOM  N_ROOM  SALE_COND  PARK_FACIL  BUILDTYPE  UTILITY_AVAIL  STREET  MZZONE  QS_ROOMS  \\\n",
      "0  P03210     1    0.4016          0.655        1.0         1.0       3         -2           1         -1              1       1      -1      0.80   \n",
      "1  P09411    -3    0.7944          0.130        2.0         1.0       5         -2           0         -1              1       0       2      0.98   \n",
      "2  P01812    -2    0.3636          0.350        1.0         1.0       3         -2           1         -1             -1       0       1      0.82   \n",
      "3  P05346     3    0.7420          0.070        3.0         2.0       5         -1           0          1             -2       1      -2      0.94   \n",
      "4  P06210     1    0.4904          0.420        1.0         1.0       3         -2           1          1              1       0       0      0.60   \n",
      "\n",
      "   QS_BATHROOM  QS_BEDROOM  QS_OVERALL   REG_FEE    COMMIS  SALES_PRICE  DATE_DIFF      YEAR  \n",
      "0         0.78        0.98    0.871227  0.386209  0.291479      7600000   0.800000  0.978607  \n",
      "1         0.84        0.50    0.757545  0.772543  0.613738     21717770   0.200000  0.992537  \n",
      "2         0.76        0.44    0.621730  0.427975  0.185937     13159200   0.363636  0.991045  \n",
      "3         0.78        0.72    0.806841  0.362144  0.155513      9630290   0.400000  0.989055  \n",
      "4         0.50        0.82    0.661972  0.240873  0.149500      7406250   0.545455  0.984577  \n",
      "   PRT_ID  AREA  INT_SQFT  DIST_MAINROAD  N_BEDROOM  N_BATHROOM  N_ROOM  SALE_COND  PARK_FACIL  BUILDTYPE  UTILITY_AVAIL  STREET  MZZONE  QS_ROOMS  \\\n",
      "0  P05996     0    0.3832          0.925          1           1       3         -2           0          1             -2       1       2      0.62   \n",
      "1  P09294    -3    0.7228          0.540          2           1       5          0           1          1              0      -1       3      0.52   \n",
      "2  P03807     1    0.6632          0.295          2           2       4          0           0          1              0       1      -2      0.50   \n",
      "3  P00539    -3    0.6368          0.510          1           1       4         -2           1          1              1       0       1      0.82   \n",
      "4  P01448     0    0.3428          0.310          1           1       3         -1           0          0             -1       0       2      0.60   \n",
      "\n",
      "   QS_BATHROOM  QS_BEDROOM  QS_OVERALL   REG_FEE    COMMIS  DATE_DIFF      YEAR  \n",
      "0         0.88        0.78    0.778226  0.197457  0.182269   0.490909  0.986070  \n",
      "1         0.82        0.42    0.559476  0.359835  0.431801   0.309091  0.990050  \n",
      "2         0.46        0.64    0.548387  0.376895  0.219856   0.290909  0.992537  \n",
      "3         0.96        0.50    0.732863  0.396482  0.164062   0.345455  0.992537  \n",
      "4         0.90        0.58    0.719758  0.176038  0.067708   0.618182  0.984080  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df.drop(['PRT_ID','SALES_PRICE'],axis=1)\n",
    "y_train = df['SALES_PRICE']\n",
    "\n",
    "X_test = test.drop(['PRT_ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "predictors = X_train.values\n",
    "target = y_train.values.reshape((-1,1)).astype('int32')\n",
    "\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model 1\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "model1.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 2\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "model2.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 3\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "model3.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 4\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n",
    "model4.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model4.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1 loss: mean_squared_error\n",
      "model2 loss: mean_squared_error\n",
      "model3 loss: mean_squared_error\n",
      "model4 loss: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model1.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "print(\"model1 loss:\",model1.loss)\n",
    "\n",
    "# Compile model\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(\"model2 loss:\",model2.loss)\n",
    "\n",
    "# Compile model\n",
    "model3.compile(loss='mean_squared_error', optimizer='adadelta')\n",
    "print(\"model3 loss:\",model3.loss)\n",
    "\n",
    "# Compile model\n",
    "model4.compile(loss='mean_squared_error', optimizer='adagrad')\n",
    "print(\"model4 loss:\",model4.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4265 samples, validate on 2844 samples\n",
      "Epoch 1/800\n",
      "4265/4265 [==============================] - 1s 186us/step - loss: 1702838181125.4622 - val_loss: 1787209751066.6440\n",
      "Epoch 2/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1702799760756.8657 - val_loss: 1785909620799.3699\n",
      "Epoch 3/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1701626629247.9700 - val_loss: 1778349823271.2461\n",
      "Epoch 4/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1705025336503.1917 - val_loss: 1774937926936.8440\n",
      "Epoch 5/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1698861953889.2981 - val_loss: 1773848201334.0984\n",
      "Epoch 6/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1699991222053.5146 - val_loss: 1772606558121.5864\n",
      "Epoch 7/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1696620308746.5042 - val_loss: 1774119596694.5034\n",
      "Epoch 8/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1693991599060.0630 - val_loss: 1777794161869.9521\n",
      "Epoch 9/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1695329889986.1157 - val_loss: 1771634423643.8145\n",
      "Epoch 10/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1691976828762.3352 - val_loss: 1766358121021.2095\n",
      "Epoch 11/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1688413569799.9832 - val_loss: 1763419941938.4080\n",
      "Epoch 12/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1685711248003.9314 - val_loss: 1759953956374.3235\n",
      "Epoch 13/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1683878746629.1619 - val_loss: 1758686448102.7961\n",
      "Epoch 14/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1685317120969.0186 - val_loss: 1755709450323.5330\n",
      "Epoch 15/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1680982255986.2246 - val_loss: 1757523610132.8833\n",
      "Epoch 16/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1678247339073.0654 - val_loss: 1752262430011.4092\n",
      "Epoch 17/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1675897802758.7227 - val_loss: 1751479009717.8284\n",
      "Epoch 18/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1678554406840.9321 - val_loss: 1749148436736.3601\n",
      "Epoch 19/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1675249734387.8152 - val_loss: 1758401102699.6567\n",
      "Epoch 20/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1671061930575.5911 - val_loss: 1747352077463.2236\n",
      "Epoch 21/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1665077110528.0601 - val_loss: 1763077528781.9521\n",
      "Epoch 22/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1671995252184.7446 - val_loss: 1743598634043.0493\n",
      "Epoch 23/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1665112391161.3975 - val_loss: 1735845918256.2476\n",
      "Epoch 24/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1656662767578.0652 - val_loss: 1736891593896.5063\n",
      "Epoch 25/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1655218168209.4368 - val_loss: 1727116892859.9495\n",
      "Epoch 26/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1652595616386.9712 - val_loss: 1726632357098.7566\n",
      "Epoch 27/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1650662565086.5669 - val_loss: 1719442522365.4795\n",
      "Epoch 28/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1648081978861.3928 - val_loss: 1719407451295.8650\n",
      "Epoch 29/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1646711708533.9463 - val_loss: 1713016025520.0676\n",
      "Epoch 30/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1640337266778.7556 - val_loss: 1707619014618.5542\n",
      "Epoch 31/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1634440890797.7678 - val_loss: 1708054064708.4106\n",
      "Epoch 32/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1632218513392.6340 - val_loss: 1698819136049.6877\n",
      "Epoch 33/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1626074578682.7781 - val_loss: 1731423012521.2266\n",
      "Epoch 34/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1625291219022.7507 - val_loss: 1696476514952.1013\n",
      "Epoch 35/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1616366147279.0811 - val_loss: 1691045451323.7693\n",
      "Epoch 36/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1615342917377.0203 - val_loss: 1688265316498.9028\n",
      "Epoch 37/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1606531856883.8752 - val_loss: 1677647183549.3896\n",
      "Epoch 38/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1604599241547.6895 - val_loss: 1672844062829.4570\n",
      "Epoch 39/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1597697091013.0571 - val_loss: 1666992904592.3826\n",
      "Epoch 40/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1594781046517.4958 - val_loss: 1677695077921.8452\n",
      "Epoch 41/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1593002451875.5640 - val_loss: 1660038159565.9521\n",
      "Epoch 42/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1584328614915.6013 - val_loss: 1658569112885.6484\n",
      "Epoch 43/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1579826083383.3416 - val_loss: 1647734199936.9001\n",
      "Epoch 44/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1577899005524.1528 - val_loss: 1647842454163.6230\n",
      "Epoch 45/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1567934709775.3660 - val_loss: 1646729629655.6736\n",
      "Epoch 46/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1562480700288.5103 - val_loss: 1634978207268.7258\n",
      "Epoch 47/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1558930213470.4768 - val_loss: 1626695632645.4009\n",
      "Epoch 48/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1552236842745.8174 - val_loss: 1622850903015.5161\n",
      "Epoch 49/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1547794514565.3721 - val_loss: 1628269108314.7341\n",
      "Epoch 50/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1545150347663.9963 - val_loss: 1610257726289.7327\n",
      "Epoch 51/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1536999513368.6697 - val_loss: 1605726858705.1926\n",
      "Epoch 52/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1532275071254.9890 - val_loss: 1599454461421.9971\n",
      "Epoch 53/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1525988902266.1477 - val_loss: 1595616722298.7793\n",
      "Epoch 54/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1523973617222.7075 - val_loss: 1588959523616.7651\n",
      "Epoch 55/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1513144963804.7664 - val_loss: 1583290346039.4487\n",
      "Epoch 56/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1511157995384.1069 - val_loss: 1577018664767.0098\n",
      "Epoch 57/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1502581312678.3848 - val_loss: 1574795655994.6892\n",
      "Epoch 58/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1500023439340.7925 - val_loss: 1572056282902.6836\n",
      "Epoch 59/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1493915107520.0752 - val_loss: 1562570522992.6975\n",
      "Epoch 60/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1487792726317.5579 - val_loss: 1554863444042.8916\n",
      "Epoch 61/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1482329510659.1814 - val_loss: 1569353773303.7188\n",
      "Epoch 62/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1478605214501.7549 - val_loss: 1546273246200.7988\n",
      "Epoch 63/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1470540523045.0945 - val_loss: 1539633156009.5864\n",
      "Epoch 64/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 67us/step - loss: 1465456175151.5386 - val_loss: 1532153268526.4473\n",
      "Epoch 65/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1460876221002.3091 - val_loss: 1530383779750.7061\n",
      "Epoch 66/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1455257798240.6377 - val_loss: 1521158537884.2644\n",
      "Epoch 67/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1446100304309.9309 - val_loss: 1524055181337.9241\n",
      "Epoch 68/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1438540241390.1130 - val_loss: 1517949118847.0999\n",
      "Epoch 69/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1441593006827.6521 - val_loss: 1505577948409.1589\n",
      "Epoch 70/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1430929724969.8962 - val_loss: 1501071444546.9705\n",
      "Epoch 71/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1425156675601.7668 - val_loss: 1493461345788.3994\n",
      "Epoch 72/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1421248901478.2200 - val_loss: 1487158551599.5273\n",
      "Epoch 73/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1414832684210.8699 - val_loss: 1487098989238.1885\n",
      "Epoch 74/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1409265846555.7908 - val_loss: 1476719183529.2266\n",
      "Epoch 75/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1403359103133.0212 - val_loss: 1468967326439.1560\n",
      "Epoch 76/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1396804632771.9165 - val_loss: 1463646647187.9832\n",
      "Epoch 77/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1391895515096.6248 - val_loss: 1459375144729.5640\n",
      "Epoch 78/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1387685398927.0359 - val_loss: 1456345652706.4753\n",
      "Epoch 79/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1379366340434.6523 - val_loss: 1447688387478.8635\n",
      "Epoch 80/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1378795501314.9409 - val_loss: 1441903816583.0210\n",
      "Epoch 81/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1370455869773.7305 - val_loss: 1439821188687.9324\n",
      "Epoch 82/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1367132626277.4995 - val_loss: 1434341923746.3853\n",
      "Epoch 83/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1362145047520.0676 - val_loss: 1425522300314.4641\n",
      "Epoch 84/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1354086112490.5715 - val_loss: 1428709108786.4080\n",
      "Epoch 85/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1350000533098.7217 - val_loss: 1415796688046.2673\n",
      "Epoch 86/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1340213140741.4622 - val_loss: 1420412793171.8931\n",
      "Epoch 87/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1337462719255.3491 - val_loss: 1417085483811.6455\n",
      "Epoch 88/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1335986835059.3650 - val_loss: 1399681916202.1265\n",
      "Epoch 89/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1328204620542.3792 - val_loss: 1396149415192.8440\n",
      "Epoch 90/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1323568016475.4756 - val_loss: 1387013414554.8242\n",
      "Epoch 91/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1317488298640.4165 - val_loss: 1382349134477.8621\n",
      "Epoch 92/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1310545478999.5742 - val_loss: 1377639864628.2083\n",
      "Epoch 93/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1305053126450.4797 - val_loss: 1373390733818.9592\n",
      "Epoch 94/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1301718817834.4966 - val_loss: 1366904366012.3093\n",
      "Epoch 95/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1295852638137.8926 - val_loss: 1364487627662.2222\n",
      "Epoch 96/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1288872760155.5359 - val_loss: 1356396720122.2390\n",
      "Epoch 97/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1285884476379.7458 - val_loss: 1360401873953.1252\n",
      "Epoch 98/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1281284799275.0366 - val_loss: 1347741389635.3306\n",
      "Epoch 99/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1275647353986.3708 - val_loss: 1342701730779.9944\n",
      "Epoch 100/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1270331609524.9707 - val_loss: 1337284417360.2925\n",
      "Epoch 101/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1266809641930.4590 - val_loss: 1332105287389.0745\n",
      "Epoch 102/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1260638084833.8081 - val_loss: 1327390605058.5205\n",
      "Epoch 103/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1257610681305.8250 - val_loss: 1326313132014.7173\n",
      "Epoch 104/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1252598264558.0530 - val_loss: 1319052529462.3684\n",
      "Epoch 105/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1248386666660.2241 - val_loss: 1320633262575.4375\n",
      "Epoch 106/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1240378561045.0083 - val_loss: 1330845840506.4192\n",
      "Epoch 107/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1239383071079.6604 - val_loss: 1307790363640.7988\n",
      "Epoch 108/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1232785720383.1445 - val_loss: 1300556291185.7778\n",
      "Epoch 109/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1228124845823.0996 - val_loss: 1295873335284.4783\n",
      "Epoch 110/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1227284918806.9290 - val_loss: 1293229084239.9324\n",
      "Epoch 111/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1220134623714.5884 - val_loss: 1289022203872.3149\n",
      "Epoch 112/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1216952859888.3340 - val_loss: 1288430260782.8074\n",
      "Epoch 113/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1211189967939.4663 - val_loss: 1279382105944.9338\n",
      "Epoch 114/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1207220674458.2002 - val_loss: 1278435328645.2207\n",
      "Epoch 115/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1205553992685.5129 - val_loss: 1271297906581.4233\n",
      "Epoch 116/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1199891288915.8528 - val_loss: 1266505213275.0942\n",
      "Epoch 117/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1195119193940.0928 - val_loss: 1269854924609.8904\n",
      "Epoch 118/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1192979044884.5281 - val_loss: 1259836963343.1223\n",
      "Epoch 119/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1191854477241.8926 - val_loss: 1255820189085.3445\n",
      "Epoch 120/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1184345967259.9409 - val_loss: 1251010409594.4192\n",
      "Epoch 121/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1179461148305.1367 - val_loss: 1251003129291.4319\n",
      "Epoch 122/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1176591190445.7678 - val_loss: 1246919069379.1504\n",
      "Epoch 123/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1173512466713.3901 - val_loss: 1241184556649.8564\n",
      "Epoch 124/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1170436153184.5776 - val_loss: 1246486299776.1799\n",
      "Epoch 125/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1164352529204.1602 - val_loss: 1235898184803.3755\n",
      "Epoch 126/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1165731836216.1218 - val_loss: 1230702425095.2012\n",
      "Epoch 127/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 57us/step - loss: 1158961100466.2695 - val_loss: 1230638181288.1462\n",
      "Epoch 128/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1156652741534.7620 - val_loss: 1224178418424.4387\n",
      "Epoch 129/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1152762302367.0022 - val_loss: 1222640686052.6357\n",
      "Epoch 130/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1152696494189.0027 - val_loss: 1215646284648.7764\n",
      "Epoch 131/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1147488742237.9365 - val_loss: 1212690481635.9155\n",
      "Epoch 132/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1144626734778.9128 - val_loss: 1209485205469.4346\n",
      "Epoch 133/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1141868131434.3616 - val_loss: 1211530057897.9465\n",
      "Epoch 134/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1138603001237.0381 - val_loss: 1203112622261.4683\n",
      "Epoch 135/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1134983938101.7810 - val_loss: 1203263834091.8369\n",
      "Epoch 136/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1130737873845.8110 - val_loss: 1212709787933.1646\n",
      "Epoch 137/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1127026261862.3398 - val_loss: 1198903582063.2573\n",
      "Epoch 138/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1125974766060.6724 - val_loss: 1191193858445.5022\n",
      "Epoch 139/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1122205255548.9087 - val_loss: 1194178632886.9087\n",
      "Epoch 140/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1119897033251.8940 - val_loss: 1186820058991.9775\n",
      "Epoch 141/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1115119643182.4580 - val_loss: 1185790209235.7131\n",
      "Epoch 142/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1112218860999.4578 - val_loss: 1179723757219.4656\n",
      "Epoch 143/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1112608477451.9448 - val_loss: 1179845877610.2166\n",
      "Epoch 144/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1107098640389.7622 - val_loss: 1175822708292.4106\n",
      "Epoch 145/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1107376762396.9312 - val_loss: 1173379046075.9495\n",
      "Epoch 146/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1101410791232.8853 - val_loss: 1168335601064.8665\n",
      "Epoch 147/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1100110488675.6389 - val_loss: 1164760408651.6118\n",
      "Epoch 148/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1098609455558.2573 - val_loss: 1162387022042.9143\n",
      "Epoch 149/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1093561783761.5419 - val_loss: 1161057167927.4487\n",
      "Epoch 150/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1091521119521.7932 - val_loss: 1157007636361.9016\n",
      "Epoch 151/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1091482494800.7317 - val_loss: 1161747069178.5991\n",
      "Epoch 152/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1087263079541.4059 - val_loss: 1151219124457.3164\n",
      "Epoch 153/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1086682976934.9852 - val_loss: 1150092501625.6990\n",
      "Epoch 154/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1083107796905.0861 - val_loss: 1147033372767.0549\n",
      "Epoch 155/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1081536711335.4653 - val_loss: 1143979949964.7820\n",
      "Epoch 156/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1077217407595.4420 - val_loss: 1150524813490.5879\n",
      "Epoch 157/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1077686571567.8987 - val_loss: 1139244333997.9072\n",
      "Epoch 158/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1072732052257.4331 - val_loss: 1136995498879.8201\n",
      "Epoch 159/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1072665709588.4081 - val_loss: 1137453379278.6724\n",
      "Epoch 160/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1069609403839.2948 - val_loss: 1132232813503.1899\n",
      "Epoch 161/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1066722980109.8655 - val_loss: 1133410252451.4656\n",
      "Epoch 162/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1063977232377.7576 - val_loss: 1153154938900.1631\n",
      "Epoch 163/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1067723851464.3582 - val_loss: 1128465670846.8298\n",
      "Epoch 164/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1060488238149.8673 - val_loss: 1129509365659.1843\n",
      "Epoch 165/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1062026799700.8732 - val_loss: 1123438772247.0437\n",
      "Epoch 166/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1057437209408.6453 - val_loss: 1120356352668.2644\n",
      "Epoch 167/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1056551609923.8264 - val_loss: 1119744833210.5090\n",
      "Epoch 168/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1052715171720.9135 - val_loss: 1119165685771.5217\n",
      "Epoch 169/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1049642329756.1810 - val_loss: 1112705595135.6399\n",
      "Epoch 170/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1053739918431.7974 - val_loss: 1119284731592.9114\n",
      "Epoch 171/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1046385562382.7058 - val_loss: 1109694241836.6470\n",
      "Epoch 172/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1043852610242.3558 - val_loss: 1107640516515.8257\n",
      "Epoch 173/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1041576000544.8928 - val_loss: 1115930284152.9790\n",
      "Epoch 174/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1044398833487.2909 - val_loss: 1106747153913.5190\n",
      "Epoch 175/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1039099560135.7581 - val_loss: 1101693981369.0688\n",
      "Epoch 176/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1040328347842.9562 - val_loss: 1100761814357.3333\n",
      "Epoch 177/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1034620413569.2905 - val_loss: 1104902451378.5879\n",
      "Epoch 178/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1037925807625.2435 - val_loss: 1098763972901.8060\n",
      "Epoch 179/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1037461941835.0294 - val_loss: 1094467092526.0873\n",
      "Epoch 180/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1029303190698.7067 - val_loss: 1096341057668.5006\n",
      "Epoch 181/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1032257936653.1451 - val_loss: 1092648647252.2533\n",
      "Epoch 182/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1027940691173.7698 - val_loss: 1093152844735.1899\n",
      "Epoch 183/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1024902619522.0708 - val_loss: 1087173457709.7272\n",
      "Epoch 184/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1023981439836.0159 - val_loss: 1085683075236.1858\n",
      "Epoch 185/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1024295855564.7400 - val_loss: 1083909773205.4233\n",
      "Epoch 186/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1021318571372.7025 - val_loss: 1082358020830.5148\n",
      "Epoch 187/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1019423547240.2607 - val_loss: 1093497278170.1940\n",
      "Epoch 188/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1018150503909.7097 - val_loss: 1079598008046.3572\n",
      "Epoch 189/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1019299337942.7639 - val_loss: 1077540290652.1744\n",
      "Epoch 190/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 80us/step - loss: 1016322601640.9059 - val_loss: 1075962864203.6118\n",
      "Epoch 191/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1013723965956.2017 - val_loss: 1080249818133.6033\n",
      "Epoch 192/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1013891711725.0927 - val_loss: 1072425710230.5034\n",
      "Epoch 193/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1010395809169.1968 - val_loss: 1071401922969.0239\n",
      "Epoch 194/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1010382941859.3838 - val_loss: 1075283718414.7623\n",
      "Epoch 195/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1009484920674.9788 - val_loss: 1077689177383.2461\n",
      "Epoch 196/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1008079969556.8281 - val_loss: 1069310940201.7665\n",
      "Epoch 197/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1003328730333.6066 - val_loss: 1072061570830.0424\n",
      "Epoch 198/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1003135692224.7352 - val_loss: 1080839786386.5427\n",
      "Epoch 199/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1002917075591.7731 - val_loss: 1061847620145.6877\n",
      "Epoch 200/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1001802943277.9180 - val_loss: 1061086699531.5220\n",
      "Epoch 201/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 998462602706.9824 - val_loss: 1065003764129.6654\n",
      "Epoch 202/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 998307809901.8429 - val_loss: 1060915224725.7834\n",
      "Epoch 203/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 996018824887.0715 - val_loss: 1062099513430.4135\n",
      "Epoch 204/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 996502136645.2070 - val_loss: 1057071049432.7539\n",
      "Epoch 205/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 995625749900.8750 - val_loss: 1054319205328.4724\n",
      "Epoch 206/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 993070205933.9929 - val_loss: 1055837188136.3264\n",
      "Epoch 207/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 993743575756.9200 - val_loss: 1053633320853.4233\n",
      "Epoch 208/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 990913467373.5127 - val_loss: 1049875554822.4810\n",
      "Epoch 209/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 990409250622.4845 - val_loss: 1062024843039.3248\n",
      "Epoch 210/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 988221718839.1615 - val_loss: 1060181404768.4952\n",
      "Epoch 211/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 987639709849.6600 - val_loss: 1053557339834.5092\n",
      "Epoch 212/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 987690803443.9353 - val_loss: 1046846756319.5951\n",
      "Epoch 213/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 986129896060.7288 - val_loss: 1046595677672.2363\n",
      "Epoch 214/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 984385890044.2185 - val_loss: 1044414151713.1252\n",
      "Epoch 215/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 983775366256.6040 - val_loss: 1049462617740.4221\n",
      "Epoch 216/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 981669482341.3796 - val_loss: 1045989807767.9438\n",
      "Epoch 217/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 980075718887.9307 - val_loss: 1052295036282.7791\n",
      "Epoch 218/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 977330806559.2722 - val_loss: 1038792435933.7947\n",
      "Epoch 219/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 980427897366.4487 - val_loss: 1043580202518.3236\n",
      "Epoch 220/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 979377423911.0153 - val_loss: 1036270913170.1830\n",
      "Epoch 221/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 978752153524.6106 - val_loss: 1034665214159.3923\n",
      "Epoch 222/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 975902710684.3610 - val_loss: 1051353705660.6694\n",
      "Epoch 223/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 974654234234.0878 - val_loss: 1035073208020.4331\n",
      "Epoch 224/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 972793333244.7588 - val_loss: 1032666867311.6174\n",
      "Epoch 225/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 971004589880.7222 - val_loss: 1030511743327.4149\n",
      "Epoch 226/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 973843414313.9564 - val_loss: 1030206294226.2728\n",
      "Epoch 227/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 966636079660.5375 - val_loss: 1034927771930.2841\n",
      "Epoch 228/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 968284952213.4584 - val_loss: 1031757574391.7188\n",
      "Epoch 229/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 966108340371.8977 - val_loss: 1030811728975.2124\n",
      "Epoch 230/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 967391392117.5859 - val_loss: 1029888046801.5527\n",
      "Epoch 231/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 967685745056.3226 - val_loss: 1025052998527.8198\n",
      "Epoch 232/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 965592052803.9465 - val_loss: 1025168919083.9269\n",
      "Epoch 233/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 964192594078.9421 - val_loss: 1022680123210.5317\n",
      "Epoch 234/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 964040953086.7395 - val_loss: 1021720836461.8170\n",
      "Epoch 235/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 962164320461.0402 - val_loss: 1023592282662.1660\n",
      "Epoch 236/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 961199582875.7008 - val_loss: 1020730043119.7975\n",
      "Epoch 237/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 960492420363.2244 - val_loss: 1018804336706.2505\n",
      "Epoch 238/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 959685396644.7043 - val_loss: 1018599499283.4429\n",
      "Epoch 239/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 959302485285.6346 - val_loss: 1017016955035.5443\n",
      "Epoch 240/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 958207640239.6285 - val_loss: 1015539593025.8901\n",
      "Epoch 241/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 957396715615.5574 - val_loss: 1014658997407.8650\n",
      "Epoch 242/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 956931364778.0464 - val_loss: 1014289035350.4135\n",
      "Epoch 243/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 957536432159.6924 - val_loss: 1018315541012.8833\n",
      "Epoch 244/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 955614310942.6119 - val_loss: 1012056385412.1406\n",
      "Epoch 245/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 954636205949.3890 - val_loss: 1011138376204.2419\n",
      "Epoch 246/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 952428479735.5367 - val_loss: 1011076802873.9692\n",
      "Epoch 247/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 954235963099.5658 - val_loss: 1010022614374.6161\n",
      "Epoch 248/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 951368168596.6180 - val_loss: 1010432936929.7552\n",
      "Epoch 249/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 950097786964.9932 - val_loss: 1010654318182.9761\n",
      "Epoch 250/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 950073844028.9238 - val_loss: 1009070588387.9155\n",
      "Epoch 251/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 948308710127.9738 - val_loss: 1023589358613.6033\n",
      "Epoch 252/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 948038498607.9587 - val_loss: 1005777353968.5176\n",
      "Epoch 253/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 61us/step - loss: 947482067973.5221 - val_loss: 1009485275493.1758\n",
      "Epoch 254/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 946315261264.8516 - val_loss: 1003757958727.2913\n",
      "Epoch 255/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 944713632246.7565 - val_loss: 1009457313927.3811\n",
      "Epoch 256/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 948306268808.2533 - val_loss: 1003032092775.6962\n",
      "Epoch 257/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 944362944178.2697 - val_loss: 1006327975826.5427\n",
      "Epoch 258/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 946277788194.9337 - val_loss: 1001652331475.3530\n",
      "Epoch 259/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 945884023551.8199 - val_loss: 1000363860256.0452\n",
      "Epoch 260/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 940589635298.2883 - val_loss: 1014669364631.5837\n",
      "Epoch 261/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 941686009968.3639 - val_loss: 1002486065166.4021\n",
      "Epoch 262/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 941172994882.5660 - val_loss: 1002214075834.1490\n",
      "Epoch 263/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 942679039944.2982 - val_loss: 999406793159.1111\n",
      "Epoch 264/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 938583928544.8478 - val_loss: 999550726437.8060\n",
      "Epoch 265/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 937560645614.7133 - val_loss: 998046492640.3151\n",
      "Epoch 266/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 938899416313.4574 - val_loss: 995815465893.2659\n",
      "Epoch 267/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 938025471687.8781 - val_loss: 999369432938.2166\n",
      "Epoch 268/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 938322878281.5288 - val_loss: 997121546663.4263\n",
      "Epoch 269/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 936879832169.8813 - val_loss: 993321262055.5162\n",
      "Epoch 270/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 937956791696.2363 - val_loss: 993136004319.2349\n",
      "Epoch 271/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 937100838124.0122 - val_loss: 998766336653.8622\n",
      "Epoch 272/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 936216895624.3733 - val_loss: 995578519609.6089\n",
      "Epoch 273/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 935599173079.7843 - val_loss: 991455603485.8845\n",
      "Epoch 274/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 933825534726.5426 - val_loss: 991856511716.2756\n",
      "Epoch 275/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 933373153378.9187 - val_loss: 989927421119.5499\n",
      "Epoch 276/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 935509383043.6313 - val_loss: 994620906194.9928\n",
      "Epoch 277/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 932853389806.1130 - val_loss: 996874075353.4740\n",
      "Epoch 278/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 931968757081.0148 - val_loss: 987952645755.1393\n",
      "Epoch 279/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 932323279320.5045 - val_loss: 994244954276.1858\n",
      "Epoch 280/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 931530196131.0237 - val_loss: 991772278138.7791\n",
      "Epoch 281/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 929429508874.8643 - val_loss: 986477824532.8833\n",
      "Epoch 282/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 928795939963.6483 - val_loss: 985651636514.9253\n",
      "Epoch 283/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 927170273308.8113 - val_loss: 985856003571.7581\n",
      "Epoch 284/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 929277729189.1245 - val_loss: 984905934631.9662\n",
      "Epoch 285/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 929901285897.4838 - val_loss: 986178904144.6526\n",
      "Epoch 286/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 928709848931.4589 - val_loss: 985444157373.7495\n",
      "Epoch 287/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 928473974357.1133 - val_loss: 982843999705.8340\n",
      "Epoch 288/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 925719019270.3024 - val_loss: 988933305535.5499\n",
      "Epoch 289/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 925999684919.4016 - val_loss: 982422364586.3066\n",
      "Epoch 290/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 923789158827.8470 - val_loss: 983330391951.6626\n",
      "Epoch 291/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 924787751650.0483 - val_loss: 980520695229.0295\n",
      "Epoch 292/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 924978325359.4635 - val_loss: 980560165782.8635\n",
      "Epoch 293/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 924720745215.8199 - val_loss: 979885867998.8748\n",
      "Epoch 294/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 922593743818.9393 - val_loss: 980003542080.8101\n",
      "Epoch 295/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 923395137987.3763 - val_loss: 983454245313.3502\n",
      "Epoch 296/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 923986974902.9514 - val_loss: 983337133495.2687\n",
      "Epoch 297/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 922827924099.4513 - val_loss: 978207816875.3867\n",
      "Epoch 298/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 921688900190.9570 - val_loss: 978405869709.1421\n",
      "Epoch 299/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 921196714750.8595 - val_loss: 977593198606.4021\n",
      "Epoch 300/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 919009667104.1726 - val_loss: 988905402961.3726\n",
      "Epoch 301/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 919789282453.5784 - val_loss: 980613132143.9774\n",
      "Epoch 302/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 918201830923.6447 - val_loss: 976526808704.9001\n",
      "Epoch 303/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 916772661541.3947 - val_loss: 977652876462.2671\n",
      "Epoch 304/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 916699482560.9755 - val_loss: 977858361594.5991\n",
      "Epoch 305/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 917329255869.8541 - val_loss: 978594355485.1646\n",
      "Epoch 306/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 916784009088.2701 - val_loss: 973688935742.2897\n",
      "Epoch 307/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 919020349656.5646 - val_loss: 973286021612.5570\n",
      "Epoch 308/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 915812506420.6406 - val_loss: 983531963174.5260\n",
      "Epoch 309/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 915245164679.8931 - val_loss: 974182298216.4163\n",
      "Epoch 310/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 915119314631.3978 - val_loss: 980602676617.1814\n",
      "Epoch 311/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 918822106744.6472 - val_loss: 974623943476.9282\n",
      "Epoch 312/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 916433069908.5730 - val_loss: 975382148817.5527\n",
      "Epoch 313/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 913597124687.7112 - val_loss: 971924199847.4261\n",
      "Epoch 314/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 912995916031.9399 - val_loss: 979218888217.2040\n",
      "Epoch 315/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 913435750062.9083 - val_loss: 972743096224.9452\n",
      "Epoch 316/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 912392863509.6685 - val_loss: 970127738352.8777\n",
      "Epoch 317/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 61us/step - loss: 913698750456.3170 - val_loss: 971738556914.3179\n",
      "Epoch 318/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 911342306540.0122 - val_loss: 968436960064.4501\n",
      "Epoch 319/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 913269892153.3824 - val_loss: 967868582282.6217\n",
      "Epoch 320/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 916360457088.5101 - val_loss: 971592038604.5120\n",
      "Epoch 321/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 908999080612.5844 - val_loss: 967733918338.3403\n",
      "Epoch 322/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 910435844165.1471 - val_loss: 970372104583.7412\n",
      "Epoch 323/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 908244328653.5203 - val_loss: 969440038410.8016\n",
      "Epoch 324/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 910110344216.7296 - val_loss: 965704261320.9114\n",
      "Epoch 325/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 909293062324.0703 - val_loss: 968111067418.2841\n",
      "Epoch 326/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 907897310770.0596 - val_loss: 985552585902.2671\n",
      "Epoch 327/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 908683159476.6106 - val_loss: 965482913895.6962\n",
      "Epoch 328/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 908510715652.3817 - val_loss: 964546990224.0225\n",
      "Epoch 329/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 907674246901.0156 - val_loss: 963801989511.7412\n",
      "Epoch 330/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 906234016832.1051 - val_loss: 977857125351.5162\n",
      "Epoch 331/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 908021330650.8455 - val_loss: 975368485713.7328\n",
      "Epoch 332/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 906727639761.7219 - val_loss: 963739293958.1210\n",
      "Epoch 333/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 905883620733.0289 - val_loss: 968117363390.8298\n",
      "Epoch 334/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 903554220811.8246 - val_loss: 964167323400.2812\n",
      "Epoch 335/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 906857075635.8903 - val_loss: 962999246882.5654\n",
      "Epoch 336/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 905410435124.3405 - val_loss: 961388744940.1969\n",
      "Epoch 337/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 903417598296.7747 - val_loss: 963715272202.8016\n",
      "Epoch 338/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 905055690761.6038 - val_loss: 962733712339.3530\n",
      "Epoch 339/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 905395436792.0168 - val_loss: 960862100040.7313\n",
      "Epoch 340/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 903207967586.7385 - val_loss: 960225519100.3994\n",
      "Epoch 341/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 904226135927.6267 - val_loss: 970764066274.4753\n",
      "Epoch 342/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 903555003894.0361 - val_loss: 964649024706.4304\n",
      "Epoch 343/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 903636250217.7614 - val_loss: 958950779470.4923\n",
      "Epoch 344/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 902826714088.7109 - val_loss: 962337562796.8270\n",
      "Epoch 345/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 902640434523.8959 - val_loss: 959167131411.8031\n",
      "Epoch 346/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 900785004722.6298 - val_loss: 957797014319.1674\n",
      "Epoch 347/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 902570964243.3876 - val_loss: 957561704684.1969\n",
      "Epoch 348/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 899842008340.3479 - val_loss: 965310114451.6230\n",
      "Epoch 349/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 900297371629.7529 - val_loss: 957349107559.3362\n",
      "Epoch 350/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 900743289652.6406 - val_loss: 964025071013.9860\n",
      "Epoch 351/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 898889669643.7646 - val_loss: 957900952848.2025\n",
      "Epoch 352/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 900238447282.5098 - val_loss: 956312738491.9493\n",
      "Epoch 353/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 899168626616.4520 - val_loss: 955548010429.7496\n",
      "Epoch 354/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 898261868307.7477 - val_loss: 959898335736.0787\n",
      "Epoch 355/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 901163329360.7316 - val_loss: 958824088612.0056\n",
      "Epoch 356/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 898952155968.6453 - val_loss: 956308190883.4656\n",
      "Epoch 357/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 897634347524.2017 - val_loss: 954961872391.9213\n",
      "Epoch 358/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 895489983797.0007 - val_loss: 963472965608.9564\n",
      "Epoch 359/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 897998462651.3932 - val_loss: 953650369554.7229\n",
      "Epoch 360/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 898513284102.9626 - val_loss: 953403530008.1238\n",
      "Epoch 361/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 895675314557.2689 - val_loss: 952617440251.6793\n",
      "Epoch 362/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 898319633474.5060 - val_loss: 953071012603.3192\n",
      "Epoch 363/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 896855400526.7509 - val_loss: 953502691875.2855\n",
      "Epoch 364/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 896432787113.1461 - val_loss: 960747847011.7356\n",
      "Epoch 365/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 898918616472.3995 - val_loss: 951484424387.8706\n",
      "Epoch 366/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 896294417906.1947 - val_loss: 950999718564.9058\n",
      "Epoch 367/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 894007805309.9893 - val_loss: 950453578226.3179\n",
      "Epoch 368/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 895259119761.0166 - val_loss: 953582533250.3403\n",
      "Epoch 369/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 894231741314.9111 - val_loss: 950902432815.5275\n",
      "Epoch 370/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 895095059351.0790 - val_loss: 952138311901.7947\n",
      "Epoch 371/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 892786709623.8069 - val_loss: 960236244882.5428\n",
      "Epoch 372/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 896560861682.4347 - val_loss: 949183855188.2532\n",
      "Epoch 373/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 892009627206.2274 - val_loss: 956663070996.5232\n",
      "Epoch 374/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 896488924344.1520 - val_loss: 950699904361.4965\n",
      "Epoch 375/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 891022456609.6731 - val_loss: 948052723830.0985\n",
      "Epoch 376/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 891011422898.0295 - val_loss: 951576376004.5907\n",
      "Epoch 377/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 891977274718.2968 - val_loss: 948023581049.3390\n",
      "Epoch 378/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 891511053289.1910 - val_loss: 952474996419.1505\n",
      "Epoch 379/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 891022230130.1646 - val_loss: 947775602319.3024\n",
      "Epoch 380/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 889359675827.0499 - val_loss: 947799941615.4374\n",
      "Epoch 381/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 87us/step - loss: 893124619739.6257 - val_loss: 951696148068.0956\n",
      "Epoch 382/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 891290251948.7474 - val_loss: 946936937574.2560\n",
      "Epoch 383/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 892461019398.1824 - val_loss: 946058155948.4669\n",
      "Epoch 384/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 890052543724.0122 - val_loss: 950914987522.1603\n",
      "Epoch 385/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 888334235256.6472 - val_loss: 952073474860.2869\n",
      "Epoch 386/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 890799066620.2786 - val_loss: 945636267441.5077\n",
      "Epoch 387/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 890051292147.7552 - val_loss: 961135300636.8044\n",
      "Epoch 388/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 889584914507.8696 - val_loss: 944412908588.6470\n",
      "Epoch 389/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 888123981908.5129 - val_loss: 946415878904.4388\n",
      "Epoch 390/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 889106068570.9956 - val_loss: 946420810972.3544\n",
      "Epoch 391/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 888420742292.6180 - val_loss: 943964909285.7159\n",
      "Epoch 392/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 888475391564.4698 - val_loss: 943856951123.1730\n",
      "Epoch 393/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 887321066521.2098 - val_loss: 943480677792.2250\n",
      "Epoch 394/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 888930639224.7072 - val_loss: 942872135198.9648\n",
      "Epoch 395/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 886448041243.5508 - val_loss: 944549553729.5303\n",
      "Epoch 396/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 885091557785.1198 - val_loss: 944684595035.8143\n",
      "Epoch 397/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 888486614201.1123 - val_loss: 942367322912.7651\n",
      "Epoch 398/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 886389048594.4272 - val_loss: 945424455826.9030\n",
      "Epoch 399/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 885804952025.9452 - val_loss: 946849835805.8846\n",
      "Epoch 400/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 886423663787.6671 - val_loss: 941338573165.8171\n",
      "Epoch 401/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 886056110302.3269 - val_loss: 941446581379.0604\n",
      "Epoch 402/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 884123505522.5848 - val_loss: 942853192068.8607\n",
      "Epoch 403/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 883355497574.5200 - val_loss: 941662188643.3755\n",
      "Epoch 404/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 886729615415.2216 - val_loss: 939923937588.2081\n",
      "Epoch 405/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 882790576086.7039 - val_loss: 955079651921.3727\n",
      "Epoch 406/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 886571504706.0258 - val_loss: 944331677868.8270\n",
      "Epoch 407/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 883990171769.9677 - val_loss: 942578067332.1406\n",
      "Epoch 408/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 886329570103.0415 - val_loss: 942085138169.8790\n",
      "Epoch 409/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 884495188866.9111 - val_loss: 940066428429.6821\n",
      "Epoch 410/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 883244745237.9685 - val_loss: 941129213759.0099\n",
      "Epoch 411/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 884181726322.0446 - val_loss: 942410668939.3418\n",
      "Epoch 412/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 882131165762.1459 - val_loss: 949231914645.0632\n",
      "Epoch 413/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 882490214964.2205 - val_loss: 942151752886.9086\n",
      "Epoch 414/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 880337098677.5709 - val_loss: 939591388751.9325\n",
      "Epoch 415/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 881261235427.8490 - val_loss: 937829095110.0309\n",
      "Epoch 416/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 880750714321.0616 - val_loss: 945793533194.4417\n",
      "Epoch 417/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 879746058996.2954 - val_loss: 937026155043.2855\n",
      "Epoch 418/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 878660443032.5195 - val_loss: 937987856392.6414\n",
      "Epoch 419/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 883531012202.1215 - val_loss: 947048334948.0956\n",
      "Epoch 420/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 880835391289.2024 - val_loss: 936601753769.9465\n",
      "Epoch 421/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 880359056741.0194 - val_loss: 941148604580.1857\n",
      "Epoch 422/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 880606898619.2131 - val_loss: 935852680423.8762\n",
      "Epoch 423/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 879950306696.0731 - val_loss: 936333705354.2616\n",
      "Epoch 424/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 877572572230.8276 - val_loss: 936181322908.9845\n",
      "Epoch 425/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 878624149810.3596 - val_loss: 939131350365.9747\n",
      "Epoch 426/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 879451133136.1614 - val_loss: 937766506110.0197\n",
      "Epoch 427/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 877968233038.6306 - val_loss: 935069165098.4867\n",
      "Epoch 428/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 877507246345.5437 - val_loss: 946135989580.6920\n",
      "Epoch 429/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 877966504716.7850 - val_loss: 935079841751.6737\n",
      "Epoch 430/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 878360415375.3359 - val_loss: 935843376800.5851\n",
      "Epoch 431/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 877584594898.3822 - val_loss: 936582053703.6512\n",
      "Epoch 432/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 878431394128.6115 - val_loss: 935791355134.9198\n",
      "Epoch 433/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 877803043396.5468 - val_loss: 934124388167.6512\n",
      "Epoch 434/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 876095199714.5885 - val_loss: 933284058015.5049\n",
      "Epoch 435/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 876680843424.3827 - val_loss: 933767775728.8777\n",
      "Epoch 436/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 876161563673.2098 - val_loss: 941034426488.9789\n",
      "Epoch 437/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 878774796284.1586 - val_loss: 932287944090.4641\n",
      "Epoch 438/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 878275701352.0806 - val_loss: 956982555954.7679\n",
      "Epoch 439/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 877789148258.6785 - val_loss: 933120135427.2405\n",
      "Epoch 440/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 875134905931.0294 - val_loss: 943515587003.5894\n",
      "Epoch 441/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 876439396676.6068 - val_loss: 931960658228.2081\n",
      "Epoch 442/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 878042462707.8752 - val_loss: 931254074700.6920\n",
      "Epoch 443/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 874271548988.1436 - val_loss: 932757114931.8481\n",
      "Epoch 444/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 874015520196.0967 - val_loss: 932004345760.9452\n",
      "Epoch 445/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 59us/step - loss: 875735493569.8158 - val_loss: 931496589424.3375\n",
      "Epoch 446/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 874926456520.8385 - val_loss: 931563144688.8777\n",
      "Epoch 447/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 878585692855.3116 - val_loss: 934319830898.8579\n",
      "Epoch 448/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 874606308197.6196 - val_loss: 937386040779.4318\n",
      "Epoch 449/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 874866159628.4849 - val_loss: 937020288695.6287\n",
      "Epoch 450/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 875278497289.2435 - val_loss: 930099757701.2208\n",
      "Epoch 451/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 872405901210.2003 - val_loss: 937948053918.7848\n",
      "Epoch 452/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 873056468308.2129 - val_loss: 944918159993.6990\n",
      "Epoch 453/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 874366506418.8099 - val_loss: 929840528808.8663\n",
      "Epoch 454/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 873201086637.8279 - val_loss: 936941366391.5387\n",
      "Epoch 455/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 875293195039.5123 - val_loss: 931016044630.4135\n",
      "Epoch 456/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 872558605145.6150 - val_loss: 933942522541.5471\n",
      "Epoch 457/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 871557533526.0135 - val_loss: 932868791686.3010\n",
      "Epoch 458/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 874355183388.3911 - val_loss: 927686233103.8425\n",
      "Epoch 459/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 871368456280.8347 - val_loss: 929000293965.0520\n",
      "Epoch 460/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 873706039919.7637 - val_loss: 930890919354.1490\n",
      "Epoch 461/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 872039115563.9972 - val_loss: 935994833077.4684\n",
      "Epoch 462/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 870667837457.2867 - val_loss: 927457781244.3994\n",
      "Epoch 463/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 871498273769.6713 - val_loss: 928114232404.9733\n",
      "Epoch 464/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 871545345320.5157 - val_loss: 927873744872.9564\n",
      "Epoch 465/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 874381827560.1107 - val_loss: 927392837741.4572\n",
      "Epoch 466/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 870647670793.1235 - val_loss: 927923752620.1069\n",
      "Epoch 467/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 870333678981.6722 - val_loss: 927242066781.2545\n",
      "Epoch 468/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 870813775665.0392 - val_loss: 926650934734.3123\n",
      "Epoch 469/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 869130277354.7517 - val_loss: 930053750323.1279\n",
      "Epoch 470/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 870379851344.7915 - val_loss: 931432371610.4641\n",
      "Epoch 471/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 870137837038.3530 - val_loss: 925316396315.7244\n",
      "Epoch 472/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 871135991007.5272 - val_loss: 927062260515.6456\n",
      "Epoch 473/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 871043394739.8302 - val_loss: 926960880932.3657\n",
      "Epoch 474/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 869487719242.2489 - val_loss: 928117935972.4557\n",
      "Epoch 475/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 869235351308.0647 - val_loss: 924660633421.4121\n",
      "Epoch 476/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 868395016247.7018 - val_loss: 925214968986.1041\n",
      "Epoch 477/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 869481259497.7913 - val_loss: 928917863595.3867\n",
      "Epoch 478/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 868333814735.7411 - val_loss: 925304473925.4908\n",
      "Epoch 479/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 868952363187.5902 - val_loss: 924654727623.1111\n",
      "Epoch 480/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 869574798331.4382 - val_loss: 923383541408.5851\n",
      "Epoch 481/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 867839206952.9360 - val_loss: 928039013207.4937\n",
      "Epoch 482/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 867810390397.2689 - val_loss: 924760526876.8044\n",
      "Epoch 483/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 867089112175.6437 - val_loss: 922492470354.0928\n",
      "Epoch 484/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 868445327120.8666 - val_loss: 928225026497.3502\n",
      "Epoch 485/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 868038630342.6176 - val_loss: 923086381274.9142\n",
      "Epoch 486/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 866259970349.0775 - val_loss: 924276000240.8777\n",
      "Epoch 487/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 866277960810.8417 - val_loss: 921855153769.8566\n",
      "Epoch 488/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 865341631483.4382 - val_loss: 924410247372.5120\n",
      "Epoch 489/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 866983222425.9001 - val_loss: 923640191076.8158\n",
      "Epoch 490/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 866601511866.1327 - val_loss: 921849509378.1603\n",
      "Epoch 491/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 869192447155.3500 - val_loss: 935761822548.6133\n",
      "Epoch 492/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 866785102180.7794 - val_loss: 923324455312.3826\n",
      "Epoch 493/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 866064251840.8553 - val_loss: 925620936577.2603\n",
      "Epoch 494/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 865060863750.3024 - val_loss: 924468449331.8481\n",
      "Epoch 495/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 865306646230.7639 - val_loss: 928963309170.4979\n",
      "Epoch 496/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 865979278262.7714 - val_loss: 920632768370.8579\n",
      "Epoch 497/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 869340557578.0239 - val_loss: 921227288287.9550\n",
      "Epoch 498/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 865412458658.7836 - val_loss: 921493306746.7792\n",
      "Epoch 499/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 865446494254.0980 - val_loss: 919789297375.9550\n",
      "Epoch 500/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 862817603871.1522 - val_loss: 922445926060.1069\n",
      "Epoch 501/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 866373745577.8064 - val_loss: 921784253588.3431\n",
      "Epoch 502/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 862668024089.8701 - val_loss: 925934549288.6864\n",
      "Epoch 503/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 864205398025.6038 - val_loss: 924473890571.1617\n",
      "Epoch 504/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 864687810572.9650 - val_loss: 921210180811.0718\n",
      "Epoch 505/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 862554618136.9097 - val_loss: 920453895186.7229\n",
      "Epoch 506/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 862277140572.1960 - val_loss: 924235557247.0999\n",
      "Epoch 507/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 863293771283.3275 - val_loss: 923197196843.9269\n",
      "Epoch 508/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 864062676129.5830 - val_loss: 918526064429.7272\n",
      "Epoch 509/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 60us/step - loss: 863630126943.6173 - val_loss: 918821550952.7764\n",
      "Epoch 510/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 863686954553.7426 - val_loss: 918499286383.2573\n",
      "Epoch 511/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 861549681137.9545 - val_loss: 919063599550.4697\n",
      "Epoch 512/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 861162863241.6937 - val_loss: 925875985913.5190\n",
      "Epoch 513/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 862374246008.6472 - val_loss: 918014651534.5823\n",
      "Epoch 514/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 863250114534.5500 - val_loss: 918474845234.4078\n",
      "Epoch 515/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 862735627055.3584 - val_loss: 920988065388.7369\n",
      "Epoch 516/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 859095792688.2589 - val_loss: 949052577178.4641\n",
      "Epoch 517/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 863971380690.0220 - val_loss: 918003141993.4965\n",
      "Epoch 518/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 860780422335.8350 - val_loss: 920072180063.4149\n",
      "Epoch 519/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 863082472186.5380 - val_loss: 916715977715.0380\n",
      "Epoch 520/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 858932899088.0262 - val_loss: 940764022832.9677\n",
      "Epoch 521/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 865311684620.0046 - val_loss: 918689372491.2517\n",
      "Epoch 522/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 860495423051.5095 - val_loss: 919555125449.6315\n",
      "Epoch 523/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 860196695150.6832 - val_loss: 915967443957.9185\n",
      "Epoch 524/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 859772984350.9720 - val_loss: 920885439146.6666\n",
      "Epoch 525/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 861325805152.6377 - val_loss: 915879466624.9001\n",
      "Epoch 526/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 859003730312.3132 - val_loss: 917451252829.6146\n",
      "Epoch 527/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 861387432733.1113 - val_loss: 915577136429.0071\n",
      "Epoch 528/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 860571338524.8713 - val_loss: 920963526995.8931\n",
      "Epoch 529/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 860070971287.7992 - val_loss: 915704657041.4628\n",
      "Epoch 530/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 860505934872.2494 - val_loss: 924737670113.7552\n",
      "Epoch 531/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 861609149841.1968 - val_loss: 915562938529.3052\n",
      "Epoch 532/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 859312330915.7439 - val_loss: 922445170137.8340\n",
      "Epoch 533/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 861750814579.3049 - val_loss: 921842012280.9789\n",
      "Epoch 534/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 858265524587.5020 - val_loss: 917076796809.1814\n",
      "Epoch 535/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 859631300174.8708 - val_loss: 924247907787.4318\n",
      "Epoch 536/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 859417846641.6244 - val_loss: 913888818551.8987\n",
      "Epoch 537/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 858007609756.9613 - val_loss: 917246621693.1195\n",
      "Epoch 538/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 857619787392.8104 - val_loss: 920934544506.4191\n",
      "Epoch 539/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 859987666442.2040 - val_loss: 915356487317.0632\n",
      "Epoch 540/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 858311265288.8834 - val_loss: 913892662040.1238\n",
      "Epoch 541/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 858574001740.4698 - val_loss: 925678484006.1660\n",
      "Epoch 542/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 857137565914.0051 - val_loss: 912714218851.7356\n",
      "Epoch 543/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 856008825986.8511 - val_loss: 912649655844.7257\n",
      "Epoch 544/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 857848717847.8893 - val_loss: 914354218261.9634\n",
      "Epoch 545/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 858852253997.5577 - val_loss: 914102804837.1758\n",
      "Epoch 546/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 857464823072.3525 - val_loss: 919368084323.0155\n",
      "Epoch 547/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 857208031865.3674 - val_loss: 918425249783.3586\n",
      "Epoch 548/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 857837741074.9674 - val_loss: 920493604564.4332\n",
      "Epoch 549/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 856423042441.0336 - val_loss: 919006396116.4332\n",
      "Epoch 550/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 857185285141.8485 - val_loss: 915558688275.4430\n",
      "Epoch 551/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 857265328891.4983 - val_loss: 911735599167.3699\n",
      "Epoch 552/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 856313013889.2905 - val_loss: 917232881162.8016\n",
      "Epoch 553/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 857530781944.7372 - val_loss: 927331667028.9733\n",
      "Epoch 554/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 857502564601.6976 - val_loss: 911460978532.4557\n",
      "Epoch 555/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 855961016314.7179 - val_loss: 910893869992.1462\n",
      "Epoch 556/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 855189365041.3993 - val_loss: 912061172521.4065\n",
      "Epoch 557/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 856176995388.7437 - val_loss: 914099633484.6920\n",
      "Epoch 558/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 853993477404.9913 - val_loss: 913655129435.0942\n",
      "Epoch 559/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 856428191204.2692 - val_loss: 916512858976.1350\n",
      "Epoch 560/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 856854108346.7930 - val_loss: 913198328582.8411\n",
      "Epoch 561/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 854849489613.8805 - val_loss: 928654490726.2560\n",
      "Epoch 562/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 855494630484.7532 - val_loss: 909622399419.5894\n",
      "Epoch 563/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 855798832646.8427 - val_loss: 911848408929.5752\n",
      "Epoch 564/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 854657005725.9817 - val_loss: 909952048541.3446\n",
      "Epoch 565/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 853908632617.0560 - val_loss: 915830445696.9001\n",
      "Epoch 566/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 855769725934.9534 - val_loss: 918368618568.0112\n",
      "Epoch 567/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 853649087922.3297 - val_loss: 918397823289.9691\n",
      "Epoch 568/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 854441516704.5027 - val_loss: 909350097247.4149\n",
      "Epoch 569/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 852591718881.1479 - val_loss: 912096057214.3798\n",
      "Epoch 570/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 854034233682.5323 - val_loss: 911405364645.9860\n",
      "Epoch 571/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 853597889070.9384 - val_loss: 908311609920.0900\n",
      "Epoch 572/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 855518786070.9290 - val_loss: 927383200815.5275\n",
      "Epoch 573/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 61us/step - loss: 854182342656.9603 - val_loss: 910118768899.2405\n",
      "Epoch 574/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 853022290923.5920 - val_loss: 915197854986.4417\n",
      "Epoch 575/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 852853726277.3871 - val_loss: 907860417050.6442\n",
      "Epoch 576/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 852930184324.2916 - val_loss: 907997621229.2771\n",
      "Epoch 577/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 851661823298.9261 - val_loss: 912212680784.6526\n",
      "Epoch 578/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 852808326194.1796 - val_loss: 911309639685.7609\n",
      "Epoch 579/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 852984455169.9208 - val_loss: 907393879539.7581\n",
      "Epoch 580/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 852608705658.6879 - val_loss: 912711905642.9368\n",
      "Epoch 581/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 852927314585.0598 - val_loss: 907780067149.4121\n",
      "Epoch 582/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 852160285021.0964 - val_loss: 911690358864.6526\n",
      "Epoch 583/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 852338030538.6992 - val_loss: 918064753023.0999\n",
      "Epoch 584/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 853107365425.5793 - val_loss: 906615039392.2250\n",
      "Epoch 585/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 853678113109.8936 - val_loss: 906357182147.1505\n",
      "Epoch 586/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 851170865272.0469 - val_loss: 911131419557.2659\n",
      "Epoch 587/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 855418977247.5874 - val_loss: 924488033995.7919\n",
      "Epoch 588/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 850979112652.1998 - val_loss: 908540632062.5598\n",
      "Epoch 589/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 850803730971.2507 - val_loss: 908887927420.5795\n",
      "Epoch 590/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 852543711312.1913 - val_loss: 909538632613.2659\n",
      "Epoch 591/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 851375404603.1831 - val_loss: 905866304510.5598\n",
      "Epoch 592/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 853715858703.5461 - val_loss: 906214435060.8383\n",
      "Epoch 593/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 851488853673.1461 - val_loss: 911249425743.5724\n",
      "Epoch 594/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 850257542007.8668 - val_loss: 905746640844.1519\n",
      "Epoch 595/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 853177348408.1219 - val_loss: 905837154263.6737\n",
      "Epoch 596/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 851379124097.7107 - val_loss: 906954263775.2349\n",
      "Epoch 597/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 851651417265.4293 - val_loss: 907875701502.1997\n",
      "Epoch 598/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 850734518017.5006 - val_loss: 913911237210.0140\n",
      "Epoch 599/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 852943320673.5981 - val_loss: 906427837605.6259\n",
      "Epoch 600/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 850870483897.1724 - val_loss: 905685103093.1984\n",
      "Epoch 601/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 850424118339.4663 - val_loss: 905588074418.2279\n",
      "Epoch 602/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 851508755150.8408 - val_loss: 912365154574.7623\n",
      "Epoch 603/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 850246031204.8994 - val_loss: 911553552142.0422\n",
      "Epoch 604/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 851112562372.0366 - val_loss: 907708571705.6090\n",
      "Epoch 605/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 851353093229.2427 - val_loss: 905335925441.7103\n",
      "Epoch 606/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 850073056385.4105 - val_loss: 905061578196.0731\n",
      "Epoch 607/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 849652331791.5461 - val_loss: 913287611842.7904\n",
      "Epoch 608/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 852149235851.2544 - val_loss: 910778202915.6456\n",
      "Epoch 609/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 851111931244.2223 - val_loss: 904988943107.9606\n",
      "Epoch 610/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 849523935081.7013 - val_loss: 906118815087.2573\n",
      "Epoch 611/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 848167114247.3229 - val_loss: 906422889868.0619\n",
      "Epoch 612/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 848957111313.5267 - val_loss: 903610950450.0479\n",
      "Epoch 613/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 850692645253.9122 - val_loss: 909382205831.7412\n",
      "Epoch 614/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 849400959151.7485 - val_loss: 906628902975.3699\n",
      "Epoch 615/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 849001044926.9346 - val_loss: 903373783224.3488\n",
      "Epoch 616/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 849512946453.9086 - val_loss: 904852158415.0323\n",
      "Epoch 617/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 848675823187.6727 - val_loss: 903161448888.7089\n",
      "Epoch 618/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 848672680463.7261 - val_loss: 904482999565.3220\n",
      "Epoch 619/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 851342854523.8284 - val_loss: 903050155303.2461\n",
      "Epoch 620/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 848702693508.5319 - val_loss: 902906115194.4191\n",
      "Epoch 621/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 848351049905.1892 - val_loss: 908964315020.7820\n",
      "Epoch 622/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 848769682878.8146 - val_loss: 902776313726.3798\n",
      "Epoch 623/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 846435979862.3137 - val_loss: 913231503717.1758\n",
      "Epoch 624/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 846045182970.4779 - val_loss: 908223257898.1266\n",
      "Epoch 625/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 847546223263.7823 - val_loss: 902073410630.5710\n",
      "Epoch 626/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 848161396926.6344 - val_loss: 901553332972.9170\n",
      "Epoch 627/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 847776265933.1602 - val_loss: 902653586459.3643\n",
      "Epoch 628/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 846652739198.8895 - val_loss: 906534069065.0914\n",
      "Epoch 629/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 847070966487.0040 - val_loss: 909823454650.1490\n",
      "Epoch 630/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 847802605381.6871 - val_loss: 901284975127.7637\n",
      "Epoch 631/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 847169941981.3065 - val_loss: 904227157252.6808\n",
      "Epoch 632/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 847652334944.6979 - val_loss: 901491644516.8158\n",
      "Epoch 633/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 848037156789.5709 - val_loss: 912860178861.1870\n",
      "Epoch 634/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 849558425187.9990 - val_loss: 900782407011.7356\n",
      "Epoch 635/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 846123010028.3124 - val_loss: 926459050119.3811\n",
      "Epoch 636/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 847512067229.7416 - val_loss: 900950383338.0366\n",
      "Epoch 637/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 76us/step - loss: 846708064897.5306 - val_loss: 901407743504.5626\n",
      "Epoch 638/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 846939701874.1646 - val_loss: 901188149328.6526\n",
      "Epoch 639/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 846028341155.8040 - val_loss: 901219520056.8889\n",
      "Epoch 640/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 847012384886.1261 - val_loss: 913016954614.9985\n",
      "Epoch 641/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 845787885376.8854 - val_loss: 905554844532.2982\n",
      "Epoch 642/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 845850604604.2635 - val_loss: 900922538649.3839\n",
      "Epoch 643/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 845685552165.2146 - val_loss: 900720334096.2025\n",
      "Epoch 644/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 845916612386.6334 - val_loss: 899983158068.9282\n",
      "Epoch 645/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 845055957109.6460 - val_loss: 900608808876.4669\n",
      "Epoch 646/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 847616901849.8851 - val_loss: 903325313591.4486\n",
      "Epoch 647/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 846673533533.5165 - val_loss: 901353766266.7792\n",
      "Epoch 648/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 846168287728.9941 - val_loss: 912867850167.9888\n",
      "Epoch 649/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 846429305278.0942 - val_loss: 900309425772.7369\n",
      "Epoch 650/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 845803066556.9537 - val_loss: 898875295862.0985\n",
      "Epoch 651/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 845287891794.8923 - val_loss: 904127581983.3250\n",
      "Epoch 652/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 845264408056.4370 - val_loss: 899329501359.7074\n",
      "Epoch 653/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 844004496480.7578 - val_loss: 899658378520.8439\n",
      "Epoch 654/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 845195541106.6448 - val_loss: 902386182342.7511\n",
      "Epoch 655/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 845335355101.9667 - val_loss: 900104400867.1956\n",
      "Epoch 656/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 844655632243.7852 - val_loss: 905810298882.8805\n",
      "Epoch 657/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 844773049860.4418 - val_loss: 913728721846.5486\n",
      "Epoch 658/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 844468395645.9292 - val_loss: 901495853937.4177\n",
      "Epoch 659/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 844885358492.6012 - val_loss: 898941403343.3925\n",
      "Epoch 660/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 845329470476.2448 - val_loss: 904690224950.3685\n",
      "Epoch 661/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 845969508144.0787 - val_loss: 898951041836.2869\n",
      "Epoch 662/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 844517659787.0143 - val_loss: 899185653987.5555\n",
      "Epoch 663/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 846639678419.5826 - val_loss: 899015287671.1786\n",
      "Epoch 664/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 844674559025.2192 - val_loss: 899709793494.5935\n",
      "Epoch 665/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 844831310918.8276 - val_loss: 900288160400.7427\n",
      "Epoch 666/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 842593551488.4502 - val_loss: 898520038940.0844\n",
      "Epoch 667/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 843411476505.9302 - val_loss: 900126384554.3066\n",
      "Epoch 668/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 844945613308.9988 - val_loss: 910578099385.7891\n",
      "Epoch 669/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 843697499043.0837 - val_loss: 898695383042.8805\n",
      "Epoch 670/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 843373836499.7627 - val_loss: 903446734655.0099\n",
      "Epoch 671/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 843827837055.2496 - val_loss: 899517004925.2996\n",
      "Epoch 672/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 843507660135.9005 - val_loss: 897043444190.1547\n",
      "Epoch 673/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 842990547455.8800 - val_loss: 898224663395.0155\n",
      "Epoch 674/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 843145745712.1989 - val_loss: 897401994872.2588\n",
      "Epoch 675/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 843701114048.7953 - val_loss: 902858091409.1027\n",
      "Epoch 676/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 842175091048.1407 - val_loss: 902502682491.4993\n",
      "Epoch 677/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 842268481523.0350 - val_loss: 898302288252.2194\n",
      "Epoch 678/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 842895598534.1373 - val_loss: 896971854451.9381\n",
      "Epoch 679/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 842377625838.1730 - val_loss: 898261993014.0084\n",
      "Epoch 680/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 844663340429.8354 - val_loss: 897422072300.5570\n",
      "Epoch 681/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 843913834700.5599 - val_loss: 897632169932.1519\n",
      "Epoch 682/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 842457371825.4293 - val_loss: 896449155722.9817\n",
      "Epoch 683/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 842985082909.7716 - val_loss: 897231733358.1772\n",
      "Epoch 684/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 842293380808.3582 - val_loss: 900410783510.6836\n",
      "Epoch 685/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 840919385930.9692 - val_loss: 901432867020.5120\n",
      "Epoch 686/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 842913806663.0077 - val_loss: 898179926917.5809\n",
      "Epoch 687/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 842998774217.6188 - val_loss: 897134024528.2926\n",
      "Epoch 688/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 843626693994.7817 - val_loss: 903535916430.9424\n",
      "Epoch 689/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 841593670860.7999 - val_loss: 899439891267.3306\n",
      "Epoch 690/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 840552098062.3456 - val_loss: 906774640844.5120\n",
      "Epoch 691/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 842416949550.9984 - val_loss: 896248161696.2250\n",
      "Epoch 692/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 844078133959.6379 - val_loss: 896690484346.4191\n",
      "Epoch 693/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 843719487761.7069 - val_loss: 895698181648.5626\n",
      "Epoch 694/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 841357890318.7058 - val_loss: 901250300408.0787\n",
      "Epoch 695/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 841138412037.1620 - val_loss: 899981329513.1365\n",
      "Epoch 696/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 842304075699.6503 - val_loss: 895972558758.7061\n",
      "Epoch 697/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 842611522915.5789 - val_loss: 895108262340.2307\n",
      "Epoch 698/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 842992124749.1301 - val_loss: 899082019173.1758\n",
      "Epoch 699/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 842281751775.5272 - val_loss: 895578100247.7637\n",
      "Epoch 700/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 841040134282.2939 - val_loss: 896905766142.9198\n",
      "Epoch 701/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 93us/step - loss: 840553436425.7838 - val_loss: 895436432984.5739\n",
      "Epoch 702/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 842489892617.1836 - val_loss: 902646447920.6075\n",
      "Epoch 703/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 841247934893.7679 - val_loss: 894328222911.5499\n",
      "Epoch 704/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 840610663659.5321 - val_loss: 899783073619.1730\n",
      "Epoch 705/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 841237300390.8652 - val_loss: 908350500328.2363\n",
      "Epoch 706/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 841226241217.7557 - val_loss: 895263594366.3798\n",
      "Epoch 707/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 840917434771.3575 - val_loss: 894953555072.1801\n",
      "Epoch 708/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 842224570005.9386 - val_loss: 894647076751.6625\n",
      "Epoch 709/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 841061775345.1141 - val_loss: 902104285737.0464\n",
      "Epoch 710/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 841801901082.6504 - val_loss: 897462069446.7511\n",
      "Epoch 711/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 840601726239.3922 - val_loss: 896741368026.9142\n",
      "Epoch 712/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 839978688809.7163 - val_loss: 893925204071.6962\n",
      "Epoch 713/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 839836420664.0619 - val_loss: 893567090757.1309\n",
      "Epoch 714/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 840235571644.1735 - val_loss: 895400525155.7356\n",
      "Epoch 715/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 839974309606.6101 - val_loss: 893947755368.7764\n",
      "Epoch 716/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 840953058727.2853 - val_loss: 893499036460.2869\n",
      "Epoch 717/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 839725235482.5905 - val_loss: 893612548038.3910\n",
      "Epoch 718/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 838837371778.9111 - val_loss: 898763834334.8749\n",
      "Epoch 719/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 842423536210.9524 - val_loss: 893240216545.7552\n",
      "Epoch 720/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 841441852460.4174 - val_loss: 892935337635.4656\n",
      "Epoch 721/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 838796216384.8253 - val_loss: 912201217422.9424\n",
      "Epoch 722/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 839655974379.4719 - val_loss: 893226276089.1589\n",
      "Epoch 723/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 839914262419.4777 - val_loss: 902936078788.2307\n",
      "Epoch 724/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 839489728601.3149 - val_loss: 895493463407.2573\n",
      "Epoch 725/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 839466198887.7805 - val_loss: 899030688539.0043\n",
      "Epoch 726/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 840285430995.0425 - val_loss: 892564907792.9226\n",
      "Epoch 727/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 840089595474.7123 - val_loss: 892438528299.5668\n",
      "Epoch 728/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 838774588644.3292 - val_loss: 895594217679.3925\n",
      "Epoch 729/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 839882097692.5712 - val_loss: 897276431583.2349\n",
      "Epoch 730/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 841027622023.1727 - val_loss: 909721872744.0563\n",
      "Epoch 731/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 840305545407.1146 - val_loss: 893502011757.8171\n",
      "Epoch 732/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 843350264160.9379 - val_loss: 893439493732.0956\n",
      "Epoch 733/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 838485397775.7861 - val_loss: 892900680336.7427\n",
      "Epoch 734/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 839928791389.5765 - val_loss: 897947310895.1674\n",
      "Epoch 735/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 837500328049.8044 - val_loss: 893414060510.1547\n",
      "Epoch 736/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 837062357999.4336 - val_loss: 914415812608.0000\n",
      "Epoch 737/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 840056927607.2666 - val_loss: 901548441830.4360\n",
      "Epoch 738/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 840705188606.3794 - val_loss: 892544836909.0071\n",
      "Epoch 739/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 839601948452.5543 - val_loss: 891885341043.5780\n",
      "Epoch 740/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 839407494243.6390 - val_loss: 892031980562.7229\n",
      "Epoch 741/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 839189448519.1278 - val_loss: 891947938800.1575\n",
      "Epoch 742/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 841314055846.7451 - val_loss: 892326573915.8143\n",
      "Epoch 743/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 840993844983.8969 - val_loss: 892276563495.6062\n",
      "Epoch 744/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 838568359175.8632 - val_loss: 893580343408.3375\n",
      "Epoch 745/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 837034079561.1686 - val_loss: 893704571274.6217\n",
      "Epoch 746/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 840303263194.6654 - val_loss: 896876460766.5148\n",
      "Epoch 747/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 837836620394.4817 - val_loss: 891770066814.3798\n",
      "Epoch 748/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 837873647443.1324 - val_loss: 892003501806.3573\n",
      "Epoch 749/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 838129632910.7357 - val_loss: 892077953006.7173\n",
      "Epoch 750/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 838550053565.0739 - val_loss: 891598514121.2715\n",
      "Epoch 751/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 837250157792.0074 - val_loss: 893051025451.2068\n",
      "Epoch 752/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 839149574987.6896 - val_loss: 891712896286.6047\n",
      "Epoch 753/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 837669346584.6697 - val_loss: 902372200592.0225\n",
      "Epoch 754/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 838393143678.2292 - val_loss: 894753002468.6357\n",
      "Epoch 755/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 838161283057.1141 - val_loss: 894114042046.1097\n",
      "Epoch 756/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 837374489445.1395 - val_loss: 894716712208.2025\n",
      "Epoch 757/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 840642184570.1477 - val_loss: 890113072933.0858\n",
      "Epoch 758/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 837954279772.1360 - val_loss: 891511224593.6427\n",
      "Epoch 759/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 838096033364.1530 - val_loss: 891462689885.6146\n",
      "Epoch 760/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 837190079542.5013 - val_loss: 896655268378.6442\n",
      "Epoch 761/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 838347667894.4113 - val_loss: 890369478800.0225\n",
      "Epoch 762/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 838514394313.4387 - val_loss: 889966313310.6948\n",
      "Epoch 763/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 837174707094.1187 - val_loss: 891403361739.4318\n",
      "Epoch 764/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 838153594554.9130 - val_loss: 890845958518.4585\n",
      "Epoch 765/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 59us/step - loss: 838076656762.2078 - val_loss: 890094863326.8749\n",
      "Epoch 766/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 839441360666.9506 - val_loss: 897468309285.0858\n",
      "Epoch 767/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 838478624349.7567 - val_loss: 894050028214.1885\n",
      "Epoch 768/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 836854994996.3405 - val_loss: 897019334776.9789\n",
      "Epoch 769/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 836342614895.2234 - val_loss: 890937965163.2968\n",
      "Epoch 770/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 836268393569.7181 - val_loss: 904150900171.4318\n",
      "Epoch 771/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 837029428041.7688 - val_loss: 897549702139.6793\n",
      "Epoch 772/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 839882168556.7324 - val_loss: 892294197011.8031\n",
      "Epoch 773/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 836890300276.7456 - val_loss: 895899394313.0015\n",
      "Epoch 774/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 838834640610.2883 - val_loss: 895623486186.0366\n",
      "Epoch 775/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 839019504497.6244 - val_loss: 892994328976.3826\n",
      "Epoch 776/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 838085892240.5365 - val_loss: 889193227242.3966\n",
      "Epoch 777/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 837816392262.4675 - val_loss: 890799871593.8566\n",
      "Epoch 778/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 835886809479.8330 - val_loss: 891967984520.4613\n",
      "Epoch 779/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 836657349806.5482 - val_loss: 889490481117.4346\n",
      "Epoch 780/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 836576083184.3339 - val_loss: 889357278966.9985\n",
      "Epoch 781/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 837642227795.5526 - val_loss: 889134962788.8158\n",
      "Epoch 782/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 835889488158.1918 - val_loss: 888517271425.2603\n",
      "Epoch 783/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 835562679733.2108 - val_loss: 888891082777.9241\n",
      "Epoch 784/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 836315640800.0675 - val_loss: 889323532996.5907\n",
      "Epoch 785/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 836419919086.4131 - val_loss: 892613842261.3334\n",
      "Epoch 786/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 837394724491.8546 - val_loss: 889774209044.1632\n",
      "Epoch 787/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 835133914548.2505 - val_loss: 888995568915.0830\n",
      "Epoch 788/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 836479230795.2094 - val_loss: 888742111615.0999\n",
      "Epoch 789/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 836232130050.0408 - val_loss: 889000434248.7313\n",
      "Epoch 790/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 833568077267.4625 - val_loss: 890339837171.3981\n",
      "Epoch 791/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 837106749181.6591 - val_loss: 899988216231.4261\n",
      "Epoch 792/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 835244929904.6641 - val_loss: 890621918863.3024\n",
      "Epoch 793/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 835765604450.9187 - val_loss: 890003819351.4937\n",
      "Epoch 794/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 834967265673.7539 - val_loss: 890052098091.2068\n",
      "Epoch 795/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 836405522494.4244 - val_loss: 899802083114.8467\n",
      "Epoch 796/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 834833896656.1614 - val_loss: 890728231251.8931\n",
      "Epoch 797/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 835393069000.2982 - val_loss: 891262829511.8312\n",
      "Epoch 798/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 836628722682.9580 - val_loss: 889709412098.5204\n",
      "Epoch 799/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 835120679380.1829 - val_loss: 888835780348.7595\n",
      "Epoch 800/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 834700047343.4336 - val_loss: 892637202573.1421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f94010b70>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_train1 = model1.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4265 samples, validate on 2844 samples\n",
      "Epoch 1/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1899231554152.0808 - val_loss: 2005104614075.9497\n",
      "Epoch 2/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1897355361577.9565 - val_loss: 2004679084495.7527\n",
      "Epoch 3/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1895276279996.7136 - val_loss: 2004716062285.0520\n",
      "Epoch 4/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1892987916690.3972 - val_loss: 2006399522478.9873\n",
      "Epoch 5/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1892279670966.9517 - val_loss: 1995684875925.0635\n",
      "Epoch 6/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1889147911262.3567 - val_loss: 1992950695017.1362\n",
      "Epoch 7/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1887737617400.5571 - val_loss: 1993433390977.2605\n",
      "Epoch 8/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1885917079995.2131 - val_loss: 1992051633087.1902\n",
      "Epoch 9/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1881610792903.3379 - val_loss: 1986192648137.2717\n",
      "Epoch 10/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1880076711955.6877 - val_loss: 1986642428065.3052\n",
      "Epoch 11/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1878861588558.2705 - val_loss: 1981453701817.0691\n",
      "Epoch 12/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1878248300795.6184 - val_loss: 1981027543791.7974\n",
      "Epoch 13/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1874483704961.4106 - val_loss: 1977576964626.0029\n",
      "Epoch 14/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1871995355608.5046 - val_loss: 1980262550820.3660\n",
      "Epoch 15/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1869828283211.6895 - val_loss: 1972626435283.7131\n",
      "Epoch 16/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1866996798933.3833 - val_loss: 1977199030220.1519\n",
      "Epoch 17/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1866193089400.3469 - val_loss: 1972543990442.6667\n",
      "Epoch 18/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1866419268288.4353 - val_loss: 1967916429697.9800\n",
      "Epoch 19/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1864422644179.2224 - val_loss: 1963850779410.3628\n",
      "Epoch 20/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1860420951266.8887 - val_loss: 1961724774425.9241\n",
      "Epoch 21/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1859600262516.3853 - val_loss: 1963924970402.3855\n",
      "Epoch 22/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1850651519907.0837 - val_loss: 1976102553579.8369\n",
      "Epoch 23/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1857270576372.6555 - val_loss: 1959209509566.8298\n",
      "Epoch 24/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1852482128328.6584 - val_loss: 1954230278590.4697\n",
      "Epoch 25/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1852521125629.1790 - val_loss: 1951529871489.6204\n",
      "Epoch 26/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1849584896660.2578 - val_loss: 1950338463141.9858\n",
      "Epoch 27/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1849058717151.4673 - val_loss: 1950992179119.3474\n",
      "Epoch 28/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1848547368815.9436 - val_loss: 1945126997837.4121\n",
      "Epoch 29/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1844488188461.9780 - val_loss: 1944488667795.6230\n",
      "Epoch 30/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1842707379456.1802 - val_loss: 1941676944987.4543\n",
      "Epoch 31/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1842668083763.2600 - val_loss: 1947579529351.3811\n",
      "Epoch 32/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1840729173836.4099 - val_loss: 1957654926700.3767\n",
      "Epoch 33/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1838938669866.5566 - val_loss: 1935526419592.8213\n",
      "Epoch 34/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1836179326510.6982 - val_loss: 1939443638859.6118\n",
      "Epoch 35/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1836156164782.1882 - val_loss: 1932404430372.7258\n",
      "Epoch 36/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1832406200824.1970 - val_loss: 1932651869237.2883\n",
      "Epoch 37/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1833648710986.1289 - val_loss: 1936485284274.9480\n",
      "Epoch 38/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1829434570921.5063 - val_loss: 1926826761145.4290\n",
      "Epoch 39/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1827825685047.5815 - val_loss: 1927430318762.6667\n",
      "Epoch 40/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1824867004904.8308 - val_loss: 1935204089763.8257\n",
      "Epoch 41/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1827763020123.6558 - val_loss: 1923804083575.8987\n",
      "Epoch 42/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1825927912982.2087 - val_loss: 1921211641458.4978\n",
      "Epoch 43/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1823323092819.6128 - val_loss: 1917705585100.8721\n",
      "Epoch 44/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1824600038868.9031 - val_loss: 1921624707031.6736\n",
      "Epoch 45/800\n",
      "4265/4265 [==============================] - 0s 51us/step - loss: 1818438062397.8843 - val_loss: 1914337817998.9424\n",
      "Epoch 46/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1817508216117.4807 - val_loss: 1912984487063.2236\n",
      "Epoch 47/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1815236606517.1809 - val_loss: 1911260175174.2109\n",
      "Epoch 48/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1814432273954.2134 - val_loss: 1908929452317.1646\n",
      "Epoch 49/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1812642107759.8235 - val_loss: 1912556917110.4585\n",
      "Epoch 50/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1809798669715.1174 - val_loss: 1906864998817.6653\n",
      "Epoch 51/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1811011022929.6321 - val_loss: 1905260148640.9451\n",
      "Epoch 52/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1808514238824.6208 - val_loss: 1904369089108.2532\n",
      "Epoch 53/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1806854212036.8169 - val_loss: 1901106648220.9846\n",
      "Epoch 54/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1805342511241.3337 - val_loss: 1900812181054.6499\n",
      "Epoch 55/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1806391270895.7937 - val_loss: 1908468912318.1096\n",
      "Epoch 56/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1805271033085.2991 - val_loss: 1896708364665.3389\n",
      "Epoch 57/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1800575635266.8062 - val_loss: 1897786845359.7075\n",
      "Epoch 58/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1800505464358.0547 - val_loss: 1899180804375.4036\n",
      "Epoch 59/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1800669531068.0535 - val_loss: 1892402193386.3967\n",
      "Epoch 60/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1796891525139.2075 - val_loss: 1892692405782.3235\n",
      "Epoch 61/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1795501611037.2915 - val_loss: 1890136390435.6455\n",
      "Epoch 62/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1791358206981.5222 - val_loss: 1897227300097.8003\n",
      "Epoch 63/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1796201894159.7861 - val_loss: 1886166977782.2786\n",
      "Epoch 64/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 61us/step - loss: 1792160993970.5098 - val_loss: 1884531003259.4993\n",
      "Epoch 65/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1792130243741.7417 - val_loss: 1884744945208.8889\n",
      "Epoch 66/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1790370776976.1162 - val_loss: 1883993992242.4080\n",
      "Epoch 67/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1788428236400.9641 - val_loss: 1880860332821.2434\n",
      "Epoch 68/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1788293006114.8735 - val_loss: 1879661652097.6204\n",
      "Epoch 69/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1785646338458.0803 - val_loss: 1879177254782.3796\n",
      "Epoch 70/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1786781865072.1238 - val_loss: 1876161480891.2292\n",
      "Epoch 71/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1785648258962.5173 - val_loss: 1874752743369.2715\n",
      "Epoch 72/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1784431308086.4412 - val_loss: 1873350826015.6851\n",
      "Epoch 73/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1781434047753.3037 - val_loss: 1872059808662.8635\n",
      "Epoch 74/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1781534207413.2107 - val_loss: 1875123249440.0449\n",
      "Epoch 75/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1778975824385.3206 - val_loss: 1869775279191.8538\n",
      "Epoch 76/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1776965527176.9736 - val_loss: 1868109630042.0142\n",
      "Epoch 77/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1777260699231.1973 - val_loss: 1869534376414.1548\n",
      "Epoch 78/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1772970949000.3132 - val_loss: 1866065764063.9551\n",
      "Epoch 79/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1775074115376.5591 - val_loss: 1864406414112.7651\n",
      "Epoch 80/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1774104490326.8540 - val_loss: 1863140866206.4248\n",
      "Epoch 81/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1772185774615.4092 - val_loss: 1861818259026.8130\n",
      "Epoch 82/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1770520870504.8008 - val_loss: 1860585092172.3320\n",
      "Epoch 83/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1771329169493.9536 - val_loss: 1859938436754.1829\n",
      "Epoch 84/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1771494538134.5986 - val_loss: 1858588377641.0464\n",
      "Epoch 85/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1769001839234.9712 - val_loss: 1862521714892.5120\n",
      "Epoch 86/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1767226064838.3774 - val_loss: 1857037940041.8115\n",
      "Epoch 87/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1766603107711.4297 - val_loss: 1854614428347.9495\n",
      "Epoch 88/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1764556242644.3628 - val_loss: 1854438978428.9395\n",
      "Epoch 89/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1764772300684.2747 - val_loss: 1852975927693.5022\n",
      "Epoch 90/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1764377735119.0208 - val_loss: 1851720561371.6343\n",
      "Epoch 91/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1762653258600.0205 - val_loss: 1851973555771.7693\n",
      "Epoch 92/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1761467015757.6704 - val_loss: 1851933332337.4177\n",
      "Epoch 93/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1763588361512.9961 - val_loss: 1852420207853.6372\n",
      "Epoch 94/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1759674942934.3438 - val_loss: 1848017725569.6204\n",
      "Epoch 95/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1759697862832.7090 - val_loss: 1846512620078.8074\n",
      "Epoch 96/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1754442944891.3481 - val_loss: 1855776445412.6357\n",
      "Epoch 97/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1758887087304.9585 - val_loss: 1852520047479.1787\n",
      "Epoch 98/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1758050240188.8340 - val_loss: 1852501445210.0142\n",
      "Epoch 99/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1755311539090.2771 - val_loss: 1843993062473.4514\n",
      "Epoch 100/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1753796951914.1816 - val_loss: 1841007506116.5908\n",
      "Epoch 101/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1753134193399.1768 - val_loss: 1839834182755.3755\n",
      "Epoch 102/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 1751414006130.7048 - val_loss: 1840034259278.1323\n",
      "Epoch 103/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1753155887705.1948 - val_loss: 1845123933991.9663\n",
      "Epoch 104/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1750672438493.6064 - val_loss: 1836846948824.3938\n",
      "Epoch 105/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1753052592182.2612 - val_loss: 1836251172754.5430\n",
      "Epoch 106/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1750391409031.3528 - val_loss: 1836026676467.3979\n",
      "Epoch 107/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1749349482300.0835 - val_loss: 1834775454122.3066\n",
      "Epoch 108/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1745971684209.6245 - val_loss: 1837783003475.8931\n",
      "Epoch 109/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1748268192142.5557 - val_loss: 1832703445087.0549\n",
      "Epoch 110/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1746355784912.6416 - val_loss: 1831217094137.5190\n",
      "Epoch 111/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1743732913431.2290 - val_loss: 1835855562247.9211\n",
      "Epoch 112/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1747906215010.1985 - val_loss: 1829198449656.7988\n",
      "Epoch 113/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1744136579119.5386 - val_loss: 1828424625271.5386\n",
      "Epoch 114/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1743373167150.4580 - val_loss: 1827413386356.6582\n",
      "Epoch 115/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1740811166350.9758 - val_loss: 1831454085217.9353\n",
      "Epoch 116/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1745290112268.1848 - val_loss: 1830033805064.2812\n",
      "Epoch 117/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1740998800851.4626 - val_loss: 1832839861911.9438\n",
      "Epoch 118/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1741756868563.3425 - val_loss: 1824503123397.6709\n",
      "Epoch 119/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1738124749181.7493 - val_loss: 1831343164385.7554\n",
      "Epoch 120/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1738836112697.8025 - val_loss: 1826855651584.3601\n",
      "Epoch 121/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1737708651606.1938 - val_loss: 1823466066248.3713\n",
      "Epoch 122/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1736800008749.4976 - val_loss: 1823324607299.3306\n",
      "Epoch 123/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1738537489285.7922 - val_loss: 1820783488810.8467\n",
      "Epoch 124/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1734499923826.3447 - val_loss: 1820476187465.0913\n",
      "Epoch 125/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1733715880732.6313 - val_loss: 1822153323573.2883\n",
      "Epoch 126/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1733240568131.1660 - val_loss: 1818070190382.4473\n",
      "Epoch 127/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 60us/step - loss: 1733714603003.4382 - val_loss: 1817396587962.1492\n",
      "Epoch 128/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1733794831513.9001 - val_loss: 1817451139169.9353\n",
      "Epoch 129/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1731912377155.5264 - val_loss: 1817359841530.5991\n",
      "Epoch 130/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1734521694409.1987 - val_loss: 1817433306741.3784\n",
      "Epoch 131/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1731583114011.6709 - val_loss: 1819663992350.9648\n",
      "Epoch 132/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1731046720137.2136 - val_loss: 1813728673722.8691\n",
      "Epoch 133/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1731053074178.9409 - val_loss: 1816059319153.4177\n",
      "Epoch 134/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1730015574316.1172 - val_loss: 1813730878718.9199\n",
      "Epoch 135/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1727808055958.1787 - val_loss: 1811841557136.7427\n",
      "Epoch 136/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1728131605607.4802 - val_loss: 1811044926407.8313\n",
      "Epoch 137/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1729628274979.9541 - val_loss: 1816387066993.7778\n",
      "Epoch 138/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1727268529309.2612 - val_loss: 1810628119475.6680\n",
      "Epoch 139/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1727037323525.7021 - val_loss: 1811825676040.2812\n",
      "Epoch 140/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1724349609158.5576 - val_loss: 1808600438216.5513\n",
      "Epoch 141/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1724418234063.0811 - val_loss: 1816127613564.5793\n",
      "Epoch 142/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1723416844966.5051 - val_loss: 1809340105164.8721\n",
      "Epoch 143/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1724037237876.4456 - val_loss: 1809331955657.2715\n",
      "Epoch 144/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1723443269050.9731 - val_loss: 1806276584380.3093\n",
      "Epoch 145/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1724054454375.7205 - val_loss: 1807487434337.2151\n",
      "Epoch 146/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1724086149828.7568 - val_loss: 1805636310196.0281\n",
      "Epoch 147/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1722500169501.3513 - val_loss: 1806660263832.3037\n",
      "Epoch 148/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1723309534368.6228 - val_loss: 1806437946916.7258\n",
      "Epoch 149/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1720882419933.3665 - val_loss: 1806647461176.5288\n",
      "Epoch 150/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1722342480327.6980 - val_loss: 1802782908888.3938\n",
      "Epoch 151/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1720194166577.5195 - val_loss: 1802930833318.7061\n",
      "Epoch 152/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1719432879503.7561 - val_loss: 1803891082276.0056\n",
      "Epoch 153/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1719189614758.1448 - val_loss: 1801795700024.5288\n",
      "Epoch 154/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1716931125357.4827 - val_loss: 1804715231672.7087\n",
      "Epoch 155/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1717133225842.8247 - val_loss: 1804908209107.3530\n",
      "Epoch 156/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1718802699403.0142 - val_loss: 1807095635057.7778\n",
      "Epoch 157/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1718635169257.5513 - val_loss: 1799738571993.4739\n",
      "Epoch 158/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1717243008231.2104 - val_loss: 1798898145519.0774\n",
      "Epoch 159/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1716089055306.4292 - val_loss: 1801068841262.4473\n",
      "Epoch 160/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1719844177339.4531 - val_loss: 1799659874594.9255\n",
      "Epoch 161/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1717410537961.7913 - val_loss: 1797744250678.3684\n",
      "Epoch 162/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1713426531717.6721 - val_loss: 1811425222690.5654\n",
      "Epoch 163/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1721592982511.9136 - val_loss: 1799880902912.3601\n",
      "Epoch 164/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1715359580866.8362 - val_loss: 1797746972588.4670\n",
      "Epoch 165/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1714179189828.4268 - val_loss: 1796249572693.3333\n",
      "Epoch 166/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1713939981968.6565 - val_loss: 1796088352474.1941\n",
      "Epoch 167/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1710580977866.6392 - val_loss: 1804323435158.5034\n",
      "Epoch 168/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1713988971350.2537 - val_loss: 1797022573408.1350\n",
      "Epoch 169/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1712277468147.2749 - val_loss: 1802093968094.5146\n",
      "Epoch 170/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1715043287566.2856 - val_loss: 1794415109628.3994\n",
      "Epoch 171/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1711452431647.8726 - val_loss: 1792729984764.7595\n",
      "Epoch 172/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1712735040652.9351 - val_loss: 1792939715081.3616\n",
      "Epoch 173/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1711911215732.0854 - val_loss: 1792560015535.7075\n",
      "Epoch 174/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1709552386920.0205 - val_loss: 1793476980684.1519\n",
      "Epoch 175/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1710865810552.7671 - val_loss: 1795931175946.0815\n",
      "Epoch 176/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1711916256241.8345 - val_loss: 1790862369148.2195\n",
      "Epoch 177/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1710537142902.9666 - val_loss: 1791883114562.2502\n",
      "Epoch 178/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1708177551629.8655 - val_loss: 1793029720619.9268\n",
      "Epoch 179/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1710060555123.0649 - val_loss: 1790956200477.5247\n",
      "Epoch 180/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1708970558342.7527 - val_loss: 1789355716233.5415\n",
      "Epoch 181/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1708996202264.5496 - val_loss: 1788899100651.8369\n",
      "Epoch 182/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1707760115552.5776 - val_loss: 1788472607664.7876\n",
      "Epoch 183/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1708006013662.2068 - val_loss: 1788321330667.1167\n",
      "Epoch 184/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1706220187164.9312 - val_loss: 1792785649254.9761\n",
      "Epoch 185/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1712427384751.3286 - val_loss: 1788531051213.2322\n",
      "Epoch 186/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1706682885000.6733 - val_loss: 1788013132833.1252\n",
      "Epoch 187/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1707046165108.5654 - val_loss: 1786869022858.2617\n",
      "Epoch 188/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1703789276128.5479 - val_loss: 1803623298308.6807\n",
      "Epoch 189/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1708819091416.6248 - val_loss: 1787222851998.7849\n",
      "Epoch 190/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 70us/step - loss: 1705738811307.9673 - val_loss: 1785866113191.0662\n",
      "Epoch 191/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1703813999534.6082 - val_loss: 1790074757500.2195\n",
      "Epoch 192/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1708201236048.3115 - val_loss: 1787428877826.1604\n",
      "Epoch 193/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1711116855689.2734 - val_loss: 1784974707451.3193\n",
      "Epoch 194/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1703718295646.5969 - val_loss: 1785990851887.8875\n",
      "Epoch 195/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1706036017021.8691 - val_loss: 1789918183977.0464\n",
      "Epoch 196/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1703811366662.3025 - val_loss: 1786971692053.6033\n",
      "Epoch 197/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1705325060105.6038 - val_loss: 1784521783638.7734\n",
      "Epoch 198/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1707208855977.4463 - val_loss: 1783772550462.2898\n",
      "Epoch 199/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1701773884614.0774 - val_loss: 1783471646204.3994\n",
      "Epoch 200/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 1704206445574.9629 - val_loss: 1783595597547.4768\n",
      "Epoch 201/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1703399534135.8218 - val_loss: 1784665407764.5232\n",
      "Epoch 202/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1704177784415.6772 - val_loss: 1782422142745.5640\n",
      "Epoch 203/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1702558579718.0024 - val_loss: 1783465578179.1504\n",
      "Epoch 204/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1702379658078.8970 - val_loss: 1785486362027.7468\n",
      "Epoch 205/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1703863590049.5830 - val_loss: 1792741613856.0449\n",
      "Epoch 206/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1702184446049.4780 - val_loss: 1783662579395.1504\n",
      "Epoch 207/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1702255134270.0645 - val_loss: 1782175524417.5303\n",
      "Epoch 208/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1701530300609.0354 - val_loss: 1780967207044.5007\n",
      "Epoch 209/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1700663552819.6802 - val_loss: 1785036294699.9268\n",
      "Epoch 210/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1702407342575.0735 - val_loss: 1783716189317.9409\n",
      "Epoch 211/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1701394988207.0283 - val_loss: 1787768287018.8467\n",
      "Epoch 212/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1703598497067.8772 - val_loss: 1784788768085.3333\n",
      "Epoch 213/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1702089034238.4395 - val_loss: 1779698287214.1772\n",
      "Epoch 214/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1702062145086.5444 - val_loss: 1781093879375.9324\n",
      "Epoch 215/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1701154370378.2490 - val_loss: 1784413993971.0381\n",
      "Epoch 216/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1703117736957.3589 - val_loss: 1780869659980.6919\n",
      "Epoch 217/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1700836328119.3118 - val_loss: 1780501685203.3530\n",
      "Epoch 218/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1701031017489.5269 - val_loss: 1779193944095.6851\n",
      "Epoch 219/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1698444851162.0652 - val_loss: 1779258489036.5120\n",
      "Epoch 220/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1700491360352.5178 - val_loss: 1779445080444.2195\n",
      "Epoch 221/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1700428848012.9951 - val_loss: 1782281440917.0632\n",
      "Epoch 222/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1697516187541.3984 - val_loss: 1784186239318.7734\n",
      "Epoch 223/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1698181091949.3625 - val_loss: 1781648062115.4656\n",
      "Epoch 224/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1698199977596.0085 - val_loss: 1781689827627.5669\n",
      "Epoch 225/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1698484062341.9724 - val_loss: 1778564915925.8735\n",
      "Epoch 226/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1701184198124.4324 - val_loss: 1776959578515.2629\n",
      "Epoch 227/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1697850804277.3008 - val_loss: 1777775985815.2236\n",
      "Epoch 228/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1698834905046.2236 - val_loss: 1776655404646.9761\n",
      "Epoch 229/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1697435397280.6228 - val_loss: 1777404654414.8523\n",
      "Epoch 230/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1698611807018.5566 - val_loss: 1778758488991.5049\n",
      "Epoch 231/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1701191371804.0911 - val_loss: 1776744386312.2812\n",
      "Epoch 232/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1698254897778.1646 - val_loss: 1776452793143.8088\n",
      "Epoch 233/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1696882764353.6658 - val_loss: 1785203087230.3796\n",
      "Epoch 234/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1700169053812.8057 - val_loss: 1778786854189.0071\n",
      "Epoch 235/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 1698732332808.7034 - val_loss: 1777837239945.5415\n",
      "Epoch 236/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1697867185219.4663 - val_loss: 1775443697099.4319\n",
      "Epoch 237/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1694210330676.8206 - val_loss: 1776376951861.2883\n",
      "Epoch 238/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1695550134370.4385 - val_loss: 1777941983249.2827\n",
      "Epoch 239/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 1696128311032.8572 - val_loss: 1781251413356.3770\n",
      "Epoch 240/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1696390251709.9141 - val_loss: 1782185460080.6975\n",
      "Epoch 241/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1697630515761.8193 - val_loss: 1776408376327.2012\n",
      "Epoch 242/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1696245798872.1443 - val_loss: 1776144836622.4023\n",
      "Epoch 243/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1696312312088.4297 - val_loss: 1777488663455.5049\n",
      "Epoch 244/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1697702563074.5811 - val_loss: 1777601614705.4177\n",
      "Epoch 245/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1697178003034.8755 - val_loss: 1776597611849.8115\n",
      "Epoch 246/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 1695593639733.3608 - val_loss: 1774157767589.2659\n",
      "Epoch 247/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1694693236127.1221 - val_loss: 1774207021534.1548\n",
      "Epoch 248/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1697341758368.2026 - val_loss: 1777302594123.6118\n",
      "Epoch 249/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1696456452381.7114 - val_loss: 1773407450441.8115\n",
      "Epoch 250/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1694583579847.5181 - val_loss: 1774651970417.4177\n",
      "Epoch 251/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1693956274756.5469 - val_loss: 1779462568693.5583\n",
      "Epoch 252/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1695116983866.7029 - val_loss: 1779846027480.0337\n",
      "Epoch 253/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 60us/step - loss: 1695388752715.6895 - val_loss: 1773638843402.0815\n",
      "Epoch 254/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1696141593820.1658 - val_loss: 1773828130726.7061\n",
      "Epoch 255/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1694189879603.3201 - val_loss: 1773035036176.5625\n",
      "Epoch 256/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1694342659001.1724 - val_loss: 1773701149795.3755\n",
      "Epoch 257/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1695595647739.4980 - val_loss: 1772783302537.9016\n",
      "Epoch 258/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1694537328318.7546 - val_loss: 1774378800146.7229\n",
      "Epoch 259/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1694366950100.1228 - val_loss: 1772582935112.7314\n",
      "Epoch 260/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1695756670190.8933 - val_loss: 1773848139761.5977\n",
      "Epoch 261/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1697619471140.3142 - val_loss: 1773021124478.3796\n",
      "Epoch 262/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1692184058173.4041 - val_loss: 1776416787978.8018\n",
      "Epoch 263/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1693921386908.9614 - val_loss: 1773789421533.4346\n",
      "Epoch 264/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1694927989966.7207 - val_loss: 1772732096892.2195\n",
      "Epoch 265/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1694237153416.8535 - val_loss: 1772263665658.2390\n",
      "Epoch 266/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1697184542129.3691 - val_loss: 1773250649616.5625\n",
      "Epoch 267/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1692956486274.9712 - val_loss: 1771531723293.5247\n",
      "Epoch 268/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1694480881908.4155 - val_loss: 1775247193440.8552\n",
      "Epoch 269/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1693846344685.5129 - val_loss: 1776333621043.4880\n",
      "Epoch 270/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1694871449086.1995 - val_loss: 1772741873462.3684\n",
      "Epoch 271/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1694389473970.5098 - val_loss: 1772823294061.4570\n",
      "Epoch 272/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1691434994846.7019 - val_loss: 1771180684640.8552\n",
      "Epoch 273/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1695838152136.8984 - val_loss: 1771114063748.1406\n",
      "Epoch 274/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1694149622428.9014 - val_loss: 1771346724106.4417\n",
      "Epoch 275/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1692225456703.7449 - val_loss: 1775737422814.8748\n",
      "Epoch 276/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1694755949085.1714 - val_loss: 1771178999139.7356\n",
      "Epoch 277/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1692624475582.3342 - val_loss: 1771291197140.4331\n",
      "Epoch 278/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1692953608222.7322 - val_loss: 1771444686139.4092\n",
      "Epoch 279/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1692960266434.4758 - val_loss: 1773507549261.7722\n",
      "Epoch 280/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1693550163883.9673 - val_loss: 1772814001591.2686\n",
      "Epoch 281/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1693145834146.1833 - val_loss: 1783584637089.3052\n",
      "Epoch 282/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1693289520927.2722 - val_loss: 1772311890752.4500\n",
      "Epoch 283/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1691028709324.1396 - val_loss: 1773102990648.5288\n",
      "Epoch 284/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1695322029599.8123 - val_loss: 1770443453677.6372\n",
      "Epoch 285/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1693694737775.1035 - val_loss: 1770342925509.3108\n",
      "Epoch 286/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1692629177579.0518 - val_loss: 1773323941539.4656\n",
      "Epoch 287/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1692646230046.2517 - val_loss: 1769898482479.1675\n",
      "Epoch 288/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1692212958919.3979 - val_loss: 1770149773865.0464\n",
      "Epoch 289/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1694553645369.0823 - val_loss: 1769765115520.9001\n",
      "Epoch 290/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1691756638865.8569 - val_loss: 1771044284960.4050\n",
      "Epoch 291/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1691032256135.5330 - val_loss: 1777988591604.4783\n",
      "Epoch 292/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1693727329527.0566 - val_loss: 1770064315105.3953\n",
      "Epoch 293/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1692499328282.5903 - val_loss: 1769523879809.2603\n",
      "Epoch 294/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1691885449190.0701 - val_loss: 1769651265570.5654\n",
      "Epoch 295/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1690948270883.3538 - val_loss: 1769699646808.2139\n",
      "Epoch 296/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1692227081935.0811 - val_loss: 1769376774440.6863\n",
      "Epoch 297/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1693510022016.0300 - val_loss: 1773683595549.1646\n",
      "Epoch 298/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1691940058164.5806 - val_loss: 1771902249273.9690\n",
      "Epoch 299/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1692015684649.5364 - val_loss: 1769604561353.9915\n",
      "Epoch 300/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1692760122625.1404 - val_loss: 1770169895537.0576\n",
      "Epoch 301/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1691745551465.4014 - val_loss: 1769258614599.6511\n",
      "Epoch 302/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1691137013979.4458 - val_loss: 1769218036662.5486\n",
      "Epoch 303/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1691156218655.0320 - val_loss: 1770417607422.1997\n",
      "Epoch 304/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1694154928924.1509 - val_loss: 1771448101201.0127\n",
      "Epoch 305/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1691231953716.1602 - val_loss: 1774620237167.2573\n",
      "Epoch 306/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1692737125097.7314 - val_loss: 1769117993331.5781\n",
      "Epoch 307/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1691227047720.3960 - val_loss: 1774475944224.0449\n",
      "Epoch 308/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1695475073876.8130 - val_loss: 1770095683160.5737\n",
      "Epoch 309/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1691696557427.9053 - val_loss: 1768510232192.9001\n",
      "Epoch 310/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1691175378400.4277 - val_loss: 1768744051183.4375\n",
      "Epoch 311/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1690073004551.0828 - val_loss: 1768893554376.9114\n",
      "Epoch 312/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1692279642052.9368 - val_loss: 1768532283812.5457\n",
      "Epoch 313/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1690228966493.8767 - val_loss: 1775026892379.4543\n",
      "Epoch 314/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1693000746415.9287 - val_loss: 1770824731249.0576\n",
      "Epoch 315/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1692561665541.8823 - val_loss: 1768537283167.7749\n",
      "Epoch 316/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 57us/step - loss: 1689511196092.4136 - val_loss: 1774608215781.7158\n",
      "Epoch 317/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1691236805654.3286 - val_loss: 1768663683216.0225\n",
      "Epoch 318/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1691195730178.8210 - val_loss: 1768263426382.1323\n",
      "Epoch 319/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1692516729665.6055 - val_loss: 1768354226832.7427\n",
      "Epoch 320/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1689808171147.2544 - val_loss: 1768415715086.0422\n",
      "Epoch 321/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1689367560603.0405 - val_loss: 1768444000829.2095\n",
      "Epoch 322/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1690373211122.3147 - val_loss: 1769446950315.7468\n",
      "Epoch 323/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1690982552987.7610 - val_loss: 1768525425299.6230\n",
      "Epoch 324/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1692231272643.4363 - val_loss: 1775239445260.6021\n",
      "Epoch 325/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1693522681911.7017 - val_loss: 1770196063112.4614\n",
      "Epoch 326/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1690011522225.4294 - val_loss: 1770446083044.6357\n",
      "Epoch 327/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1692583171337.7837 - val_loss: 1768602047114.9817\n",
      "Epoch 328/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1691225850001.9771 - val_loss: 1768246241939.6230\n",
      "Epoch 329/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1689626233254.0850 - val_loss: 1782590277704.0112\n",
      "Epoch 330/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 1691725548280.1370 - val_loss: 1769724562102.1885\n",
      "Epoch 331/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1691690897268.7456 - val_loss: 1767825649773.4570\n",
      "Epoch 332/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1690915535059.7627 - val_loss: 1768736080862.8748\n",
      "Epoch 333/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1690965672881.0090 - val_loss: 1773820492049.6428\n",
      "Epoch 334/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1693240693303.5815 - val_loss: 1768968814960.6975\n",
      "Epoch 335/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1688357303694.3157 - val_loss: 1774077945109.9634\n",
      "Epoch 336/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1691694879276.0574 - val_loss: 1769351752040.0562\n",
      "Epoch 337/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1688359209324.4624 - val_loss: 1775626391004.7146\n",
      "Epoch 338/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1694171711364.5918 - val_loss: 1768639179941.6260\n",
      "Epoch 339/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1692748882179.3013 - val_loss: 1775674205554.1379\n",
      "Epoch 340/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1690490652064.5627 - val_loss: 1769116935538.1379\n",
      "Epoch 341/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1690498752437.0908 - val_loss: 1769100388389.4458\n",
      "Epoch 342/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1690776257553.0466 - val_loss: 1768064494296.7539\n",
      "Epoch 343/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1690108522143.5422 - val_loss: 1770394182588.3093\n",
      "Epoch 344/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1690244160489.1912 - val_loss: 1767592195793.5527\n",
      "Epoch 345/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1692849140611.8716 - val_loss: 1767827996165.0408\n",
      "Epoch 346/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1688974721438.8821 - val_loss: 1767568602649.2039\n",
      "Epoch 347/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1689391944236.0574 - val_loss: 1767587315712.0000\n",
      "Epoch 348/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1689837506612.8206 - val_loss: 1768321650186.8018\n",
      "Epoch 349/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1692146080422.5051 - val_loss: 1770410430170.1941\n",
      "Epoch 350/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1692009327672.9021 - val_loss: 1767792694106.3740\n",
      "Epoch 351/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1690313500635.0254 - val_loss: 1767388709002.2617\n",
      "Epoch 352/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1687940500727.0566 - val_loss: 1778164670890.3066\n",
      "Epoch 353/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1692134500142.1580 - val_loss: 1768071878203.7693\n",
      "Epoch 354/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1689958288369.8345 - val_loss: 1772307184511.8201\n",
      "Epoch 355/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1691218968491.4868 - val_loss: 1767280124053.7834\n",
      "Epoch 356/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1691275196704.5928 - val_loss: 1767556826816.2700\n",
      "Epoch 357/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1690129987772.7136 - val_loss: 1767717735301.5808\n",
      "Epoch 358/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1690028494651.1230 - val_loss: 1768907115812.3657\n",
      "Epoch 359/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1688938603343.0508 - val_loss: 1770613174957.5471\n",
      "Epoch 360/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1691536546512.7615 - val_loss: 1770110811376.5176\n",
      "Epoch 361/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1690678310536.0132 - val_loss: 1768581079097.6089\n",
      "Epoch 362/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1692222889321.3411 - val_loss: 1774878518322.4080\n",
      "Epoch 363/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1691064832352.4578 - val_loss: 1767307178346.9368\n",
      "Epoch 364/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1691452881395.8752 - val_loss: 1770692132287.9099\n",
      "Epoch 365/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1689565165426.3447 - val_loss: 1771312014544.8325\n",
      "Epoch 366/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1696025360023.3792 - val_loss: 1767051880139.7917\n",
      "Epoch 367/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1690009127526.1599 - val_loss: 1769273751111.2913\n",
      "Epoch 368/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 1692404782038.4636 - val_loss: 1767768696685.0972\n",
      "Epoch 369/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1689964415363.5115 - val_loss: 1772689468812.0618\n",
      "Epoch 370/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1689416110178.9185 - val_loss: 1767720513069.3672\n",
      "Epoch 371/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1687965616058.3728 - val_loss: 1776778964617.5415\n",
      "Epoch 372/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1694759301823.9551 - val_loss: 1767586977431.9438\n",
      "Epoch 373/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1690916064756.5955 - val_loss: 1768824262270.0198\n",
      "Epoch 374/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1689193018044.1135 - val_loss: 1767000117023.3250\n",
      "Epoch 375/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1690425179034.4404 - val_loss: 1767687223730.9480\n",
      "Epoch 376/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 1690619320161.0581 - val_loss: 1772034903908.4558\n",
      "Epoch 377/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1692298324100.0515 - val_loss: 1767166285164.3770\n",
      "Epoch 378/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 1690337514362.2676 - val_loss: 1767030490885.4009\n",
      "Epoch 379/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 63us/step - loss: 1690313814915.8716 - val_loss: 1767152655704.2139\n",
      "Epoch 380/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1690059057906.3745 - val_loss: 1770998765922.2954\n",
      "Epoch 381/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1690531706673.0391 - val_loss: 1767244119810.5205\n",
      "Epoch 382/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1690599278207.3696 - val_loss: 1768371647610.4192\n",
      "Epoch 383/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1689336321581.7380 - val_loss: 1768124124498.4529\n",
      "Epoch 384/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1692412090531.7441 - val_loss: 1767280339557.5359\n",
      "Epoch 385/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1688896283282.8174 - val_loss: 1768332577093.4910\n",
      "Epoch 386/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1689273616214.4937 - val_loss: 1767614885075.7131\n",
      "Epoch 387/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1691760151626.1890 - val_loss: 1770843583679.5500\n",
      "Epoch 388/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1690820216428.1621 - val_loss: 1767046818659.0154\n",
      "Epoch 389/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1689332239849.0710 - val_loss: 1767706444121.6541\n",
      "Epoch 390/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1688987904301.0776 - val_loss: 1766905420913.7778\n",
      "Epoch 391/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1690218183540.0254 - val_loss: 1768542307963.1392\n",
      "Epoch 392/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1689759105519.5535 - val_loss: 1767055110115.1956\n",
      "Epoch 393/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1691389063745.1855 - val_loss: 1768440217254.3459\n",
      "Epoch 394/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1688946745941.3533 - val_loss: 1772071304685.9971\n",
      "Epoch 395/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1689611753731.7815 - val_loss: 1773384449610.1716\n",
      "Epoch 396/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1691402683339.8997 - val_loss: 1766627752716.6021\n",
      "Epoch 397/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1689577690930.9600 - val_loss: 1766506739709.1196\n",
      "Epoch 398/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1688842538968.6248 - val_loss: 1769619318291.4431\n",
      "Epoch 399/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 1689441391201.8384 - val_loss: 1768187610355.3979\n",
      "Epoch 400/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1688457072719.7109 - val_loss: 1766981314551.3586\n",
      "Epoch 401/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1690078634224.0938 - val_loss: 1768279566673.0127\n",
      "Epoch 402/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1687994139907.0613 - val_loss: 1777138527374.5823\n",
      "Epoch 403/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1691404651202.3560 - val_loss: 1767934495022.4473\n",
      "Epoch 404/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1689790396293.5522 - val_loss: 1766856506355.0381\n",
      "Epoch 405/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1691413137906.9148 - val_loss: 1773339240831.0999\n",
      "Epoch 406/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1689785111633.3918 - val_loss: 1766937012031.0098\n",
      "Epoch 407/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1690535989116.4287 - val_loss: 1767389095950.4023\n",
      "Epoch 408/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1691554978585.7500 - val_loss: 1766697681195.5669\n",
      "Epoch 409/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1688980349314.0708 - val_loss: 1768004517519.3025\n",
      "Epoch 410/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1692577307456.1650 - val_loss: 1767006690532.9958\n",
      "Epoch 411/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1689609276517.0796 - val_loss: 1767073504254.5598\n",
      "Epoch 412/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1690283815808.0300 - val_loss: 1769986818554.9592\n",
      "Epoch 413/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1689626335268.9746 - val_loss: 1767166505645.5471\n",
      "Epoch 414/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1691065235448.7971 - val_loss: 1769126096198.9312\n",
      "Epoch 415/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 1692379646516.2205 - val_loss: 1767371398280.8213\n",
      "Epoch 416/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1690555326692.5693 - val_loss: 1768003039594.9368\n",
      "Epoch 417/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1686818635288.1294 - val_loss: 1776262120990.9648\n",
      "Epoch 418/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1686710858239.8799 - val_loss: 1773006414918.5710\n",
      "Epoch 419/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 1688965169747.4326 - val_loss: 1767042781235.8481\n",
      "Epoch 420/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 1688653950915.4963 - val_loss: 1776408913703.9663\n",
      "Epoch 421/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 1688746006576.4988 - val_loss: 1768482152182.9985\n",
      "Epoch 422/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1689589067365.9197 - val_loss: 1766685272861.8848\n",
      "Epoch 423/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 1687892047880.1631 - val_loss: 1772942414912.8101\n",
      "Epoch 424/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 1693391171627.2168 - val_loss: 1767567939177.8564\n",
      "Epoch 425/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 1688758742380.2222 - val_loss: 1767292069118.9199\n",
      "Epoch 426/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 1689906188740.3367 - val_loss: 1768067739573.1084\n",
      "Epoch 427/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 1688830833578.2866 - val_loss: 1768474344089.3840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f7f290e48>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_train2 = model2.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4265 samples, validate on 2844 samples\n",
      "Epoch 1/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 940176583863.6718 - val_loss: 999523096884.2081\n",
      "Epoch 2/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 940492130484.0703 - val_loss: 997121777744.6526\n",
      "Epoch 3/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 940683023437.5503 - val_loss: 1002085967149.0072\n",
      "Epoch 4/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 939015803701.1207 - val_loss: 998926165131.7018\n",
      "Epoch 5/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 939927403022.7657 - val_loss: 996174095476.6582\n",
      "Epoch 6/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 939063200780.7250 - val_loss: 995663777982.1099\n",
      "Epoch 7/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 939367777443.0237 - val_loss: 995348567427.4205\n",
      "Epoch 8/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 939202737465.8025 - val_loss: 995206175212.5571\n",
      "Epoch 9/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 938361243332.5167 - val_loss: 996831920681.0464\n",
      "Epoch 10/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 938667766513.8945 - val_loss: 994635291946.1265\n",
      "Epoch 11/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 938718281970.0145 - val_loss: 998688953846.6384\n",
      "Epoch 12/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 938270697565.6365 - val_loss: 994152333749.8284\n",
      "Epoch 13/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 937381725143.4242 - val_loss: 993818316944.0226\n",
      "Epoch 14/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 938014295948.2748 - val_loss: 993561270833.6877\n",
      "Epoch 15/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 935517690442.3091 - val_loss: 993494129985.1702\n",
      "Epoch 16/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 937085605959.5480 - val_loss: 992968793040.4724\n",
      "Epoch 17/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 937144482549.2559 - val_loss: 994651665253.8959\n",
      "Epoch 18/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 936017649297.1366 - val_loss: 992627006577.7778\n",
      "Epoch 19/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 935509987849.4838 - val_loss: 1000703292768.8550\n",
      "Epoch 20/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 936672190412.1398 - val_loss: 994605903151.8873\n",
      "Epoch 21/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 935028318286.7509 - val_loss: 993859605914.4642\n",
      "Epoch 22/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 935918143798.6813 - val_loss: 996266300296.4614\n",
      "Epoch 23/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 935491871409.5493 - val_loss: 992887744180.7483\n",
      "Epoch 24/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 935338221020.5862 - val_loss: 990781556047.5725\n",
      "Epoch 25/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 934525650958.8859 - val_loss: 992608041971.0380\n",
      "Epoch 26/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 934693723939.1138 - val_loss: 991910968260.9507\n",
      "Epoch 27/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 934999109529.2399 - val_loss: 990335584950.1884\n",
      "Epoch 28/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 933478414121.3561 - val_loss: 996428587895.1786\n",
      "Epoch 29/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 934148366045.9667 - val_loss: 990613756419.6007\n",
      "Epoch 30/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 933564370877.7341 - val_loss: 989848631823.1224\n",
      "Epoch 31/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 933378687782.7151 - val_loss: 988956565838.1322\n",
      "Epoch 32/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 931440133594.1852 - val_loss: 994753735592.1462\n",
      "Epoch 33/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 933747137044.5281 - val_loss: 989054110874.1041\n",
      "Epoch 34/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 932221246614.0586 - val_loss: 988168000738.1155\n",
      "Epoch 35/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 933166547166.8071 - val_loss: 987831411252.5681\n",
      "Epoch 36/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 932441260331.6370 - val_loss: 988541795315.0380\n",
      "Epoch 37/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 931230295896.4144 - val_loss: 1000238147460.1406\n",
      "Epoch 38/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 932223075483.3407 - val_loss: 987146072542.1548\n",
      "Epoch 39/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 931606606881.3730 - val_loss: 989791629052.7595\n",
      "Epoch 40/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 931603271229.5841 - val_loss: 986683854231.5837\n",
      "Epoch 41/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 930631135974.3700 - val_loss: 986466106585.4740\n",
      "Epoch 42/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 930672806290.8774 - val_loss: 991091556997.2209\n",
      "Epoch 43/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 931618151335.8856 - val_loss: 986052755824.6978\n",
      "Epoch 44/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 929519798382.9233 - val_loss: 989730261547.9269\n",
      "Epoch 45/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 929872661278.5520 - val_loss: 989927355214.8522\n",
      "Epoch 46/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 930397127370.5190 - val_loss: 986304466511.9325\n",
      "Epoch 47/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 930239888174.8783 - val_loss: 985028763659.5220\n",
      "Epoch 48/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 930222336942.6082 - val_loss: 984985008286.4249\n",
      "Epoch 49/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 930264955360.6677 - val_loss: 984620844920.6189\n",
      "Epoch 50/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 929375719223.2816 - val_loss: 984400156911.0775\n",
      "Epoch 51/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 929289466056.7184 - val_loss: 984188658412.9171\n",
      "Epoch 52/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 930112982804.7081 - val_loss: 983875866053.6710\n",
      "Epoch 53/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 929267974574.2480 - val_loss: 983647954520.5737\n",
      "Epoch 54/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 928132448328.9885 - val_loss: 983392693367.5386\n",
      "Epoch 55/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 928888652913.0842 - val_loss: 983275131646.1997\n",
      "Epoch 56/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 928286012053.6985 - val_loss: 984572213705.9916\n",
      "Epoch 57/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 927566899631.6886 - val_loss: 985783216488.0562\n",
      "Epoch 58/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 927685249849.4424 - val_loss: 984293065372.2643\n",
      "Epoch 59/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 926614214854.3175 - val_loss: 984664042822.9310\n",
      "Epoch 60/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 926238763343.6511 - val_loss: 984091150799.7524\n",
      "Epoch 61/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 926658579461.0420 - val_loss: 984989501327.6626\n",
      "Epoch 62/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 926652863400.3658 - val_loss: 993958413002.3516\n",
      "Epoch 63/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 927540902537.6937 - val_loss: 981604256299.9269\n",
      "Epoch 64/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 925986588437.1882 - val_loss: 981220371206.8412\n",
      "Epoch 65/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 56us/step - loss: 926260036190.9570 - val_loss: 982085539779.5104\n",
      "Epoch 66/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 927026168195.9916 - val_loss: 981330273419.7018\n",
      "Epoch 67/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 925228504160.7578 - val_loss: 980316662580.9282\n",
      "Epoch 68/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 925671366285.2952 - val_loss: 981827662440.4163\n",
      "Epoch 69/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 925430238474.2640 - val_loss: 980913740614.2109\n",
      "Epoch 70/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 924660981700.4567 - val_loss: 980303303191.7637\n",
      "Epoch 71/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 925492748239.9813 - val_loss: 986387457394.1378\n",
      "Epoch 72/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 925137544157.1864 - val_loss: 979308339142.3911\n",
      "Epoch 73/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 924451382759.6305 - val_loss: 979169692618.7115\n",
      "Epoch 74/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 924826567927.5367 - val_loss: 979017226479.0775\n",
      "Epoch 75/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 923233084792.2269 - val_loss: 979649128370.2279\n",
      "Epoch 76/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 923945499450.4028 - val_loss: 982542634391.5837\n",
      "Epoch 77/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 923702717980.9313 - val_loss: 983107026183.5612\n",
      "Epoch 78/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 924061338794.2264 - val_loss: 977851553501.0747\n",
      "Epoch 79/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 923634007802.7780 - val_loss: 978019729938.0028\n",
      "Epoch 80/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 922661682739.0199 - val_loss: 977473920505.5190\n",
      "Epoch 81/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 921567472800.6227 - val_loss: 980994823025.4177\n",
      "Epoch 82/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 923322531044.8094 - val_loss: 977697857939.2631\n",
      "Epoch 83/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 923154558281.6488 - val_loss: 977175351209.5865\n",
      "Epoch 84/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 921738782168.2645 - val_loss: 985104242512.2925\n",
      "Epoch 85/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 922476053978.1852 - val_loss: 977404965296.0675\n",
      "Epoch 86/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 922007732200.9509 - val_loss: 976163312958.2897\n",
      "Epoch 87/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 922231265923.9315 - val_loss: 976513065225.0015\n",
      "Epoch 88/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 921332225927.4730 - val_loss: 976386763434.6666\n",
      "Epoch 89/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 921181933855.1522 - val_loss: 975759577831.1561\n",
      "Epoch 90/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 921517985606.6476 - val_loss: 975540850739.8481\n",
      "Epoch 91/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 921375614693.1696 - val_loss: 975324302651.4093\n",
      "Epoch 92/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 920067470088.9435 - val_loss: 974956861102.9873\n",
      "Epoch 93/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 920295747058.6748 - val_loss: 974737317438.6498\n",
      "Epoch 94/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 920763837347.0837 - val_loss: 974372624623.0774\n",
      "Epoch 95/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 920710091004.5786 - val_loss: 974640879048.5514\n",
      "Epoch 96/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 919607875993.6000 - val_loss: 984226998816.4049\n",
      "Epoch 97/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 920137258607.0435 - val_loss: 976319409386.7567\n",
      "Epoch 98/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 920159954561.2905 - val_loss: 973484302485.7834\n",
      "Epoch 99/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 919893784361.3561 - val_loss: 974073043160.0338\n",
      "Epoch 100/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 919629160073.4536 - val_loss: 974491943833.7440\n",
      "Epoch 101/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 919052168698.3578 - val_loss: 975105558507.8368\n",
      "Epoch 102/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 916159157087.3773 - val_loss: 972803368329.1814\n",
      "Epoch 103/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 918477112148.8132 - val_loss: 974154780009.4965\n",
      "Epoch 104/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 919229020029.8693 - val_loss: 972202904545.7552\n",
      "Epoch 105/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 918194552931.8790 - val_loss: 972224814954.2166\n",
      "Epoch 106/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 917929992666.4254 - val_loss: 973359266249.9916\n",
      "Epoch 107/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 917989994800.4390 - val_loss: 972078234334.5148\n",
      "Epoch 108/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 918066437266.2172 - val_loss: 971589545379.1055\n",
      "Epoch 109/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 916668642771.4625 - val_loss: 978403398882.1155\n",
      "Epoch 110/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 916262161332.3705 - val_loss: 971889138916.9957\n",
      "Epoch 111/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 917106575897.3300 - val_loss: 978554913966.2671\n",
      "Epoch 112/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 917073504818.2996 - val_loss: 971972205750.9086\n",
      "Epoch 113/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 917317784626.6598 - val_loss: 971611873890.6554\n",
      "Epoch 114/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 916767029840.0713 - val_loss: 974660653223.0662\n",
      "Epoch 115/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 916286373619.0950 - val_loss: 971611537177.5640\n",
      "Epoch 116/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 917041953283.2412 - val_loss: 970848145552.0225\n",
      "Epoch 117/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 915809154749.7941 - val_loss: 970237825098.8917\n",
      "Epoch 118/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 916772752815.4485 - val_loss: 971742374685.8846\n",
      "Epoch 119/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 916313894298.3203 - val_loss: 969461389136.2926\n",
      "Epoch 120/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 914936949087.0171 - val_loss: 969112052576.1350\n",
      "Epoch 121/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 914753749614.3231 - val_loss: 969463795256.8889\n",
      "Epoch 122/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 915841743851.1118 - val_loss: 968782978253.9521\n",
      "Epoch 123/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 914702895722.0015 - val_loss: 969889174592.8102\n",
      "Epoch 124/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 914317955543.5442 - val_loss: 972395589971.8931\n",
      "Epoch 125/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 913389728901.2521 - val_loss: 971926611314.1378\n",
      "Epoch 126/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 914344821431.3116 - val_loss: 968648030689.0352\n",
      "Epoch 127/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 914633044118.7789 - val_loss: 971723805138.6329\n",
      "Epoch 128/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 914436120885.2407 - val_loss: 967519727425.8903\n",
      "Epoch 129/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 67us/step - loss: 914143096829.1189 - val_loss: 968198584708.8607\n",
      "Epoch 130/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 913544422810.8004 - val_loss: 968716089225.9015\n",
      "Epoch 131/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 913424021239.4167 - val_loss: 967062223743.8199\n",
      "Epoch 132/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 913712646331.7534 - val_loss: 966672317265.7328\n",
      "Epoch 133/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 914364593988.2466 - val_loss: 966558208397.5021\n",
      "Epoch 134/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 912884032227.4889 - val_loss: 966251664540.9845\n",
      "Epoch 135/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 912783133462.8690 - val_loss: 970082995331.0604\n",
      "Epoch 136/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 913672567687.2328 - val_loss: 966737102873.9241\n",
      "Epoch 137/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 912710508019.6351 - val_loss: 966840842674.9480\n",
      "Epoch 138/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 912436661441.2755 - val_loss: 965680409350.8411\n",
      "Epoch 139/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 912886866363.6932 - val_loss: 966494958691.3755\n",
      "Epoch 140/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 912497061124.5017 - val_loss: 965540707983.3024\n",
      "Epoch 141/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 911695177687.1840 - val_loss: 965811567626.0815\n",
      "Epoch 142/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 912359847410.1947 - val_loss: 964835328253.4796\n",
      "Epoch 143/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 911935045470.6570 - val_loss: 965508892144.8777\n",
      "Epoch 144/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 911900217719.5067 - val_loss: 966851097169.3727\n",
      "Epoch 145/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 911159814010.9880 - val_loss: 964568878510.6273\n",
      "Epoch 146/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 911760267120.4240 - val_loss: 965232523883.2968\n",
      "Epoch 147/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 911399059870.8821 - val_loss: 965976202998.9985\n",
      "Epoch 148/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 911028359428.2617 - val_loss: 965655018344.7764\n",
      "Epoch 149/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 911255171457.1105 - val_loss: 964412903014.9761\n",
      "Epoch 150/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 911098687254.6288 - val_loss: 966136091574.5486\n",
      "Epoch 151/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 911092079335.8105 - val_loss: 963299790001.1477\n",
      "Epoch 152/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 910900552519.3679 - val_loss: 964186293721.8340\n",
      "Epoch 153/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 910303977097.4536 - val_loss: 963887103078.2560\n",
      "Epoch 154/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 909615910653.6591 - val_loss: 962756631873.1702\n",
      "Epoch 155/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 909585010391.2440 - val_loss: 962987875603.0830\n",
      "Epoch 156/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 907919396355.9615 - val_loss: 963816880194.2504\n",
      "Epoch 157/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 908034132973.9929 - val_loss: 972558519841.8453\n",
      "Epoch 158/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 910049851692.1172 - val_loss: 962901484130.6554\n",
      "Epoch 159/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 909361440850.1121 - val_loss: 962041820858.5092\n",
      "Epoch 160/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 909157940575.7373 - val_loss: 964395780974.5372\n",
      "Epoch 161/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 909497768330.2340 - val_loss: 962039431513.6541\n",
      "Epoch 162/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 907927739956.9407 - val_loss: 962641551394.5654\n",
      "Epoch 163/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 908320033274.5979 - val_loss: 961265092979.5780\n",
      "Epoch 164/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 908849886932.8431 - val_loss: 960988626828.7820\n",
      "Epoch 165/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 908114602300.4436 - val_loss: 963655803029.7834\n",
      "Epoch 166/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 907487181872.2589 - val_loss: 962153310406.7511\n",
      "Epoch 167/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 907566595154.8324 - val_loss: 961896628846.1772\n",
      "Epoch 168/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 908202857426.6222 - val_loss: 960632755748.7257\n",
      "Epoch 169/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 907910415111.9832 - val_loss: 960649670212.4106\n",
      "Epoch 170/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 907508289304.5496 - val_loss: 961682114401.5752\n",
      "Epoch 171/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 907257828076.1322 - val_loss: 960278280742.1660\n",
      "Epoch 172/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 905803501567.0397 - val_loss: 966789168506.7792\n",
      "Epoch 173/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 907322715093.0232 - val_loss: 960574446129.6877\n",
      "Epoch 174/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 905520503197.4415 - val_loss: 959544130708.3431\n",
      "Epoch 175/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 907110040278.0436 - val_loss: 960585582364.4445\n",
      "Epoch 176/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 905637899002.0576 - val_loss: 959756467532.6920\n",
      "Epoch 177/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 906705188111.5461 - val_loss: 958884544972.8721\n",
      "Epoch 178/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 906313804430.4957 - val_loss: 959187148281.5190\n",
      "Epoch 179/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 906221121498.5454 - val_loss: 961558238733.6821\n",
      "Epoch 180/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 906621491494.3550 - val_loss: 958579021302.6385\n",
      "Epoch 181/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 904928094044.2561 - val_loss: 963203337600.5400\n",
      "Epoch 182/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 905850261095.8406 - val_loss: 959332698114.8805\n",
      "Epoch 183/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 905996033442.9636 - val_loss: 958168654872.4838\n",
      "Epoch 184/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 905096543649.0430 - val_loss: 958391963768.9789\n",
      "Epoch 185/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 905794549994.8118 - val_loss: 957736178291.9381\n",
      "Epoch 186/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 905399106907.8959 - val_loss: 962074374184.3263\n",
      "Epoch 187/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 905024835160.4746 - val_loss: 958845145762.0253\n",
      "Epoch 188/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 904327642110.3193 - val_loss: 957277056764.7595\n",
      "Epoch 189/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 905281983181.4003 - val_loss: 957155045231.9775\n",
      "Epoch 190/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 903962813040.2438 - val_loss: 957154029470.0647\n",
      "Epoch 191/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 905086874688.1051 - val_loss: 956980637252.4106\n",
      "Epoch 192/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 905297935873.0803 - val_loss: 956589362629.6709\n",
      "Epoch 193/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 57us/step - loss: 903562940649.3712 - val_loss: 957163979328.0900\n",
      "Epoch 194/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 904007917658.2753 - val_loss: 956140753500.8945\n",
      "Epoch 195/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 902811776106.3616 - val_loss: 958877482385.8228\n",
      "Epoch 196/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 904220817907.8752 - val_loss: 955749906936.0787\n",
      "Epoch 197/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 903187718868.1228 - val_loss: 956660034587.3643\n",
      "Epoch 198/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 903071155493.6346 - val_loss: 964109361739.6118\n",
      "Epoch 199/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 904635261225.7163 - val_loss: 956380732414.5598\n",
      "Epoch 200/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 904068970089.2811 - val_loss: 956212487307.7018\n",
      "Epoch 201/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 903800425557.7135 - val_loss: 955378829568.3601\n",
      "Epoch 202/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 903657529964.4023 - val_loss: 954928013046.9985\n",
      "Epoch 203/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 903083435357.3364 - val_loss: 956043514115.2405\n",
      "Epoch 204/800\n",
      "4265/4265 [==============================] - ETA: 0s - loss: 901916368896.00 - 0s 59us/step - loss: 903301798543.4561 - val_loss: 954595720732.0844\n",
      "Epoch 205/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 902203875825.4744 - val_loss: 954390310537.5415\n",
      "Epoch 206/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 902140330352.7841 - val_loss: 954245723465.8115\n",
      "Epoch 207/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 902305400554.9318 - val_loss: 955390756014.2672\n",
      "Epoch 208/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 902683984924.5712 - val_loss: 955283469434.4191\n",
      "Epoch 209/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 902404891993.4950 - val_loss: 954145276964.0056\n",
      "Epoch 210/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 901659121839.2684 - val_loss: 953685380157.9297\n",
      "Epoch 211/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 901710532142.2180 - val_loss: 953751358134.1885\n",
      "Epoch 212/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 900630599448.0693 - val_loss: 953356616740.0056\n",
      "Epoch 213/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 902333279753.7238 - val_loss: 953308876114.4529\n",
      "Epoch 214/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 901835209780.1003 - val_loss: 953237163046.8861\n",
      "Epoch 215/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 901782805949.8541 - val_loss: 955327603275.6118\n",
      "Epoch 216/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 901426638349.5653 - val_loss: 953301118993.2827\n",
      "Epoch 217/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 901254986212.5093 - val_loss: 953326262787.6006\n",
      "Epoch 218/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 901295074391.6342 - val_loss: 952422705880.7539\n",
      "Epoch 219/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 900798076734.4845 - val_loss: 954822743150.8973\n",
      "Epoch 220/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 901036102487.6942 - val_loss: 952686057641.9465\n",
      "Epoch 221/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 900505312079.5311 - val_loss: 953993144587.8818\n",
      "Epoch 222/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 899730811910.9626 - val_loss: 959560194586.6442\n",
      "Epoch 223/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 901144212901.3646 - val_loss: 952189910579.1279\n",
      "Epoch 224/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 899534912836.8469 - val_loss: 959398277436.8495\n",
      "Epoch 225/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 899780242542.6832 - val_loss: 952370773624.2588\n",
      "Epoch 226/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 900231750440.6359 - val_loss: 952848418795.8368\n",
      "Epoch 227/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 899202159720.4408 - val_loss: 951517448817.0576\n",
      "Epoch 228/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 899826414967.2666 - val_loss: 951287561525.6484\n",
      "Epoch 229/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 898609107907.2563 - val_loss: 955546431574.4135\n",
      "Epoch 230/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 898960600770.1158 - val_loss: 955116998869.1533\n",
      "Epoch 231/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 899438949063.6379 - val_loss: 951233776932.3657\n",
      "Epoch 232/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 899379478324.4004 - val_loss: 951689589951.5499\n",
      "Epoch 233/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 899733127260.1960 - val_loss: 951570148701.9747\n",
      "Epoch 234/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 897507391984.0338 - val_loss: 953063788895.4149\n",
      "Epoch 235/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 899002346648.9397 - val_loss: 950079309596.4445\n",
      "Epoch 236/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 898645602577.7069 - val_loss: 950513976148.6133\n",
      "Epoch 237/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 899276532132.1642 - val_loss: 950323681726.4697\n",
      "Epoch 238/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 898153209527.5518 - val_loss: 952150314760.2812\n",
      "Epoch 239/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 898260240587.1194 - val_loss: 949845386172.3094\n",
      "Epoch 240/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 897556305303.9194 - val_loss: 953457969039.6625\n",
      "Epoch 241/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 898163511079.9155 - val_loss: 950626753741.9521\n",
      "Epoch 242/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 898043904637.6891 - val_loss: 949263192922.3741\n",
      "Epoch 243/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 897820783107.7214 - val_loss: 949254846161.5527\n",
      "Epoch 244/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 898028705218.1758 - val_loss: 948924928535.7637\n",
      "Epoch 245/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 897211816802.7385 - val_loss: 948871142375.5162\n",
      "Epoch 246/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 897223066406.7151 - val_loss: 950098321857.3502\n",
      "Epoch 247/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 895202583536.3939 - val_loss: 948791764361.1814\n",
      "Epoch 248/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 896357181172.5355 - val_loss: 948314585499.9044\n",
      "Epoch 249/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 896400239738.2078 - val_loss: 954912176378.5991\n",
      "Epoch 250/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 897284451096.7897 - val_loss: 948968011312.2476\n",
      "Epoch 251/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 896682223091.1550 - val_loss: 948003109667.6456\n",
      "Epoch 252/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 896872052179.4625 - val_loss: 948586930441.0015\n",
      "Epoch 253/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 896052651176.7859 - val_loss: 949951299630.0872\n",
      "Epoch 254/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 896974121126.6250 - val_loss: 948293350453.2883\n",
      "Epoch 255/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 895315574299.9708 - val_loss: 947526145757.0746\n",
      "Epoch 256/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 896143652468.8057 - val_loss: 947920660033.5303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 895627084140.9425 - val_loss: 952436119560.6414\n",
      "Epoch 258/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 896374320803.3838 - val_loss: 948336673897.1365\n",
      "Epoch 259/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 894832849784.3469 - val_loss: 949811703710.0647\n",
      "Epoch 260/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 895390272432.5291 - val_loss: 947495972927.3699\n",
      "Epoch 261/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 895055534612.5281 - val_loss: 947807836352.9901\n",
      "Epoch 262/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 895682912123.4681 - val_loss: 946589354171.2292\n",
      "Epoch 263/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 895813511342.3081 - val_loss: 947565889792.3601\n",
      "Epoch 264/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 894966571505.4744 - val_loss: 946312791698.1829\n",
      "Epoch 265/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 895313360335.6211 - val_loss: 946083023035.2292\n",
      "Epoch 266/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 895105040768.1501 - val_loss: 946729688961.2603\n",
      "Epoch 267/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 894735407555.8564 - val_loss: 949817814504.2363\n",
      "Epoch 268/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 895125031030.6063 - val_loss: 946862727133.4346\n",
      "Epoch 269/800\n",
      "4265/4265 [==============================] - ETA: 0s - loss: 886974534354.82 - 0s 72us/step - loss: 895367252947.1025 - val_loss: 945601950266.3291\n",
      "Epoch 270/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 894436268522.2715 - val_loss: 947427317837.7721\n",
      "Epoch 271/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 894953136464.1312 - val_loss: 945666928058.1490\n",
      "Epoch 272/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 894262754947.6914 - val_loss: 945268208508.9396\n",
      "Epoch 273/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 894577242374.1824 - val_loss: 945384837333.1533\n",
      "Epoch 274/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 894679378586.0201 - val_loss: 944933556344.9789\n",
      "Epoch 275/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 893472403848.7935 - val_loss: 945570818177.6202\n",
      "Epoch 276/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 894040238285.0402 - val_loss: 944716428756.0731\n",
      "Epoch 277/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 892788492541.0588 - val_loss: 944891134919.8312\n",
      "Epoch 278/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 893582643321.9677 - val_loss: 948968168930.4753\n",
      "Epoch 279/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 893505897308.4961 - val_loss: 949932384060.1294\n",
      "Epoch 280/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 892966645695.6549 - val_loss: 945776018306.7004\n",
      "Epoch 281/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 893531334327.0715 - val_loss: 945763981988.9058\n",
      "Epoch 282/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 893244669030.0399 - val_loss: 944387176352.9452\n",
      "Epoch 283/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 893255529843.4252 - val_loss: 944186594437.9409\n",
      "Epoch 284/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 892257230327.4767 - val_loss: 944010310709.2883\n",
      "Epoch 285/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 892265360848.1013 - val_loss: 944363024447.3699\n",
      "Epoch 286/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 892080486911.3998 - val_loss: 943655353907.1279\n",
      "Epoch 287/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 892045573256.6134 - val_loss: 943411942879.5950\n",
      "Epoch 288/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 893191357668.3292 - val_loss: 947552966664.6414\n",
      "Epoch 289/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 891931486612.7981 - val_loss: 944719139881.7665\n",
      "Epoch 290/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 891855141060.3967 - val_loss: 944980616999.9662\n",
      "Epoch 291/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 892523351809.5006 - val_loss: 943505214525.9297\n",
      "Epoch 292/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 890679952566.2312 - val_loss: 944287380878.9424\n",
      "Epoch 293/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 891917402242.6110 - val_loss: 942626765096.6864\n",
      "Epoch 294/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 891560910404.3068 - val_loss: 942478757525.0632\n",
      "Epoch 295/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 891170448391.4429 - val_loss: 942386471223.0886\n",
      "Epoch 296/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 891213984716.8600 - val_loss: 942161853173.5583\n",
      "Epoch 297/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 891362140008.2607 - val_loss: 943275780548.2307\n",
      "Epoch 298/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 890591623713.9733 - val_loss: 945272692701.4346\n",
      "Epoch 299/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 890830520658.0520 - val_loss: 941722387929.8340\n",
      "Epoch 300/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 891566259432.6509 - val_loss: 941649676336.9677\n",
      "Epoch 301/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 890872959021.1376 - val_loss: 941576111116.9620\n",
      "Epoch 302/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 890287510428.8413 - val_loss: 942757585921.4402\n",
      "Epoch 303/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 891001989325.0402 - val_loss: 944938089205.5583\n",
      "Epoch 304/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 890520992293.8148 - val_loss: 942788511170.7904\n",
      "Epoch 305/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 890223162466.6785 - val_loss: 941174795383.5387\n",
      "Epoch 306/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 890146565603.3088 - val_loss: 941509584334.3123\n",
      "Epoch 307/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 890453105349.2371 - val_loss: 940858179814.4360\n",
      "Epoch 308/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 889341053807.2234 - val_loss: 940724557440.9001\n",
      "Epoch 309/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 889598714407.2554 - val_loss: 946112638947.1956\n",
      "Epoch 310/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 890595116928.7504 - val_loss: 940423855137.1251\n",
      "Epoch 311/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 889902940786.1646 - val_loss: 944357835489.3953\n",
      "Epoch 312/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 890127656462.0455 - val_loss: 940323816414.8749\n",
      "Epoch 313/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 890278832555.8470 - val_loss: 940287545879.7637\n",
      "Epoch 314/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 890299531006.6194 - val_loss: 940226400515.2405\n",
      "Epoch 315/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 889319054701.1826 - val_loss: 942492344400.6526\n",
      "Epoch 316/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 889208783597.0927 - val_loss: 939887466205.0746\n",
      "Epoch 317/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 888299231011.5940 - val_loss: 942481487546.5092\n",
      "Epoch 318/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 888925845738.8118 - val_loss: 940482557419.1167\n",
      "Epoch 319/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 887906702240.6827 - val_loss: 945282095873.0802\n",
      "Epoch 320/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 889320709051.0931 - val_loss: 939798883286.2335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 889193188702.7770 - val_loss: 939697008542.0647\n",
      "Epoch 322/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 888568936344.7598 - val_loss: 943046702732.4220\n",
      "Epoch 323/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 888885738198.5238 - val_loss: 942499079399.8762\n",
      "Epoch 324/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 888052243421.6666 - val_loss: 939097819346.2728\n",
      "Epoch 325/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 888255442991.0583 - val_loss: 944202604209.8678\n",
      "Epoch 326/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 887485525678.9083 - val_loss: 938947738864.5176\n",
      "Epoch 327/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 887556949961.7388 - val_loss: 946555170925.4572\n",
      "Epoch 328/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 888159906915.1587 - val_loss: 939366176361.8566\n",
      "Epoch 329/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 888022903023.6136 - val_loss: 940748732297.9015\n",
      "Epoch 330/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 888334219122.3446 - val_loss: 939534427920.9226\n",
      "Epoch 331/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 888023994532.2241 - val_loss: 942517894131.0380\n",
      "Epoch 332/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 887656038734.2106 - val_loss: 938697060991.4600\n",
      "Epoch 333/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 888020556419.9315 - val_loss: 938026573236.3882\n",
      "Epoch 334/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 887493232343.7242 - val_loss: 937681642428.3094\n",
      "Epoch 335/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 887619876276.7306 - val_loss: 937576272799.5049\n",
      "Epoch 336/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 887047733563.0031 - val_loss: 937605995357.2545\n",
      "Epoch 337/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 887547679898.3804 - val_loss: 941137039181.4121\n",
      "Epoch 338/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 887412656285.0212 - val_loss: 937995224353.4852\n",
      "Epoch 339/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 886570867016.9285 - val_loss: 939599654767.9775\n",
      "Epoch 340/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 885907044890.2903 - val_loss: 945125585397.1984\n",
      "Epoch 341/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 886524599845.5746 - val_loss: 937841506577.6427\n",
      "Epoch 342/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 886116999300.5319 - val_loss: 939141977613.6821\n",
      "Epoch 343/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 886428027962.8230 - val_loss: 938325584877.2771\n",
      "Epoch 344/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 884486097871.2610 - val_loss: 942018589818.4191\n",
      "Epoch 345/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 886914386645.5634 - val_loss: 938275697417.7216\n",
      "Epoch 346/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 886932535062.1487 - val_loss: 937288664146.0928\n",
      "Epoch 347/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 885141803352.7747 - val_loss: 940736517346.1154\n",
      "Epoch 348/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 886646032583.2778 - val_loss: 936895992251.5894\n",
      "Epoch 349/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 885198329236.0778 - val_loss: 938092941847.7637\n",
      "Epoch 350/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 886343256847.1859 - val_loss: 937318792086.8635\n",
      "Epoch 351/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 885815326874.1403 - val_loss: 937707725479.7863\n",
      "Epoch 352/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 886002204337.7894 - val_loss: 936452156650.7567\n",
      "Epoch 353/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 885657708353.3656 - val_loss: 938485758916.9508\n",
      "Epoch 354/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 884790310320.8890 - val_loss: 935907488982.5935\n",
      "Epoch 355/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 886082685572.8918 - val_loss: 935735603632.0675\n",
      "Epoch 356/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 884808942528.3751 - val_loss: 935428207991.8987\n",
      "Epoch 357/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 885986883004.4136 - val_loss: 936292279228.3094\n",
      "Epoch 358/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 885161302087.5480 - val_loss: 935951721450.3966\n",
      "Epoch 359/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 884339053319.0227 - val_loss: 935164066480.4276\n",
      "Epoch 360/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 884533733193.0486 - val_loss: 935708896652.0619\n",
      "Epoch 361/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 885081583451.7759 - val_loss: 935054495542.3685\n",
      "Epoch 362/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 885011649133.3628 - val_loss: 935260861861.9860\n",
      "Epoch 363/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 883994553708.2223 - val_loss: 942071688541.9747\n",
      "Epoch 364/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 885240800928.0225 - val_loss: 934612949000.6414\n",
      "Epoch 365/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 884789840052.0703 - val_loss: 936247968504.4388\n",
      "Epoch 366/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 884302177371.9559 - val_loss: 935793195673.3839\n",
      "Epoch 367/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 883742964661.3308 - val_loss: 934490574116.3657\n",
      "Epoch 368/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 883748508284.7288 - val_loss: 934631674599.1561\n",
      "Epoch 369/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 884339123787.5095 - val_loss: 938560169376.2250\n",
      "Epoch 370/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 883590536069.3121 - val_loss: 934423817819.4543\n",
      "Epoch 371/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 882665814949.9648 - val_loss: 951811611439.1674\n",
      "Epoch 372/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 884350963415.2440 - val_loss: 934339137933.5021\n",
      "Epoch 373/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 883064158354.4572 - val_loss: 933640379828.3882\n",
      "Epoch 374/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 882353428993.3206 - val_loss: 938233491865.0239\n",
      "Epoch 375/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 883794109355.2468 - val_loss: 935670971586.4304\n",
      "Epoch 376/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 882621408739.7889 - val_loss: 936836019990.6836\n",
      "Epoch 377/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 883684279339.9371 - val_loss: 933477927329.6653\n",
      "Epoch 378/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 882667152890.8380 - val_loss: 934783437908.9733\n",
      "Epoch 379/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 883718931311.7036 - val_loss: 933512089651.8481\n",
      "Epoch 380/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 882428348304.1163 - val_loss: 934893011151.3925\n",
      "Epoch 381/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 882105427861.8785 - val_loss: 933209485052.7595\n",
      "Epoch 382/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 882341077631.6099 - val_loss: 932867534796.1519\n",
      "Epoch 383/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 882548464590.3005 - val_loss: 933534273449.5865\n",
      "Epoch 384/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 881533240667.4156 - val_loss: 938730846965.5583\n",
      "Epoch 385/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 57us/step - loss: 882941930211.0087 - val_loss: 932770095852.9170\n",
      "Epoch 386/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 882115989591.3942 - val_loss: 932504335758.9424\n",
      "Epoch 387/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 882659958766.7133 - val_loss: 935093212570.4641\n",
      "Epoch 388/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 882826186167.1315 - val_loss: 933065979002.4191\n",
      "Epoch 389/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 882641100048.9867 - val_loss: 932227813750.4585\n",
      "Epoch 390/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 881533180448.0525 - val_loss: 932821320025.6541\n",
      "Epoch 391/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 881762344581.6122 - val_loss: 934684652758.5935\n",
      "Epoch 392/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 881118428504.0544 - val_loss: 932094646349.7721\n",
      "Epoch 393/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 881530080234.1515 - val_loss: 933268299326.6498\n",
      "Epoch 394/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 881695341051.5582 - val_loss: 931837561618.3629\n",
      "Epoch 395/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 880782075932.3311 - val_loss: 931444552273.3727\n",
      "Epoch 396/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 880238180298.6992 - val_loss: 932778916851.0380\n",
      "Epoch 397/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 881339411501.3778 - val_loss: 931386656701.7496\n",
      "Epoch 398/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 881008712788.7532 - val_loss: 933376273435.3643\n",
      "Epoch 399/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 880626798575.6736 - val_loss: 935907863948.0619\n",
      "Epoch 400/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 881673165159.6605 - val_loss: 932030230268.7595\n",
      "Epoch 401/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 880238090146.3635 - val_loss: 932189428826.7341\n",
      "Epoch 402/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 881169654637.7828 - val_loss: 930885191544.6189\n",
      "Epoch 403/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 880292759970.2433 - val_loss: 933166521273.4290\n",
      "Epoch 404/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 880939700077.0626 - val_loss: 937182969879.0436\n",
      "Epoch 405/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 880233034455.0040 - val_loss: 932127352316.3994\n",
      "Epoch 406/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 879866203294.7020 - val_loss: 930589681793.6202\n",
      "Epoch 407/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 880399269815.0115 - val_loss: 931255323295.1449\n",
      "Epoch 408/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 880304971685.7247 - val_loss: 933091038918.0309\n",
      "Epoch 409/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 880669810712.9697 - val_loss: 930284173424.3375\n",
      "Epoch 410/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 880034519288.2571 - val_loss: 929998365029.1758\n",
      "Epoch 411/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 879065615380.4080 - val_loss: 932512599470.6273\n",
      "Epoch 412/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 879791767221.6309 - val_loss: 930242963078.6610\n",
      "Epoch 413/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 879168818384.6416 - val_loss: 934206392783.7524\n",
      "Epoch 414/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 879663844691.2526 - val_loss: 929794149967.9325\n",
      "Epoch 415/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 878268076465.1292 - val_loss: 929667162768.7427\n",
      "Epoch 416/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 879237148446.3119 - val_loss: 930365733114.5991\n",
      "Epoch 417/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 879104886887.2402 - val_loss: 930800111898.2841\n",
      "Epoch 418/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 878974907071.9550 - val_loss: 929365554298.4191\n",
      "Epoch 419/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 878887321811.2825 - val_loss: 931266107423.6849\n",
      "Epoch 420/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 878966983131.8660 - val_loss: 929932793542.0309\n",
      "Epoch 421/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 879822853497.1873 - val_loss: 929939657769.7665\n",
      "Epoch 422/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 878883110326.1711 - val_loss: 929203629221.6259\n",
      "Epoch 423/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 879212798126.7882 - val_loss: 929374409401.0690\n",
      "Epoch 424/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 878149137276.1885 - val_loss: 934019009137.0576\n",
      "Epoch 425/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 878640509309.2689 - val_loss: 929615268303.7524\n",
      "Epoch 426/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 878268310704.4689 - val_loss: 928568690644.7932\n",
      "Epoch 427/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 876817770562.7460 - val_loss: 928655996395.1167\n",
      "Epoch 428/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 878280700189.2314 - val_loss: 929251270759.6962\n",
      "Epoch 429/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 878475775399.7655 - val_loss: 928376941987.1055\n",
      "Epoch 430/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 876296636311.7992 - val_loss: 928804341417.2264\n",
      "Epoch 431/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 878218347963.4532 - val_loss: 928886042503.0211\n",
      "Epoch 432/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 877972309692.8337 - val_loss: 927973056154.8242\n",
      "Epoch 433/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 877892789024.9529 - val_loss: 929444523242.7567\n",
      "Epoch 434/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 877148975093.4359 - val_loss: 927966467805.0746\n",
      "Epoch 435/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 877284163729.9771 - val_loss: 928277620013.0071\n",
      "Epoch 436/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 877585118049.2980 - val_loss: 927518830839.7188\n",
      "Epoch 437/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 876748269119.9850 - val_loss: 932258015430.7511\n",
      "Epoch 438/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 877687791587.1887 - val_loss: 931557803550.9648\n",
      "Epoch 439/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 877769682468.6143 - val_loss: 927250184212.1632\n",
      "Epoch 440/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 877153363669.8035 - val_loss: 927207797536.7651\n",
      "Epoch 441/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 877928533527.1691 - val_loss: 930495559469.7272\n",
      "Epoch 442/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 877086828710.6250 - val_loss: 930119618980.5457\n",
      "Epoch 443/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 876555239800.4670 - val_loss: 926809702837.8284\n",
      "Epoch 444/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 876642087047.4128 - val_loss: 933044492780.5570\n",
      "Epoch 445/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 877373836254.6270 - val_loss: 927915692245.1533\n",
      "Epoch 446/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 877436869532.3610 - val_loss: 929222736237.8171\n",
      "Epoch 447/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 875813245126.7976 - val_loss: 926547875641.2489\n",
      "Epoch 448/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 874608964003.6840 - val_loss: 928995538042.4191\n",
      "Epoch 449/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 79us/step - loss: 877643034571.1793 - val_loss: 927277788878.6722\n",
      "Epoch 450/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 875576725057.4255 - val_loss: 926250021987.3755\n",
      "Epoch 451/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 876306560260.0215 - val_loss: 926112311494.7511\n",
      "Epoch 452/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 876450310840.7522 - val_loss: 930352320909.5021\n",
      "Epoch 453/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 875877725259.6295 - val_loss: 929668809925.3108\n",
      "Epoch 454/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 876079102759.1953 - val_loss: 926695470179.3755\n",
      "Epoch 455/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 875916266090.0015 - val_loss: 925893722019.8256\n",
      "Epoch 456/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 875491642711.0941 - val_loss: 927144442383.1223\n",
      "Epoch 457/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 875701542771.7852 - val_loss: 926226331783.3811\n",
      "Epoch 458/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 876236273553.0767 - val_loss: 925473551643.7244\n",
      "Epoch 459/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 875291859395.8564 - val_loss: 925463008551.2461\n",
      "Epoch 460/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 875370378269.5316 - val_loss: 925544702078.7397\n",
      "Epoch 461/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 875378842401.4331 - val_loss: 925903864338.0028\n",
      "Epoch 462/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 875040988525.1826 - val_loss: 925183052493.2321\n",
      "Epoch 463/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 875054220466.3896 - val_loss: 925566055477.2883\n",
      "Epoch 464/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 875518400445.2540 - val_loss: 925170811179.5668\n",
      "Epoch 465/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 874670700767.2872 - val_loss: 924791118915.6906\n",
      "Epoch 466/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 875015790494.0417 - val_loss: 932915420596.3882\n",
      "Epoch 467/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 874963906818.3409 - val_loss: 926187475111.0662\n",
      "Epoch 468/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 875347343517.9817 - val_loss: 925293221735.3362\n",
      "Epoch 469/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 873809053614.8483 - val_loss: 927421277415.8762\n",
      "Epoch 470/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 874190599735.3416 - val_loss: 928950927659.5668\n",
      "Epoch 471/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 875108128612.8994 - val_loss: 924287900140.5570\n",
      "Epoch 472/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 874083484992.5251 - val_loss: 928050295999.5499\n",
      "Epoch 473/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 873504764173.8655 - val_loss: 931120665157.8510\n",
      "Epoch 474/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 874290181864.0507 - val_loss: 929465265709.3671\n",
      "Epoch 475/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 874073576387.2563 - val_loss: 923952382827.6569\n",
      "Epoch 476/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 874304124667.2582 - val_loss: 924752361423.0323\n",
      "Epoch 477/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 873964068858.2377 - val_loss: 923940303556.5907\n",
      "Epoch 478/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 872186753451.3669 - val_loss: 925854059190.1885\n",
      "Epoch 479/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 873513151322.0952 - val_loss: 927475995459.3306\n",
      "Epoch 480/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 873909107525.9274 - val_loss: 923790194224.2476\n",
      "Epoch 481/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 873626400165.3646 - val_loss: 923397133630.2897\n",
      "Epoch 482/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 874114457644.1772 - val_loss: 925432799957.8734\n",
      "Epoch 483/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 873357664910.9760 - val_loss: 923218161358.6722\n",
      "Epoch 484/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 872555334200.0619 - val_loss: 926309180256.1350\n",
      "Epoch 485/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 872753999738.5078 - val_loss: 932115386327.6737\n",
      "Epoch 486/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 873645728929.1029 - val_loss: 922862513617.1926\n",
      "Epoch 487/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 872412737221.9573 - val_loss: 923087587376.9677\n",
      "Epoch 488/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 872778372984.1069 - val_loss: 922700438756.9957\n",
      "Epoch 489/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 872684132707.8190 - val_loss: 923274041444.8158\n",
      "Epoch 490/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 873226710553.0898 - val_loss: 924357475315.0380\n",
      "Epoch 491/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 872878665098.9542 - val_loss: 928288429489.5077\n",
      "Epoch 492/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 873084052080.2438 - val_loss: 922958453435.9493\n",
      "Epoch 493/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 871909621206.8240 - val_loss: 922364466286.8973\n",
      "Epoch 494/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 871036869659.1306 - val_loss: 922418424168.0563\n",
      "Epoch 495/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 872407609295.0209 - val_loss: 922145925203.5331\n",
      "Epoch 496/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 872330145189.8447 - val_loss: 925042405231.9775\n",
      "Epoch 497/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 871899115455.6549 - val_loss: 921834886370.1154\n",
      "Epoch 498/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 872279567854.8333 - val_loss: 923349948078.9873\n",
      "Epoch 499/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 871769651354.1403 - val_loss: 924257194542.8074\n",
      "Epoch 500/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 872992388667.1831 - val_loss: 921650245096.2363\n",
      "Epoch 501/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 871412594738.4198 - val_loss: 925571565939.5780\n",
      "Epoch 502/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 871556731528.2533 - val_loss: 923702098539.2968\n",
      "Epoch 503/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 872450344298.7817 - val_loss: 922613901003.7919\n",
      "Epoch 504/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 872551743242.1440 - val_loss: 921529280295.9662\n",
      "Epoch 505/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 871501130847.3173 - val_loss: 921509253379.2405\n",
      "Epoch 506/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 872035172002.4235 - val_loss: 921341549796.9957\n",
      "Epoch 507/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 871626711744.9153 - val_loss: 922067956720.1575\n",
      "Epoch 508/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 871046661121.6807 - val_loss: 922640984055.3586\n",
      "Epoch 509/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 871147486068.0253 - val_loss: 924103120328.5514\n",
      "Epoch 510/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 871030695945.1235 - val_loss: 921403520491.1167\n",
      "Epoch 511/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 871448061699.9015 - val_loss: 920912763049.9465\n",
      "Epoch 512/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 871041111966.5219 - val_loss: 921246623896.6638\n",
      "Epoch 513/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 58us/step - loss: 870910593419.1943 - val_loss: 920906864349.0746\n",
      "Epoch 514/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 870767117144.6547 - val_loss: 920733326864.5626\n",
      "Epoch 515/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 871006020504.0394 - val_loss: 920963309279.9550\n",
      "Epoch 516/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 870657887755.6447 - val_loss: 920982969250.3854\n",
      "Epoch 517/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 870765761302.3888 - val_loss: 921488089399.0886\n",
      "Epoch 518/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 871622093984.6227 - val_loss: 920804446974.1997\n",
      "Epoch 519/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 870627525195.7495 - val_loss: 920257067350.7736\n",
      "Epoch 520/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 871172592118.2762 - val_loss: 920792945913.1589\n",
      "Epoch 521/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 871106717620.6106 - val_loss: 920448068934.9310\n",
      "Epoch 522/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 870328168963.7214 - val_loss: 920604432945.6877\n",
      "Epoch 523/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 869486518911.6099 - val_loss: 919884778903.5837\n",
      "Epoch 524/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 870080582883.3689 - val_loss: 919758033570.0253\n",
      "Epoch 525/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 869716274405.7698 - val_loss: 920911077825.3502\n",
      "Epoch 526/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 870378095241.2136 - val_loss: 920212999598.6273\n",
      "Epoch 527/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 869727121405.1189 - val_loss: 919419571653.6709\n",
      "Epoch 528/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 868968424639.8350 - val_loss: 919670694452.5682\n",
      "Epoch 529/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 869742313925.2970 - val_loss: 919308819039.7750\n",
      "Epoch 530/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 869787687648.1276 - val_loss: 920107919581.7947\n",
      "Epoch 531/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 870101854347.7345 - val_loss: 920060338510.1322\n",
      "Epoch 532/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 869726682183.3079 - val_loss: 918977149455.1223\n",
      "Epoch 533/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 868914254733.4753 - val_loss: 926632187935.6849\n",
      "Epoch 534/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 868997690896.9266 - val_loss: 920542426395.7244\n",
      "Epoch 535/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 868923103476.4154 - val_loss: 918966453537.4852\n",
      "Epoch 536/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 869457434050.1758 - val_loss: 919998449666.8805\n",
      "Epoch 537/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 868468956237.0701 - val_loss: 919568955942.1660\n",
      "Epoch 538/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 868409869256.0581 - val_loss: 918937674661.2659\n",
      "Epoch 539/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 869148943467.3219 - val_loss: 922386577082.5092\n",
      "Epoch 540/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 868702631245.0100 - val_loss: 926160068988.2194\n",
      "Epoch 541/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 869221493417.3862 - val_loss: 918673740542.1997\n",
      "Epoch 542/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 867980769445.6647 - val_loss: 921649098077.9747\n",
      "Epoch 543/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 867916648085.9386 - val_loss: 921089697437.7046\n",
      "Epoch 544/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 867976674479.0284 - val_loss: 924762017889.9353\n",
      "Epoch 545/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 868546485447.9982 - val_loss: 919671555269.3108\n",
      "Epoch 546/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 868704963650.5060 - val_loss: 919049798369.3953\n",
      "Epoch 547/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 867968696638.8445 - val_loss: 918253694878.0647\n",
      "Epoch 548/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 867688643286.5238 - val_loss: 918434450724.3657\n",
      "Epoch 549/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 868080151017.3112 - val_loss: 922679608540.3544\n",
      "Epoch 550/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 867749926545.6168 - val_loss: 917930980128.7651\n",
      "Epoch 551/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 867892365330.7273 - val_loss: 917541852155.6793\n",
      "Epoch 552/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 868080644757.6985 - val_loss: 918846342272.1801\n",
      "Epoch 553/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 867756452194.8585 - val_loss: 918028805835.7919\n",
      "Epoch 554/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 867149322388.3779 - val_loss: 918861199956.2532\n",
      "Epoch 555/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 867981771985.1217 - val_loss: 919764452226.7004\n",
      "Epoch 556/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 866432530432.4802 - val_loss: 918323485556.2982\n",
      "Epoch 557/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 867721674214.1898 - val_loss: 919200787875.1055\n",
      "Epoch 558/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 866974795333.7472 - val_loss: 918388242406.0759\n",
      "Epoch 559/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 867734097135.3735 - val_loss: 917311246549.1533\n",
      "Epoch 560/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 867261129662.4543 - val_loss: 917343710004.9282\n",
      "Epoch 561/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 867683380953.1648 - val_loss: 921114501845.8734\n",
      "Epoch 562/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 867671938782.4469 - val_loss: 917003347268.0507\n",
      "Epoch 563/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 866603323978.7892 - val_loss: 924821360691.8481\n",
      "Epoch 564/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 867121009576.3658 - val_loss: 917006433082.6892\n",
      "Epoch 565/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 867192668605.6140 - val_loss: 917172594955.8818\n",
      "Epoch 566/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 867154180334.6532 - val_loss: 916410239923.6681\n",
      "Epoch 567/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 866906164697.7051 - val_loss: 917001837359.1674\n",
      "Epoch 568/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 867449345827.5940 - val_loss: 918652567344.6075\n",
      "Epoch 569/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 866767552769.3806 - val_loss: 918099591075.8256\n",
      "Epoch 570/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 866437317664.4126 - val_loss: 917953598713.1589\n",
      "Epoch 571/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 865497917495.2216 - val_loss: 917255468223.5499\n",
      "Epoch 572/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 866697282288.9341 - val_loss: 916356112533.7834\n",
      "Epoch 573/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 865916253605.3646 - val_loss: 917269507214.5823\n",
      "Epoch 574/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 866159879667.1550 - val_loss: 916345463809.4402\n",
      "Epoch 575/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 865022666449.9620 - val_loss: 915600826633.0015\n",
      "Epoch 576/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 865188433484.9501 - val_loss: 915564210904.7539\n",
      "Epoch 577/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 64us/step - loss: 866148621358.3381 - val_loss: 919120735881.5415\n",
      "Epoch 578/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 866202582402.3109 - val_loss: 915799697330.2279\n",
      "Epoch 579/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 865423024214.4338 - val_loss: 915815212309.9634\n",
      "Epoch 580/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 865874161238.5538 - val_loss: 918083326257.3278\n",
      "Epoch 581/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 866716248741.5447 - val_loss: 918271604733.1195\n",
      "Epoch 582/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 864888059916.0046 - val_loss: 918442875280.3826\n",
      "Epoch 583/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 865581056572.3835 - val_loss: 915338437909.9634\n",
      "Epoch 584/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 865193768975.8462 - val_loss: 915086792482.2053\n",
      "Epoch 585/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 865452710332.1735 - val_loss: 916603802665.7665\n",
      "Epoch 586/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 865773098011.6108 - val_loss: 916838420531.8481\n",
      "Epoch 587/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 865845548820.2279 - val_loss: 915553965719.9437\n",
      "Epoch 588/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 865411370684.3535 - val_loss: 915231007077.1758\n",
      "Epoch 589/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 864227191265.6281 - val_loss: 915033971572.2982\n",
      "Epoch 590/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 864267713380.6593 - val_loss: 914626744786.6329\n",
      "Epoch 591/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 864475315417.2849 - val_loss: 919440600045.2771\n",
      "Epoch 592/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 864495162568.4784 - val_loss: 916170079149.9072\n",
      "Epoch 593/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 864836892188.4510 - val_loss: 917341791763.4430\n",
      "Epoch 594/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 864380424544.4576 - val_loss: 915170129847.9888\n",
      "Epoch 595/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 863604129258.0315 - val_loss: 917371891683.1956\n",
      "Epoch 596/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 865095842810.4779 - val_loss: 914572933094.0759\n",
      "Epoch 597/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 864597391598.6532 - val_loss: 915076210492.1294\n",
      "Epoch 598/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 864443106678.3063 - val_loss: 913808674291.7581\n",
      "Epoch 599/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 864254189792.7278 - val_loss: 915588073751.4037\n",
      "Epoch 600/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 863420830851.3313 - val_loss: 913698574641.3278\n",
      "Epoch 601/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 863550568029.7567 - val_loss: 915995643241.4965\n",
      "Epoch 602/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 863632758663.4730 - val_loss: 913999466086.9761\n",
      "Epoch 603/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 864163317928.5459 - val_loss: 913535031442.9030\n",
      "Epoch 604/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 862437399801.9376 - val_loss: 919639093657.0239\n",
      "Epoch 605/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 864108370072.2195 - val_loss: 914018342759.3362\n",
      "Epoch 606/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 864463964228.6669 - val_loss: 914031727865.1589\n",
      "Epoch 607/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 863860376101.3346 - val_loss: 913524027201.8903\n",
      "Epoch 608/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 864122286768.5890 - val_loss: 917086971204.0507\n",
      "Epoch 609/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 862560320614.0399 - val_loss: 920395900158.9198\n",
      "Epoch 610/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 863639361864.9285 - val_loss: 912916435111.0662\n",
      "Epoch 611/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 862918687565.6104 - val_loss: 912948893589.4233\n",
      "Epoch 612/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 863427853237.3308 - val_loss: 913648059197.5696\n",
      "Epoch 613/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 863244185800.7184 - val_loss: 918576316346.8691\n",
      "Epoch 614/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 863185114101.1958 - val_loss: 913418436354.5204\n",
      "Epoch 615/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 862643616762.4779 - val_loss: 923118767304.1913\n",
      "Epoch 616/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 863121026239.3547 - val_loss: 912916093328.3826\n",
      "Epoch 617/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 862618067372.3273 - val_loss: 913752838541.5021\n",
      "Epoch 618/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 862509754911.0922 - val_loss: 913432553140.7483\n",
      "Epoch 619/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 861807858780.9163 - val_loss: 913342387587.4205\n",
      "Epoch 620/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 862930499012.0967 - val_loss: 912381834362.4191\n",
      "Epoch 621/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 862415217948.0310 - val_loss: 912279359562.8917\n",
      "Epoch 622/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 862710962093.8879 - val_loss: 912040801333.2883\n",
      "Epoch 623/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 862791861407.1821 - val_loss: 912490512130.5204\n",
      "Epoch 624/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 862045658354.2546 - val_loss: 913144960249.1589\n",
      "Epoch 625/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 862009835500.7925 - val_loss: 912595010987.7468\n",
      "Epoch 626/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 861792037399.4091 - val_loss: 916818314529.4852\n",
      "Epoch 627/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 862884401635.0687 - val_loss: 911745517225.2264\n",
      "Epoch 628/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 861967406219.0143 - val_loss: 913956681310.3347\n",
      "Epoch 629/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 861918702614.3287 - val_loss: 911979134538.1716\n",
      "Epoch 630/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 862142324138.6467 - val_loss: 911590251011.6006\n",
      "Epoch 631/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 861557012949.1433 - val_loss: 913079744186.5092\n",
      "Epoch 632/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 861565936201.8289 - val_loss: 912947464056.6189\n",
      "Epoch 633/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 861316198589.1938 - val_loss: 911241170143.2349\n",
      "Epoch 634/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 861649096299.6820 - val_loss: 911104648516.0507\n",
      "Epoch 635/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 861495732594.2246 - val_loss: 912476167120.4725\n",
      "Epoch 636/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 861668823772.7662 - val_loss: 911803345878.2335\n",
      "Epoch 637/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 861855383037.2389 - val_loss: 913276909657.2939\n",
      "Epoch 638/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 860665477585.3018 - val_loss: 915832974132.9282\n",
      "Epoch 639/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 860744018985.5363 - val_loss: 910758485244.0394\n",
      "Epoch 640/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 860844008344.0394 - val_loss: 916415409340.6694\n",
      "Epoch 641/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 60us/step - loss: 860574639956.5730 - val_loss: 914377371457.8903\n",
      "Epoch 642/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 860540687176.3282 - val_loss: 910482688740.2756\n",
      "Epoch 643/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 860192437922.9036 - val_loss: 913698869322.8917\n",
      "Epoch 644/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 861779094615.6342 - val_loss: 910560590858.0815\n",
      "Epoch 645/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 861209171801.3750 - val_loss: 914254662257.0576\n",
      "Epoch 646/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 860838868984.7971 - val_loss: 910517249532.3994\n",
      "Epoch 647/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 860027570390.6439 - val_loss: 911475036603.5894\n",
      "Epoch 648/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 861013034123.9747 - val_loss: 911083238338.0703\n",
      "Epoch 649/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 860681636319.7074 - val_loss: 909939303494.5710\n",
      "Epoch 650/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 860293533695.0397 - val_loss: 914557131728.4725\n",
      "Epoch 651/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 860475123594.3540 - val_loss: 911004041722.9592\n",
      "Epoch 652/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 860291188320.6377 - val_loss: 909911962999.8987\n",
      "Epoch 653/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 860813744080.4614 - val_loss: 910236244990.5598\n",
      "Epoch 654/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 859964885729.0880 - val_loss: 910161744937.7665\n",
      "Epoch 655/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 860086586618.8979 - val_loss: 912596242622.1097\n",
      "Epoch 656/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 858568630999.9644 - val_loss: 910497698051.2405\n",
      "Epoch 657/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 860104886467.4364 - val_loss: 910172323425.2152\n",
      "Epoch 658/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 859365511943.0227 - val_loss: 911936973413.5359\n",
      "Epoch 659/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 860125389800.7109 - val_loss: 910522402173.6597\n",
      "Epoch 660/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 859822628447.1973 - val_loss: 910180189404.3544\n",
      "Epoch 661/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 859316634264.5796 - val_loss: 909884859854.3123\n",
      "Epoch 662/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 858808816306.5098 - val_loss: 908972890466.2954\n",
      "Epoch 663/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 858872111943.3679 - val_loss: 913993051485.9747\n",
      "Epoch 664/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 859779592822.9664 - val_loss: 909439471760.0225\n",
      "Epoch 665/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 859471126595.2262 - val_loss: 912967055725.8171\n",
      "Epoch 666/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 859426898753.6056 - val_loss: 912461537844.5682\n",
      "Epoch 667/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 858734773142.1187 - val_loss: 912764639724.5570\n",
      "Epoch 668/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 859292571389.1788 - val_loss: 909638696314.7792\n",
      "Epoch 669/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 858553647995.2281 - val_loss: 910089321297.7328\n",
      "Epoch 670/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 858982472769.5457 - val_loss: 908666441272.8889\n",
      "Epoch 671/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 858390413538.6486 - val_loss: 910364673336.5288\n",
      "Epoch 672/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 859445467507.1849 - val_loss: 908525978452.6133\n",
      "Epoch 673/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 858164868913.0392 - val_loss: 908489871895.7637\n",
      "Epoch 674/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 858634370230.7114 - val_loss: 912632805257.9015\n",
      "Epoch 675/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 859097472561.0992 - val_loss: 908348059642.2391\n",
      "Epoch 676/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 858133367886.0305 - val_loss: 908620061971.0830\n",
      "Epoch 677/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 858384122866.3147 - val_loss: 910865306343.1561\n",
      "Epoch 678/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 858453489979.4833 - val_loss: 909715338869.3783\n",
      "Epoch 679/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 858300137583.4034 - val_loss: 907975690081.5752\n",
      "Epoch 680/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 857745658738.5848 - val_loss: 908078616812.1969\n",
      "Epoch 681/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 857938059911.2928 - val_loss: 909870112318.6498\n",
      "Epoch 682/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 857535253236.2954 - val_loss: 912899228823.2236\n",
      "Epoch 683/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 858113550383.5386 - val_loss: 912959829160.5063\n",
      "Epoch 684/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 858430975574.5538 - val_loss: 912410877318.3010\n",
      "Epoch 685/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 858612018997.3608 - val_loss: 907501560257.3502\n",
      "Epoch 686/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 857348260988.6086 - val_loss: 908147963486.3347\n",
      "Epoch 687/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 857547651625.6562 - val_loss: 907270611393.3502\n",
      "Epoch 688/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 857900196222.2292 - val_loss: 909321167758.2222\n",
      "Epoch 689/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 858322047966.1469 - val_loss: 909042969189.5359\n",
      "Epoch 690/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 857899184340.2429 - val_loss: 907783288460.4220\n",
      "Epoch 691/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 857695065192.4408 - val_loss: 907756176834.7904\n",
      "Epoch 692/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 857237400751.9888 - val_loss: 915272648891.2292\n",
      "Epoch 693/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 858084529921.0204 - val_loss: 907432165560.3488\n",
      "Epoch 694/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 857002686372.2842 - val_loss: 907298246846.1097\n",
      "Epoch 695/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 857437272859.6708 - val_loss: 907131617556.5232\n",
      "Epoch 696/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 856227372550.8427 - val_loss: 907407184453.8510\n",
      "Epoch 697/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 857433720570.2977 - val_loss: 911600472599.7637\n",
      "Epoch 698/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 856779083732.7831 - val_loss: 907313933915.4543\n",
      "Epoch 699/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 857535622327.6718 - val_loss: 906424001860.0507\n",
      "Epoch 700/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 856955002117.4620 - val_loss: 906545391123.4430\n",
      "Epoch 701/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 856283358365.2615 - val_loss: 909853182922.7117\n",
      "Epoch 702/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 856992352151.7992 - val_loss: 909842885567.1898\n",
      "Epoch 703/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 857046151998.7245 - val_loss: 906351114582.7736\n",
      "Epoch 704/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 856710469169.5793 - val_loss: 910232817301.0632\n",
      "Epoch 705/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 65us/step - loss: 857061964337.3392 - val_loss: 906814083838.1997\n",
      "Epoch 706/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 857120544755.0350 - val_loss: 906402297553.5527\n",
      "Epoch 707/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 856269209642.2566 - val_loss: 906801142275.6006\n",
      "Epoch 708/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 856555114491.1981 - val_loss: 906111078520.9789\n",
      "Epoch 709/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 856429986982.8652 - val_loss: 905864257409.2603\n",
      "Epoch 710/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 855689284497.7969 - val_loss: 906092128662.1434\n",
      "Epoch 711/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 855993068566.3287 - val_loss: 906252564108.4220\n",
      "Epoch 712/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 855852974141.4640 - val_loss: 909682406706.7679\n",
      "Epoch 713/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 856021459652.7570 - val_loss: 905796132691.1730\n",
      "Epoch 714/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 856206253363.5602 - val_loss: 907241587333.2208\n",
      "Epoch 715/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 854901828461.3026 - val_loss: 906541270005.9185\n",
      "Epoch 716/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 855693884833.0430 - val_loss: 905577843981.3220\n",
      "Epoch 717/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 855691066969.4349 - val_loss: 905598133134.2222\n",
      "Epoch 718/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 855856440650.1289 - val_loss: 905424324749.1421\n",
      "Epoch 719/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 855611127739.3331 - val_loss: 906572136079.3024\n",
      "Epoch 720/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 855105714583.6792 - val_loss: 913162291433.3164\n",
      "Epoch 721/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 856204422611.9427 - val_loss: 905926538731.1167\n",
      "Epoch 722/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 854428227924.4530 - val_loss: 910854711172.1406\n",
      "Epoch 723/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 855363913762.0934 - val_loss: 904794456409.6541\n",
      "Epoch 724/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 855180481110.3137 - val_loss: 904947716263.0662\n",
      "Epoch 725/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 855282904397.4902 - val_loss: 905147799238.0309\n",
      "Epoch 726/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 854561710604.3649 - val_loss: 907307727997.2996\n",
      "Epoch 727/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 855513124467.3651 - val_loss: 905795756619.6118\n",
      "Epoch 728/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 853281442917.0795 - val_loss: 908025460110.9424\n",
      "Epoch 729/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 854830826098.1646 - val_loss: 904534077272.9338\n",
      "Epoch 730/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 854834956533.3759 - val_loss: 904485643495.8762\n",
      "Epoch 731/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 854901074758.8877 - val_loss: 906544727375.5724\n",
      "Epoch 732/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 855064463397.9348 - val_loss: 909233878599.2911\n",
      "Epoch 733/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 855249664569.0222 - val_loss: 904882984300.3770\n",
      "Epoch 734/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 854892124923.0181 - val_loss: 907245390080.3601\n",
      "Epoch 735/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 854386388105.5737 - val_loss: 904361117168.8777\n",
      "Epoch 736/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 854523329695.6624 - val_loss: 904060756647.7863\n",
      "Epoch 737/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 852407433062.8202 - val_loss: 910526350740.7032\n",
      "Epoch 738/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 855341820385.6281 - val_loss: 905869365806.8074\n",
      "Epoch 739/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 854115023543.7917 - val_loss: 908224424180.8383\n",
      "Epoch 740/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 854842729986.5210 - val_loss: 906846967609.2489\n",
      "Epoch 741/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 854219417781.9911 - val_loss: 907931908254.4248\n",
      "Epoch 742/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 854501124347.1381 - val_loss: 905217969674.8016\n",
      "Epoch 743/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 854616434178.5210 - val_loss: 903435167504.9226\n",
      "Epoch 744/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 853532707480.0995 - val_loss: 906358386590.0647\n",
      "Epoch 745/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 853476855027.2150 - val_loss: 909168581515.3418\n",
      "Epoch 746/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 853201031535.8237 - val_loss: 904934907062.9086\n",
      "Epoch 747/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 852958148374.1487 - val_loss: 904395860281.9691\n",
      "Epoch 748/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 853806883444.0854 - val_loss: 903044761257.2264\n",
      "Epoch 749/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 854078305374.8370 - val_loss: 903108251907.2405\n",
      "Epoch 750/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 852224120496.3489 - val_loss: 903241920091.4543\n",
      "Epoch 751/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 852792880042.7667 - val_loss: 908004942549.8734\n",
      "Epoch 752/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 853630702546.6222 - val_loss: 904076240564.7483\n",
      "Epoch 753/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 853717375020.6575 - val_loss: 902811391432.5514\n",
      "Epoch 754/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 853294746801.9094 - val_loss: 902802969733.9409\n",
      "Epoch 755/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 853124850691.6014 - val_loss: 904728074156.4669\n",
      "Epoch 756/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 852879396920.6621 - val_loss: 902602875237.1758\n",
      "Epoch 757/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 853364656457.8889 - val_loss: 903841136847.3925\n",
      "Epoch 758/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 853360185289.2587 - val_loss: 905181282880.0900\n",
      "Epoch 759/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 853122922627.0913 - val_loss: 907839823537.8678\n",
      "Epoch 760/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 853165631362.6710 - val_loss: 906255759610.5991\n",
      "Epoch 761/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 852823390592.6302 - val_loss: 902509029067.7919\n",
      "Epoch 762/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 852971161042.5022 - val_loss: 903850888438.2784\n",
      "Epoch 763/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 852684195816.4707 - val_loss: 903014485100.0168\n",
      "Epoch 764/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 850771913604.3517 - val_loss: 902023459939.3755\n",
      "Epoch 765/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 853099186548.1454 - val_loss: 903172653332.5232\n",
      "Epoch 766/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 851776635525.3722 - val_loss: 902472742586.5092\n",
      "Epoch 767/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 853610630859.7196 - val_loss: 901816741421.3671\n",
      "Epoch 768/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 852323494039.2592 - val_loss: 905561436108.1519\n",
      "Epoch 769/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 57us/step - loss: 852376472657.8721 - val_loss: 904277904791.5837\n",
      "Epoch 770/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 852276253193.4838 - val_loss: 905915094633.8566\n",
      "Epoch 771/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 852650195693.5728 - val_loss: 904097896848.3826\n",
      "Epoch 772/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 851536886075.4833 - val_loss: 902792252892.7145\n",
      "Epoch 773/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 852570190005.5109 - val_loss: 901350985088.5400\n",
      "Epoch 774/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 851991592475.9708 - val_loss: 903481622850.6104\n",
      "Epoch 775/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 852713384659.6427 - val_loss: 902549641293.7721\n",
      "Epoch 776/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 852167618399.6173 - val_loss: 901401358583.7188\n",
      "Epoch 777/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 851717818832.3413 - val_loss: 901699776529.2827\n",
      "Epoch 778/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 851351761818.9205 - val_loss: 901125936123.6793\n",
      "Epoch 779/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 852030806383.3435 - val_loss: 902827009739.7919\n",
      "Epoch 780/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 850968365301.3759 - val_loss: 903294760911.0323\n",
      "Epoch 781/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 851896065588.9407 - val_loss: 901173832708.3207\n",
      "Epoch 782/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 851216242244.7869 - val_loss: 902469010629.3108\n",
      "Epoch 783/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 851941130086.3400 - val_loss: 900857839316.4332\n",
      "Epoch 784/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 851340266550.9816 - val_loss: 901439065196.0168\n",
      "Epoch 785/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 851335986520.0544 - val_loss: 901356213497.1589\n",
      "Epoch 786/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 851773247414.0511 - val_loss: 902170594803.7581\n",
      "Epoch 787/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 851619330440.3132 - val_loss: 900925138612.7483\n",
      "Epoch 788/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 850780463985.6244 - val_loss: 907618781394.2728\n",
      "Epoch 789/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 851951925671.0453 - val_loss: 904127114566.9310\n",
      "Epoch 790/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 850374952271.8912 - val_loss: 906006576529.8228\n",
      "Epoch 791/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 851359195314.6298 - val_loss: 901005448694.6385\n",
      "Epoch 792/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 850210593131.5020 - val_loss: 903227117988.5457\n",
      "Epoch 793/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 850532780684.5748 - val_loss: 900280471304.2812\n",
      "Epoch 794/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 850654211762.7498 - val_loss: 900385528185.3390\n",
      "Epoch 795/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 851091697661.1189 - val_loss: 906528102616.0338\n",
      "Epoch 796/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 850617776735.4373 - val_loss: 900584247624.3713\n",
      "Epoch 797/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 850365778607.1484 - val_loss: 900546480453.4908\n",
      "Epoch 798/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 849531817933.1001 - val_loss: 904031779873.1251\n",
      "Epoch 799/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 850738271632.2363 - val_loss: 900032451740.9845\n",
      "Epoch 800/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 849831740663.5367 - val_loss: 905862692195.7356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f94010160>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_train3 = model3.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train3'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4265 samples, validate on 2844 samples\n",
      "Epoch 1/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3843225573266.9971 - val_loss: 4114525916314.1040\n",
      "Epoch 2/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3841770584406.3740 - val_loss: 4113004377970.8574\n",
      "Epoch 3/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3840247139067.9785 - val_loss: 4111520429086.2446\n",
      "Epoch 4/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3838776825401.2627 - val_loss: 4109991802162.7681\n",
      "Epoch 5/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3837312043556.8545 - val_loss: 4108480249256.8662\n",
      "Epoch 6/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3835801195513.0376 - val_loss: 4106919190216.9116\n",
      "Epoch 7/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3834334801191.0752 - val_loss: 4105418307788.5122\n",
      "Epoch 8/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3832860833519.7339 - val_loss: 4103938740874.9819\n",
      "Epoch 9/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3831369702023.7729 - val_loss: 4102496518973.5693\n",
      "Epoch 10/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3829925867421.0815 - val_loss: 4100964187745.2153\n",
      "Epoch 11/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3828443359356.3687 - val_loss: 4099493183833.6538\n",
      "Epoch 12/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3827000713990.0620 - val_loss: 4098094381606.1660\n",
      "Epoch 13/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3825566517958.1973 - val_loss: 4096625965951.8193\n",
      "Epoch 14/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3824075549710.6460 - val_loss: 4095168257276.0400\n",
      "Epoch 15/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3822684530301.4492 - val_loss: 4093718284613.4902\n",
      "Epoch 16/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3821226530733.8877 - val_loss: 4092197044546.6104\n",
      "Epoch 17/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3819781034543.8989 - val_loss: 4090756716276.1182\n",
      "Epoch 18/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3818303992934.5200 - val_loss: 4089296617702.4355\n",
      "Epoch 19/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3816905167381.0083 - val_loss: 4087826641351.1113\n",
      "Epoch 20/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3815444145720.3018 - val_loss: 4086319302290.1821\n",
      "Epoch 21/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3814042719225.0376 - val_loss: 4084878534163.4429\n",
      "Epoch 22/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3812629608790.3740 - val_loss: 4083433732484.8613\n",
      "Epoch 23/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3811222119485.2241 - val_loss: 4082052396937.9009\n",
      "Epoch 24/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3809810402009.4048 - val_loss: 4080574219645.6597\n",
      "Epoch 25/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3808398402536.7109 - val_loss: 4079083814497.2153\n",
      "Epoch 26/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3807019728050.8701 - val_loss: 4077664307318.0991\n",
      "Epoch 27/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3805613201763.8188 - val_loss: 4076272387200.1807\n",
      "Epoch 28/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3804192178514.0518 - val_loss: 4074898851082.4409\n",
      "Epoch 29/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3802828376781.8804 - val_loss: 4073559339117.4565\n",
      "Epoch 30/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3801422163592.4932 - val_loss: 4072129672844.4219\n",
      "Epoch 31/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3800045595908.5015 - val_loss: 4070746496390.3003\n",
      "Epoch 32/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3798669230318.1733 - val_loss: 4069292194111.7300\n",
      "Epoch 33/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3797231084363.2090 - val_loss: 4067879890177.8003\n",
      "Epoch 34/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3795907428094.6196 - val_loss: 4066490307971.4199\n",
      "Epoch 35/800\n",
      "4265/4265 [==============================] - 0s 51us/step - loss: 3794550302706.0742 - val_loss: 4065063436057.5645\n",
      "Epoch 36/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3793125499911.4429 - val_loss: 4063658648393.0913\n",
      "Epoch 37/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3791741980586.5264 - val_loss: 4062223359481.5190\n",
      "Epoch 38/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3790428400358.8501 - val_loss: 4060869259626.9365\n",
      "Epoch 39/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3789046695451.4907 - val_loss: 4059436436196.2759\n",
      "Epoch 40/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3787694577412.8618 - val_loss: 4058057965061.0410\n",
      "Epoch 41/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3786287433043.7329 - val_loss: 4056718423968.9453\n",
      "Epoch 42/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3784974867218.0669 - val_loss: 4055362040032.6753\n",
      "Epoch 43/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3783622036750.1055 - val_loss: 4053933960634.1494\n",
      "Epoch 44/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3782253284645.1543 - val_loss: 4052547521574.8862\n",
      "Epoch 45/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3780941497507.0239 - val_loss: 4051200340330.9365\n",
      "Epoch 46/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3779580900078.0527 - val_loss: 4049829945516.8271\n",
      "Epoch 47/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3778262897207.5815 - val_loss: 4048509807873.8003\n",
      "Epoch 48/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3776936201297.8716 - val_loss: 4047140405593.6538\n",
      "Epoch 49/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3775569351811.3315 - val_loss: 4045815865332.4785\n",
      "Epoch 50/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3774238499275.0596 - val_loss: 4044597403429.0859\n",
      "Epoch 51/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3772880016330.9390 - val_loss: 4043352539827.3081\n",
      "Epoch 52/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3771613011501.2578 - val_loss: 4042017534084.5010\n",
      "Epoch 53/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3770289619958.1562 - val_loss: 4040731088028.9844\n",
      "Epoch 54/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3768995160384.7651 - val_loss: 4039320575596.7368\n",
      "Epoch 55/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3767653968864.5474 - val_loss: 4037937812452.6353\n",
      "Epoch 56/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3766362870511.9736 - val_loss: 4036573472759.3584\n",
      "Epoch 57/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3765061668459.6821 - val_loss: 4035210720309.2881\n",
      "Epoch 58/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3763795103466.4517 - val_loss: 4033847898912.7646\n",
      "Epoch 59/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3762475258212.2988 - val_loss: 4032511242142.0649\n",
      "Epoch 60/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3761148426590.0566 - val_loss: 4031139769624.8438\n",
      "Epoch 61/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3759832973123.7666 - val_loss: 4029832722959.1226\n",
      "Epoch 62/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3758558230589.9443 - val_loss: 4028535172837.7163\n",
      "Epoch 63/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3757264996117.1880 - val_loss: 4027208149029.4463\n",
      "Epoch 64/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 56us/step - loss: 3755970386694.5425 - val_loss: 4025928240351.2354\n",
      "Epoch 65/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3754697595335.6982 - val_loss: 4024516521909.1084\n",
      "Epoch 66/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3753428255180.9800 - val_loss: 4023296282883.2402\n",
      "Epoch 67/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3752132169085.5093 - val_loss: 4022042422322.4082\n",
      "Epoch 68/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3750880508684.3047 - val_loss: 4020747343212.3770\n",
      "Epoch 69/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3749555050106.5679 - val_loss: 4019509728942.9868\n",
      "Epoch 70/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3748314332450.2734 - val_loss: 4018213592039.5166\n",
      "Epoch 71/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3747038580677.4170 - val_loss: 4016910747420.4443\n",
      "Epoch 72/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3745792372379.7012 - val_loss: 4015508258954.2622\n",
      "Epoch 73/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3744543281973.1206 - val_loss: 4014227121406.9199\n",
      "Epoch 74/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3743228498956.2446 - val_loss: 4012930062390.7285\n",
      "Epoch 75/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3742024656140.6650 - val_loss: 4011687169477.6709\n",
      "Epoch 76/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3740704509958.7227 - val_loss: 4010389974412.0615\n",
      "Epoch 77/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3739499856678.7148 - val_loss: 4009113285201.3726\n",
      "Epoch 78/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3738268487206.5347 - val_loss: 4007867924963.9160\n",
      "Epoch 79/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3736997146582.7036 - val_loss: 4006597404432.9229\n",
      "Epoch 80/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3735781928868.7642 - val_loss: 4005333377404.2197\n",
      "Epoch 81/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3734543931609.5249 - val_loss: 4004021297278.7397\n",
      "Epoch 82/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3733284867345.9473 - val_loss: 4002799734259.7588\n",
      "Epoch 83/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3732084615472.9189 - val_loss: 4001597122666.5762\n",
      "Epoch 84/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3730831239488.2852 - val_loss: 4000293755971.6899\n",
      "Epoch 85/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3729596746538.0762 - val_loss: 3999003380705.7554\n",
      "Epoch 86/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3728363177453.3926 - val_loss: 3997681836481.3496\n",
      "Epoch 87/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3727181961199.1934 - val_loss: 3996445806505.5869\n",
      "Epoch 88/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3725921984410.2002 - val_loss: 3995183000234.6665\n",
      "Epoch 89/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3724697661786.6953 - val_loss: 3993987659358.3350\n",
      "Epoch 90/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3723531451109.8901 - val_loss: 3992764027577.0684\n",
      "Epoch 91/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3722319051520.7808 - val_loss: 3991546543459.7354\n",
      "Epoch 92/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3721084078413.7305 - val_loss: 3990360528789.4238\n",
      "Epoch 93/800\n",
      "4265/4265 [==============================] - 0s 51us/step - loss: 3719882415111.9233 - val_loss: 3989141725103.3472\n",
      "Epoch 94/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3718705482960.6416 - val_loss: 3987905564977.3271\n",
      "Epoch 95/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3717489812379.8809 - val_loss: 3986691897617.6426\n",
      "Epoch 96/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3716273801165.5801 - val_loss: 3985530639516.9844\n",
      "Epoch 97/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3715034724844.1924 - val_loss: 3984302359196.2646\n",
      "Epoch 98/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3713840078027.8394 - val_loss: 3983148118517.1978\n",
      "Epoch 99/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3712639685617.3540 - val_loss: 3981921863452.4443\n",
      "Epoch 100/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3711479272717.6250 - val_loss: 3980608783050.3516\n",
      "Epoch 101/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3710322030684.6763 - val_loss: 3979525819730.4531\n",
      "Epoch 102/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3709044796276.9854 - val_loss: 3978326476641.5757\n",
      "Epoch 103/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3707912062648.9922 - val_loss: 3977000051598.2222\n",
      "Epoch 104/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3706721099032.6694 - val_loss: 3975730765835.5215\n",
      "Epoch 105/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3705543258351.1333 - val_loss: 3974542731071.0098\n",
      "Epoch 106/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3704338621297.6240 - val_loss: 3973341349084.3540\n",
      "Epoch 107/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3703198420222.7393 - val_loss: 3972169612228.9507\n",
      "Epoch 108/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3701983079765.1733 - val_loss: 3970970692036.2310\n",
      "Epoch 109/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3700875540603.4082 - val_loss: 3969708356592.1582\n",
      "Epoch 110/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3699651520075.5098 - val_loss: 3968545898012.0840\n",
      "Epoch 111/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3698526262130.5850 - val_loss: 3967317640550.6162\n",
      "Epoch 112/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3697340394526.0117 - val_loss: 3966269810794.5762\n",
      "Epoch 113/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3696143572744.2231 - val_loss: 3965004627898.8691\n",
      "Epoch 114/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3694962625517.9932 - val_loss: 3963786942103.9434\n",
      "Epoch 115/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3693820826409.5962 - val_loss: 3962651762353.8682\n",
      "Epoch 116/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3692712610805.6763 - val_loss: 3961523355578.8691\n",
      "Epoch 117/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3691535109436.2036 - val_loss: 3960256028706.5649\n",
      "Epoch 118/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3690380129831.9756 - val_loss: 3959059157735.1562\n",
      "Epoch 119/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3689255901101.8877 - val_loss: 3957878280128.6304\n",
      "Epoch 120/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3688104515052.4326 - val_loss: 3956646240935.7866\n",
      "Epoch 121/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3686929506294.3965 - val_loss: 3955455833970.8574\n",
      "Epoch 122/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3685796092259.8188 - val_loss: 3954229487469.0972\n",
      "Epoch 123/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3684702517598.0566 - val_loss: 3953154121900.8271\n",
      "Epoch 124/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3683521418432.7954 - val_loss: 3951940857896.3262\n",
      "Epoch 125/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3682397911536.0337 - val_loss: 3950779589103.4375\n",
      "Epoch 126/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3681276267982.1807 - val_loss: 3949614183523.3760\n",
      "Epoch 127/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 58us/step - loss: 3680145628156.3989 - val_loss: 3948496767624.1006\n",
      "Epoch 128/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3679009336407.6343 - val_loss: 3947388559578.9141\n",
      "Epoch 129/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3677858193079.5518 - val_loss: 3946241929555.8931\n",
      "Epoch 130/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3676756137552.3110 - val_loss: 3945120779855.9321\n",
      "Epoch 131/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3675662540210.3296 - val_loss: 3944013435669.2432\n",
      "Epoch 132/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3674493428063.4976 - val_loss: 3942884552905.6309\n",
      "Epoch 133/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3673401813581.1899 - val_loss: 3941682609392.5176\n",
      "Epoch 134/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3672287923558.4600 - val_loss: 3940618948498.5435\n",
      "Epoch 135/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3671164403016.6885 - val_loss: 3939424642925.0972\n",
      "Epoch 136/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3670066321936.6865 - val_loss: 3938271626322.0928\n",
      "Epoch 137/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3668931591343.0283 - val_loss: 3937246440129.7100\n",
      "Epoch 138/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3667834205298.0444 - val_loss: 3936011284704.6753\n",
      "Epoch 139/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3666745397538.9937 - val_loss: 3934863467475.3535\n",
      "Epoch 140/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3665624279502.4209 - val_loss: 3933757478621.0747\n",
      "Epoch 141/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3664492677549.7681 - val_loss: 3932561602027.1162\n",
      "Epoch 142/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3663404328834.6709 - val_loss: 3931440823974.3462\n",
      "Epoch 143/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3662290095105.2002 - val_loss: 3930341067584.4507\n",
      "Epoch 144/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3661196833418.1743 - val_loss: 3929166141863.4263\n",
      "Epoch 145/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3660101702672.0859 - val_loss: 3928048044610.9707\n",
      "Epoch 146/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3659031817407.3545 - val_loss: 3926968940080.2476\n",
      "Epoch 147/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3657923610254.7358 - val_loss: 3925841653408.5850\n",
      "Epoch 148/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3656830995537.8716 - val_loss: 3924723934439.8770\n",
      "Epoch 149/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3655737140661.6909 - val_loss: 3923590995265.1699\n",
      "Epoch 150/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3654677859628.8374 - val_loss: 3922491421380.5913\n",
      "Epoch 151/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3653631237907.7476 - val_loss: 3921414448659.4429\n",
      "Epoch 152/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3652526859286.8091 - val_loss: 3920296623137.1250\n",
      "Epoch 153/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3651470925737.5664 - val_loss: 3919196729960.4165\n",
      "Epoch 154/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3650385431933.5093 - val_loss: 3918091171744.9453\n",
      "Epoch 155/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3649337508988.6089 - val_loss: 3916917421800.5962\n",
      "Epoch 156/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3648208511450.4253 - val_loss: 3915784251452.4897\n",
      "Epoch 157/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3647156059898.7778 - val_loss: 3914679274673.1475\n",
      "Epoch 158/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3646087495680.0000 - val_loss: 3913655789282.8350\n",
      "Epoch 159/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3645054081314.5132 - val_loss: 3912557703830.5034\n",
      "Epoch 160/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3643966384056.4521 - val_loss: 3911472492192.5850\n",
      "Epoch 161/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3642887647021.6782 - val_loss: 3910425929650.2285\n",
      "Epoch 162/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3641884548119.2891 - val_loss: 3909350226723.6455\n",
      "Epoch 163/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3640809771118.4434 - val_loss: 3908157441659.1392\n",
      "Epoch 164/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3639727516959.3921 - val_loss: 3907113271226.8691\n",
      "Epoch 165/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3638662734067.9355 - val_loss: 3906026446168.2139\n",
      "Epoch 166/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3637612104309.5259 - val_loss: 3904917114332.7144\n",
      "Epoch 167/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3636602477990.8052 - val_loss: 3903800686174.3350\n",
      "Epoch 168/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3635543030759.2705 - val_loss: 3902677778155.4766\n",
      "Epoch 169/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3634510679721.1460 - val_loss: 3901640203723.4316\n",
      "Epoch 170/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3633423828330.7817 - val_loss: 3900602369728.2700\n",
      "Epoch 171/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3632358310360.5044 - val_loss: 3899491335984.6074\n",
      "Epoch 172/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3631347788234.5791 - val_loss: 3898441065081.6992\n",
      "Epoch 173/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3630332012072.9360 - val_loss: 3897323690845.2544\n",
      "Epoch 174/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3629247890514.3525 - val_loss: 3896206672033.3052\n",
      "Epoch 175/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3628217584752.6040 - val_loss: 3895145288584.4614\n",
      "Epoch 176/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3627167539885.7080 - val_loss: 3894097894260.2983\n",
      "Epoch 177/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3626133153528.3770 - val_loss: 3893097011508.2080\n",
      "Epoch 178/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3625146692011.3667 - val_loss: 3892011690735.7974\n",
      "Epoch 179/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3624113409793.5005 - val_loss: 3890993902460.9395\n",
      "Epoch 180/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3623057551906.4536 - val_loss: 3890004306648.7539\n",
      "Epoch 181/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3622061001072.3037 - val_loss: 3888935681610.1714\n",
      "Epoch 182/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3621016910531.3164 - val_loss: 3887911878033.8228\n",
      "Epoch 183/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3620029515929.1797 - val_loss: 3886838589707.8818\n",
      "Epoch 184/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3619008969668.6968 - val_loss: 3885876176864.3149\n",
      "Epoch 185/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3618022946912.5176 - val_loss: 3884845266272.8550\n",
      "Epoch 186/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3616935049589.1060 - val_loss: 3883753517626.3291\n",
      "Epoch 187/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3615946639048.3584 - val_loss: 3882701364890.8242\n",
      "Epoch 188/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3614910128433.3989 - val_loss: 3881604186434.6104\n",
      "Epoch 189/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3613887531672.0991 - val_loss: 3880537001160.1914\n",
      "Epoch 190/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 55us/step - loss: 3612936861385.3188 - val_loss: 3879504896725.8735\n",
      "Epoch 191/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3611891266789.2896 - val_loss: 3878489867046.5259\n",
      "Epoch 192/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3610908116061.8765 - val_loss: 3877501688507.9492\n",
      "Epoch 193/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3609882115884.7173 - val_loss: 3876508680030.6948\n",
      "Epoch 194/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3608881961262.0381 - val_loss: 3875488468675.1504\n",
      "Epoch 195/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3607890168049.5342 - val_loss: 3874431471621.7607\n",
      "Epoch 196/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3606871338232.9771 - val_loss: 3873322550146.7002\n",
      "Epoch 197/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3605930691988.5581 - val_loss: 3872213768776.7314\n",
      "Epoch 198/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3604872779482.6055 - val_loss: 3871237034243.2407\n",
      "Epoch 199/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3603855238627.7891 - val_loss: 3870327640484.5459\n",
      "Epoch 200/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3602863375930.2227 - val_loss: 3869232450783.2349\n",
      "Epoch 201/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3601887544951.6870 - val_loss: 3868277947609.4741\n",
      "Epoch 202/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3600877870629.5747 - val_loss: 3867263104122.4189\n",
      "Epoch 203/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3599883595930.6206 - val_loss: 3866252871198.9648\n",
      "Epoch 204/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3598903843947.8022 - val_loss: 3865220311791.7974\n",
      "Epoch 205/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3597872283213.9106 - val_loss: 3864153419263.2798\n",
      "Epoch 206/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3596888200354.0630 - val_loss: 3863161967796.0283\n",
      "Epoch 207/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3595925852335.7485 - val_loss: 3862136522040.5288\n",
      "Epoch 208/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3594939095361.2456 - val_loss: 3861129677448.1011\n",
      "Epoch 209/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3593934065106.0220 - val_loss: 3860087801421.0522\n",
      "Epoch 210/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3592977443259.2129 - val_loss: 3859017669884.0396\n",
      "Epoch 211/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3591994842696.1484 - val_loss: 3857915862060.6470\n",
      "Epoch 212/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3591040761587.0952 - val_loss: 3856955944562.4980\n",
      "Epoch 213/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3590052298955.5996 - val_loss: 3855942216385.7104\n",
      "Epoch 214/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3589088139562.1968 - val_loss: 3854926375571.6230\n",
      "Epoch 215/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3588101571604.8882 - val_loss: 3853922721319.6060\n",
      "Epoch 216/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3587126776125.8838 - val_loss: 3853014006333.2095\n",
      "Epoch 217/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3586181306370.4009 - val_loss: 3851993749259.1616\n",
      "Epoch 218/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3585146274825.3638 - val_loss: 3850935946970.1943\n",
      "Epoch 219/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3584237007525.0645 - val_loss: 3849941500572.2646\n",
      "Epoch 220/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3583282238603.0146 - val_loss: 3848933333461.5132\n",
      "Epoch 221/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3582266462871.6191 - val_loss: 3847928594869.8286\n",
      "Epoch 222/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3581294001319.3452 - val_loss: 3846919072426.6665\n",
      "Epoch 223/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3580350716793.7876 - val_loss: 3845932241109.1533\n",
      "Epoch 224/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3579390134329.1421 - val_loss: 3844952340937.9917\n",
      "Epoch 225/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3578417559621.6270 - val_loss: 3843972979434.0366\n",
      "Epoch 226/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3577466207453.1265 - val_loss: 3842994360118.3687\n",
      "Epoch 227/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3576523702905.8477 - val_loss: 3841949328039.7861\n",
      "Epoch 228/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3575530601862.1523 - val_loss: 3840884982105.6538\n",
      "Epoch 229/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3574586863240.4932 - val_loss: 3839845624651.9717\n",
      "Epoch 230/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3573653126810.7402 - val_loss: 3838932423721.7666\n",
      "Epoch 231/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3572754126704.4238 - val_loss: 3838030614810.2842\n",
      "Epoch 232/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3571758097945.5698 - val_loss: 3837055316722.6777\n",
      "Epoch 233/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3570829408540.7510 - val_loss: 3836013879077.0859\n",
      "Epoch 234/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3569846171671.5293 - val_loss: 3834995748650.8467\n",
      "Epoch 235/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3568964571688.2158 - val_loss: 3834104355446.8184\n",
      "Epoch 236/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3567982155117.6626 - val_loss: 3833193954406.2559\n",
      "Epoch 237/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 3567023812032.7354 - val_loss: 3832222363573.1084\n",
      "Epoch 238/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3566123521827.5938 - val_loss: 3831205536124.2192\n",
      "Epoch 239/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3565164910882.5132 - val_loss: 3830155198118.3462\n",
      "Epoch 240/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3564211676255.7974 - val_loss: 3829250609595.5894\n",
      "Epoch 241/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3563292778006.2085 - val_loss: 3828276690437.0410\n",
      "Epoch 242/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3562364785357.3999 - val_loss: 3827377508127.3247\n",
      "Epoch 243/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3561420190972.3389 - val_loss: 3826413617912.4390\n",
      "Epoch 244/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3560507753788.6836 - val_loss: 3825392472277.1533\n",
      "Epoch 245/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3559571216321.0957 - val_loss: 3824530487869.2095\n",
      "Epoch 246/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3558650623385.8398 - val_loss: 3823638549730.1152\n",
      "Epoch 247/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3557734033232.7314 - val_loss: 3822631079089.1475\n",
      "Epoch 248/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3556790590406.6177 - val_loss: 3821648258832.9229\n",
      "Epoch 249/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3555851935626.8345 - val_loss: 3820656008766.6499\n",
      "Epoch 250/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3554927213600.8931 - val_loss: 3819798371303.5161\n",
      "Epoch 251/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3554020294195.0200 - val_loss: 3818842906935.0884\n",
      "Epoch 252/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3553076983485.7939 - val_loss: 3817863769599.2798\n",
      "Epoch 253/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 58us/step - loss: 3552174938298.7930 - val_loss: 3816800298215.8765\n",
      "Epoch 254/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3551247154381.7603 - val_loss: 3815814099214.7622\n",
      "Epoch 255/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3550322052999.9531 - val_loss: 3814879653190.9312\n",
      "Epoch 256/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3549404343034.0576 - val_loss: 3813928629785.2041\n",
      "Epoch 257/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3548488438204.6538 - val_loss: 3813029521952.4053\n",
      "Epoch 258/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3547589004008.7705 - val_loss: 3812053264144.9229\n",
      "Epoch 259/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3546663978319.8916 - val_loss: 3811080426007.7637\n",
      "Epoch 260/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3545740430348.7246 - val_loss: 3810175098246.3008\n",
      "Epoch 261/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3544833033674.8193 - val_loss: 3809137890668.3770\n",
      "Epoch 262/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3543948680779.9897 - val_loss: 3808238326756.6357\n",
      "Epoch 263/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3542981962855.0005 - val_loss: 3807262747352.7539\n",
      "Epoch 264/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3542143641524.6108 - val_loss: 3806308061304.9790\n",
      "Epoch 265/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3541175257679.1108 - val_loss: 3805334678970.1489\n",
      "Epoch 266/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3540332036314.9653 - val_loss: 3804384698978.6553\n",
      "Epoch 267/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3539433110979.6162 - val_loss: 3803376058828.8721\n",
      "Epoch 268/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3538499598739.8374 - val_loss: 3802416338583.9438\n",
      "Epoch 269/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3537575033354.9243 - val_loss: 3801528488195.2407\n",
      "Epoch 270/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3536681680315.9336 - val_loss: 3800643491894.7285\n",
      "Epoch 271/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3535808918058.1367 - val_loss: 3799812056295.8765\n",
      "Epoch 272/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3534874458104.7974 - val_loss: 3798805792402.1826\n",
      "Epoch 273/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3533968440398.9907 - val_loss: 3797918469619.7583\n",
      "Epoch 274/800\n",
      "4265/4265 [==============================] - 0s 51us/step - loss: 3533080364022.1562 - val_loss: 3796989747257.6089\n",
      "Epoch 275/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3532205120677.4243 - val_loss: 3796088742475.6118\n",
      "Epoch 276/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3531324381436.0981 - val_loss: 3795182038734.6724\n",
      "Epoch 277/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3530406316085.3008 - val_loss: 3794278149262.5825\n",
      "Epoch 278/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3529509025999.9214 - val_loss: 3793316098848.7651\n",
      "Epoch 279/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3528631875088.9268 - val_loss: 3792469415760.2925\n",
      "Epoch 280/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3527750574373.8745 - val_loss: 3791514892308.1631\n",
      "Epoch 281/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3526859023674.7627 - val_loss: 3790559899135.2798\n",
      "Epoch 282/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3526008121829.4697 - val_loss: 3789651017128.8662\n",
      "Epoch 283/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3525112177068.3271 - val_loss: 3788810900176.1123\n",
      "Epoch 284/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3524197986716.0010 - val_loss: 3787856398845.8398\n",
      "Epoch 285/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3523318872275.7627 - val_loss: 3786891644035.0605\n",
      "Epoch 286/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3522428114954.0840 - val_loss: 3785907992734.4248\n",
      "Epoch 287/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3521557605659.5508 - val_loss: 3784972602476.0171\n",
      "Epoch 288/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3520676527997.3887 - val_loss: 3784059631244.4219\n",
      "Epoch 289/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3519801172204.2524 - val_loss: 3783183453084.6245\n",
      "Epoch 290/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3518953367653.3198 - val_loss: 3782313123943.6963\n",
      "Epoch 291/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3518054245448.7480 - val_loss: 3781438768256.1802\n",
      "Epoch 292/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3517154452548.9067 - val_loss: 3780607312747.6567\n",
      "Epoch 293/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3516312044138.4814 - val_loss: 3779743729298.1826\n",
      "Epoch 294/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3515416722161.4141 - val_loss: 3778808706631.2910\n",
      "Epoch 295/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3514616652219.6934 - val_loss: 3777887164286.3799\n",
      "Epoch 296/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3513707116352.4053 - val_loss: 3776960685817.8789\n",
      "Epoch 297/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3512860282318.1807 - val_loss: 3776080507657.7217\n",
      "Epoch 298/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3511995947636.5659 - val_loss: 3775080283399.5610\n",
      "Epoch 299/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3511093789786.0352 - val_loss: 3774140610718.4248\n",
      "Epoch 300/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3510256624716.5898 - val_loss: 3773342522343.5161\n",
      "Epoch 301/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3509345250153.4614 - val_loss: 3772471142783.1001\n",
      "Epoch 302/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3508486329959.3604 - val_loss: 3771552886271.2798\n",
      "Epoch 303/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3507646629560.9922 - val_loss: 3770664745717.5586\n",
      "Epoch 304/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3506785983396.0439 - val_loss: 3769803388095.5498\n",
      "Epoch 305/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3505951794168.0767 - val_loss: 3768942421661.7046\n",
      "Epoch 306/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3505086834889.1982 - val_loss: 3768051710916.9507\n",
      "Epoch 307/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3504204536121.5625 - val_loss: 3767102285446.6611\n",
      "Epoch 308/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3503344755984.5063 - val_loss: 3766173150143.1899\n",
      "Epoch 309/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3502501660867.4360 - val_loss: 3765278936771.1504\n",
      "Epoch 310/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3501647051758.9531 - val_loss: 3764309609003.9268\n",
      "Epoch 311/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3500797785716.8057 - val_loss: 3763410180772.9058\n",
      "Epoch 312/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3499915191030.6963 - val_loss: 3762544655761.8228\n",
      "Epoch 313/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3499124334267.3931 - val_loss: 3761633699093.9634\n",
      "Epoch 314/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3498250001850.0127 - val_loss: 3760749130851.3755\n",
      "Epoch 315/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3497408437985.8081 - val_loss: 3759954067640.3486\n",
      "Epoch 316/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 63us/step - loss: 3496527930340.8696 - val_loss: 3759143564479.5498\n",
      "Epoch 317/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3495687446061.2578 - val_loss: 3758234116891.0044\n",
      "Epoch 318/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3494838712020.8433 - val_loss: 3757319384022.2334\n",
      "Epoch 319/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3494022307342.5254 - val_loss: 3756374427904.3599\n",
      "Epoch 320/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3493166783034.9429 - val_loss: 3755525328740.4556\n",
      "Epoch 321/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3492327679513.5698 - val_loss: 3754575691637.7383\n",
      "Epoch 322/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3491507745141.5859 - val_loss: 3753738092520.9565\n",
      "Epoch 323/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3490632360521.8286 - val_loss: 3752915253844.2529\n",
      "Epoch 324/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3489759055045.1172 - val_loss: 3751999947650.7002\n",
      "Epoch 325/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3488926475866.8755 - val_loss: 3751134095973.5356\n",
      "Epoch 326/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3488118088062.2295 - val_loss: 3750308157656.0337\n",
      "Epoch 327/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3487279212142.0830 - val_loss: 3749520095275.2065\n",
      "Epoch 328/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3486420987904.9604 - val_loss: 3748723426386.0928\n",
      "Epoch 329/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3485587341096.8760 - val_loss: 3747803340765.4346\n",
      "Epoch 330/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3484768022102.5537 - val_loss: 3746864306299.8594\n",
      "Epoch 331/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3483933265500.3159 - val_loss: 3746002007024.1577\n",
      "Epoch 332/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3483081238863.6509 - val_loss: 3745133511785.1362\n",
      "Epoch 333/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3482229297170.2471 - val_loss: 3744160282091.1167\n",
      "Epoch 334/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3481417367624.9883 - val_loss: 3743276982537.0015\n",
      "Epoch 335/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3480623703831.1089 - val_loss: 3742443680222.1548\n",
      "Epoch 336/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3479804738685.0889 - val_loss: 3741540834532.9956\n",
      "Epoch 337/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3478942105880.4297 - val_loss: 3740668676283.2295\n",
      "Epoch 338/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3478114887531.6221 - val_loss: 3739851524398.4473\n",
      "Epoch 339/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3477307639392.6377 - val_loss: 3738996536027.6343\n",
      "Epoch 340/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3476465647431.6079 - val_loss: 3738176217767.7861\n",
      "Epoch 341/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3475652560519.0527 - val_loss: 3737275658741.1982\n",
      "Epoch 342/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3474813915088.9414 - val_loss: 3736384019615.8647\n",
      "Epoch 343/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3474001638825.2061 - val_loss: 3735570438567.4263\n",
      "Epoch 344/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3473154013503.3247 - val_loss: 3734729032279.1338\n",
      "Epoch 345/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3472327719511.7544 - val_loss: 3733824841581.0972\n",
      "Epoch 346/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3471547097704.5610 - val_loss: 3732931378139.9941\n",
      "Epoch 347/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3470701787762.6450 - val_loss: 3732112918116.0957\n",
      "Epoch 348/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3469905963539.5674 - val_loss: 3731255955738.2842\n",
      "Epoch 349/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3469074344552.0806 - val_loss: 3730413567239.5610\n",
      "Epoch 350/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3468242546519.9346 - val_loss: 3729556342349.0522\n",
      "Epoch 351/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3467420315791.0957 - val_loss: 3728778652906.7568\n",
      "Epoch 352/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3466603891505.7593 - val_loss: 3727954930830.5825\n",
      "Epoch 353/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 3465826429686.2163 - val_loss: 3727063152706.2505\n",
      "Epoch 354/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3465006546083.5039 - val_loss: 3726183884491.7920\n",
      "Epoch 355/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3464189556206.3535 - val_loss: 3725372958434.8354\n",
      "Epoch 356/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3463344204561.1069 - val_loss: 3724484232020.6133\n",
      "Epoch 357/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3462573574834.9897 - val_loss: 3723759981978.4644\n",
      "Epoch 358/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3461715704200.3130 - val_loss: 3722894657907.5781\n",
      "Epoch 359/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3460901079704.0991 - val_loss: 3722071658110.0195\n",
      "Epoch 360/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 3460111675857.7817 - val_loss: 3721223902480.2026\n",
      "Epoch 361/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3459271726776.0317 - val_loss: 3720303905346.9707\n",
      "Epoch 362/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3458500344177.2642 - val_loss: 3719368512138.9819\n",
      "Epoch 363/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3457698860194.5435 - val_loss: 3718564602766.2222\n",
      "Epoch 364/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3456865944931.0986 - val_loss: 3717769272986.8242\n",
      "Epoch 365/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3456078691157.2935 - val_loss: 3716993283240.5063\n",
      "Epoch 366/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3455264371706.9580 - val_loss: 3716119527424.0000\n",
      "Epoch 367/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3454461784998.4448 - val_loss: 3715327277375.7300\n",
      "Epoch 368/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3453679660826.4702 - val_loss: 3714456082532.8159\n",
      "Epoch 369/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3452880895519.8125 - val_loss: 3713734250778.2842\n",
      "Epoch 370/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3452098895505.1367 - val_loss: 3712770259171.5557\n",
      "Epoch 371/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3451308580762.2002 - val_loss: 3711918981742.1772\n",
      "Epoch 372/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3450507297213.6138 - val_loss: 3711103044347.3193\n",
      "Epoch 373/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3449684582148.3818 - val_loss: 3710273929699.9155\n",
      "Epoch 374/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3448922244285.9146 - val_loss: 3709437970181.4009\n",
      "Epoch 375/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3448100136608.0225 - val_loss: 3708608847422.6499\n",
      "Epoch 376/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3447359377652.8955 - val_loss: 3707681651468.6021\n",
      "Epoch 377/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3446552954545.3091 - val_loss: 3706863938057.3613\n",
      "Epoch 378/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3445741677256.1182 - val_loss: 3706109324661.0181\n",
      "Epoch 379/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 53us/step - loss: 3444940544663.1392 - val_loss: 3705327406625.8452\n",
      "Epoch 380/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3444151657053.0366 - val_loss: 3704576524818.0029\n",
      "Epoch 381/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3443362853771.5547 - val_loss: 3703728378737.4175\n",
      "Epoch 382/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3442602149745.3843 - val_loss: 3702913512777.8115\n",
      "Epoch 383/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3441796917041.9995 - val_loss: 3702079212767.2349\n",
      "Epoch 384/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3441011433124.3442 - val_loss: 3701234782789.8511\n",
      "Epoch 385/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3440248563997.7119 - val_loss: 3700399533014.2334\n",
      "Epoch 386/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3439438742391.3867 - val_loss: 3699570823565.5020\n",
      "Epoch 387/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3438689001055.1968 - val_loss: 3698781304091.7241\n",
      "Epoch 388/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3437904658585.6597 - val_loss: 3697961347524.2305\n",
      "Epoch 389/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3437092754179.1812 - val_loss: 3697157709144.2139\n",
      "Epoch 390/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3436317444276.0703 - val_loss: 3696344692203.1167\n",
      "Epoch 391/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3435560645980.3765 - val_loss: 3695469301212.7144\n",
      "Epoch 392/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3434752755911.2778 - val_loss: 3694625228134.6162\n",
      "Epoch 393/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3433976995292.5864 - val_loss: 3693835594733.2769\n",
      "Epoch 394/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3433197639090.8101 - val_loss: 3693004531519.0098\n",
      "Epoch 395/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3432438194907.8057 - val_loss: 3692192562785.2153\n",
      "Epoch 396/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3431657114993.9844 - val_loss: 3691314322130.9932\n",
      "Epoch 397/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3430882947522.4160 - val_loss: 3690523722741.9185\n",
      "Epoch 398/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3430101909296.5591 - val_loss: 3689800245489.9580\n",
      "Epoch 399/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3429367387954.9600 - val_loss: 3688949561295.0322\n",
      "Epoch 400/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3428563447255.7842 - val_loss: 3688122253634.6104\n",
      "Epoch 401/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3427800843530.7441 - val_loss: 3687296556814.0420\n",
      "Epoch 402/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3427022001859.5562 - val_loss: 3686487598864.9229\n",
      "Epoch 403/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3426262768313.7129 - val_loss: 3685696794848.6753\n",
      "Epoch 404/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3425493918247.2554 - val_loss: 3684876541848.3037\n",
      "Epoch 405/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 3424712249247.0024 - val_loss: 3684134531642.3291\n",
      "Epoch 406/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 3423951295246.7056 - val_loss: 3683375106373.4907\n",
      "Epoch 407/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3423177079157.1060 - val_loss: 3682599577867.8818\n",
      "Epoch 408/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3422411923600.0562 - val_loss: 3681809184949.4683\n",
      "Epoch 409/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3421641217412.7119 - val_loss: 3681020216678.6162\n",
      "Epoch 410/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3420844535059.8677 - val_loss: 3680255638491.9941\n",
      "Epoch 411/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3420097980294.5122 - val_loss: 3679419205934.4473\n",
      "Epoch 412/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3419309395456.1201 - val_loss: 3678595299509.4683\n",
      "Epoch 413/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3418590438522.6880 - val_loss: 3677780978853.6260\n",
      "Epoch 414/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3417812929621.7134 - val_loss: 3676922305350.2109\n",
      "Epoch 415/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3417036319447.4844 - val_loss: 3676091783384.0337\n",
      "Epoch 416/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3416284998086.7373 - val_loss: 3675275984250.7793\n",
      "Epoch 417/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3415503048103.7656 - val_loss: 3674424244843.2969\n",
      "Epoch 418/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3414788154064.2812 - val_loss: 3673697528582.8413\n",
      "Epoch 419/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3414010313011.8003 - val_loss: 3672881313558.6836\n",
      "Epoch 420/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3413252639222.7563 - val_loss: 3672057539496.1465\n",
      "Epoch 421/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3412542132791.5815 - val_loss: 3671273366875.0942\n",
      "Epoch 422/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3411748903855.5684 - val_loss: 3670474014236.0845\n",
      "Epoch 423/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3410988310894.8633 - val_loss: 3669740783088.8774\n",
      "Epoch 424/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3410242822714.4629 - val_loss: 3668927495892.4331\n",
      "Epoch 425/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3409507840331.3296 - val_loss: 3668106457197.4570\n",
      "Epoch 426/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3408760599504.7017 - val_loss: 3667342282366.0195\n",
      "Epoch 427/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3407963090399.4673 - val_loss: 3666537187999.1450\n",
      "Epoch 428/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3407228753388.4326 - val_loss: 3665825201945.5640\n",
      "Epoch 429/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3406441492546.2656 - val_loss: 3665098331200.8101\n",
      "Epoch 430/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3405692577931.7349 - val_loss: 3664365072769.9805\n",
      "Epoch 431/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3404963588350.9795 - val_loss: 3663617123582.9199\n",
      "Epoch 432/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3404232721413.5225 - val_loss: 3662879999726.3574\n",
      "Epoch 433/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3403469489249.7183 - val_loss: 3662072124273.4175\n",
      "Epoch 434/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3402722561643.6821 - val_loss: 3661270291036.8945\n",
      "Epoch 435/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3401993041812.9180 - val_loss: 3660482646021.7607\n",
      "Epoch 436/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3401239098458.9951 - val_loss: 3659670941275.4541\n",
      "Epoch 437/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3400481746556.9688 - val_loss: 3658897127951.1226\n",
      "Epoch 438/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3399740965474.7988 - val_loss: 3658058137000.8662\n",
      "Epoch 439/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3398991545195.8618 - val_loss: 3657235446171.9043\n",
      "Epoch 440/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3398261434634.5039 - val_loss: 3656489806137.9692\n",
      "Epoch 441/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3397502139486.3569 - val_loss: 3655623160654.8525\n",
      "Epoch 442/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 56us/step - loss: 3396787825113.9453 - val_loss: 3654870771242.4868\n",
      "Epoch 443/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3396066068556.3501 - val_loss: 3654144408563.0381\n",
      "Epoch 444/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3395304366969.3076 - val_loss: 3653314997851.4541\n",
      "Epoch 445/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3394588031045.6270 - val_loss: 3652555875283.3530\n",
      "Epoch 446/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3393864018635.9600 - val_loss: 3651727126015.2798\n",
      "Epoch 447/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3393118131308.2822 - val_loss: 3650921150461.1196\n",
      "Epoch 448/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3392357670064.9492 - val_loss: 3650196545363.1729\n",
      "Epoch 449/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3391660577539.1812 - val_loss: 3649409985425.1025\n",
      "Epoch 450/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3390899647817.1685 - val_loss: 3648628086562.2056\n",
      "Epoch 451/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3390157834385.4971 - val_loss: 3647848822412.4219\n",
      "Epoch 452/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3389417419874.6787 - val_loss: 3647186471905.7554\n",
      "Epoch 453/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3388665911724.8076 - val_loss: 3646503685596.7144\n",
      "Epoch 454/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3387939571611.6406 - val_loss: 3645774495006.6050\n",
      "Epoch 455/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3387197674711.8442 - val_loss: 3644994569782.0083\n",
      "Epoch 456/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3386486305756.2261 - val_loss: 3644195925411.1055\n",
      "Epoch 457/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3385733619648.6152 - val_loss: 3643394561895.3359\n",
      "Epoch 458/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3385029598530.9263 - val_loss: 3642664439606.3687\n",
      "Epoch 459/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3384285831773.0366 - val_loss: 3641898721038.0420\n",
      "Epoch 460/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3383560773307.3931 - val_loss: 3641196688345.1138\n",
      "Epoch 461/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3382850930871.9121 - val_loss: 3640423915854.1323\n",
      "Epoch 462/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3382103310197.7061 - val_loss: 3639725215081.4966\n",
      "Epoch 463/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3381387415216.8291 - val_loss: 3638960841890.7456\n",
      "Epoch 464/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3380675125315.2266 - val_loss: 3638167636915.6680\n",
      "Epoch 465/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3379921682432.0000 - val_loss: 3637399342625.8452\n",
      "Epoch 466/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3379196850153.9116 - val_loss: 3636662934780.0396\n",
      "Epoch 467/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3378508811399.6528 - val_loss: 3635913067628.0171\n",
      "Epoch 468/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3377744177364.7231 - val_loss: 3635168268224.6299\n",
      "Epoch 469/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 3377023210435.2563 - val_loss: 3634397574819.4653\n",
      "Epoch 470/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3376327629678.2632 - val_loss: 3633582589033.1362\n",
      "Epoch 471/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3375608678522.4478 - val_loss: 3632863105882.3740\n",
      "Epoch 472/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3374878779243.6221 - val_loss: 3632130025901.1870\n",
      "Epoch 473/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3374181536837.8672 - val_loss: 3631426001872.4727\n",
      "Epoch 474/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3373448818235.6631 - val_loss: 3630679594947.5107\n",
      "Epoch 475/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3372714894201.3076 - val_loss: 3629919175669.9185\n",
      "Epoch 476/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3371986780912.9341 - val_loss: 3629203201782.9985\n",
      "Epoch 477/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 3371269902446.4434 - val_loss: 3628498404595.3979\n",
      "Epoch 478/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3370534925933.1226 - val_loss: 3627735518730.8018\n",
      "Epoch 479/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3369869466074.9053 - val_loss: 3626810522795.3867\n",
      "Epoch 480/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3369116298888.4932 - val_loss: 3626001376073.0913\n",
      "Epoch 481/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3368432907735.3042 - val_loss: 3625324876426.9819\n",
      "Epoch 482/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3367673895628.9199 - val_loss: 3624484414373.2656\n",
      "Epoch 483/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3366962493974.4487 - val_loss: 3623694858767.1226\n",
      "Epoch 484/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3366265186570.5039 - val_loss: 3622939441978.6890\n",
      "Epoch 485/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3365509124429.2505 - val_loss: 3622207400249.9692\n",
      "Epoch 486/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3364828039405.6929 - val_loss: 3621543414200.7090\n",
      "Epoch 487/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3364128974868.6479 - val_loss: 3620879567519.1450\n",
      "Epoch 488/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3363415042310.6626 - val_loss: 3620151902968.4390\n",
      "Epoch 489/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3362702863874.0405 - val_loss: 3619363825102.3120\n",
      "Epoch 490/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 3361983888624.3340 - val_loss: 3618662837298.4077\n",
      "Epoch 491/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3361305062956.2974 - val_loss: 3617821765050.1489\n",
      "Epoch 492/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3360612634688.3452 - val_loss: 3617019789496.3486\n",
      "Epoch 493/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3359876948357.9126 - val_loss: 3616324652554.8018\n",
      "Epoch 494/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3359166660830.8071 - val_loss: 3615517465091.6006\n",
      "Epoch 495/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3358481541278.9419 - val_loss: 3614816122600.5962\n",
      "Epoch 496/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3357747313550.9155 - val_loss: 3614140569401.2490\n",
      "Epoch 497/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3357068264952.9170 - val_loss: 3613420842172.6694\n",
      "Epoch 498/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3356339172419.9468 - val_loss: 3612681467785.9014\n",
      "Epoch 499/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3355658546409.6113 - val_loss: 3611920706725.6260\n",
      "Epoch 500/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3354984591579.9263 - val_loss: 3611103807767.4038\n",
      "Epoch 501/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 3354253845480.7109 - val_loss: 3610372013803.4766\n",
      "Epoch 502/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3353546739361.4634 - val_loss: 3609659760812.8271\n",
      "Epoch 503/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3352863084957.9219 - val_loss: 3608913122714.4644\n",
      "Epoch 504/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3352153297348.3369 - val_loss: 3608227508494.7622\n",
      "Epoch 505/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 63us/step - loss: 3351447348289.0654 - val_loss: 3607484736190.8296\n",
      "Epoch 506/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3350766245871.1934 - val_loss: 3606786247419.3193\n",
      "Epoch 507/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3350083899175.9155 - val_loss: 3606012511521.4854\n",
      "Epoch 508/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3349360683892.9854 - val_loss: 3605225017303.6738\n",
      "Epoch 509/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3348624714213.4697 - val_loss: 3604581770126.2222\n",
      "Epoch 510/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3347962965705.7988 - val_loss: 3603864716324.0059\n",
      "Epoch 511/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3347252059635.6348 - val_loss: 3603137291795.4429\n",
      "Epoch 512/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3346567001578.5112 - val_loss: 3602449879437.5020\n",
      "Epoch 513/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3345907270995.7329 - val_loss: 3601718233321.3164\n",
      "Epoch 514/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3345212776000.4653 - val_loss: 3601055967615.1001\n",
      "Epoch 515/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3344474903087.8989 - val_loss: 3600228488878.9873\n",
      "Epoch 516/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3343809738779.3706 - val_loss: 3599478976045.3672\n",
      "Epoch 517/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 3343100038694.5347 - val_loss: 3598633865452.1968\n",
      "Epoch 518/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3342416358906.3579 - val_loss: 3597923720674.4756\n",
      "Epoch 519/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3341768033953.9434 - val_loss: 3597264382427.2744\n",
      "Epoch 520/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 3341030380836.6743 - val_loss: 3596536286131.6680\n",
      "Epoch 521/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3340336690589.6816 - val_loss: 3595844685000.1914\n",
      "Epoch 522/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3339649876628.7378 - val_loss: 3595125469155.1953\n",
      "Epoch 523/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 3338950818115.1660 - val_loss: 3594405311287.8086\n",
      "Epoch 524/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3338274740779.5771 - val_loss: 3593633458697.3613\n",
      "Epoch 525/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3337552403786.6094 - val_loss: 3592964073071.6177\n",
      "Epoch 526/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3336890531859.2075 - val_loss: 3592197850124.9619\n",
      "Epoch 527/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3336225288077.7153 - val_loss: 3591461280789.6035\n",
      "Epoch 528/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3335526795862.7939 - val_loss: 3590731684632.1235\n",
      "Epoch 529/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3334845228992.1353 - val_loss: 3590020086079.7300\n",
      "Epoch 530/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3334145827079.1426 - val_loss: 3589195660159.8198\n",
      "Epoch 531/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3333461877771.2847 - val_loss: 3588471312044.1069\n",
      "Epoch 532/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3332767966192.3940 - val_loss: 3587736872306.1377\n",
      "Epoch 533/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3332094635714.1162 - val_loss: 3587102643840.8999\n",
      "Epoch 534/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3331415298726.9849 - val_loss: 3586392915691.4766\n",
      "Epoch 535/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3330767473046.7188 - val_loss: 3585634596884.1631\n",
      "Epoch 536/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3330029952876.1021 - val_loss: 3585045932069.4458\n",
      "Epoch 537/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3329361832456.0430 - val_loss: 3584254995322.0591\n",
      "Epoch 538/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3328678176638.8296 - val_loss: 3583594994893.9521\n",
      "Epoch 539/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3328024502227.8228 - val_loss: 3582892922381.6821\n",
      "Epoch 540/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3327340973272.3242 - val_loss: 3582215772352.9902\n",
      "Epoch 541/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3326688059908.6816 - val_loss: 3581455139230.7847\n",
      "Epoch 542/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3325970570674.3296 - val_loss: 3580742919422.9199\n",
      "Epoch 543/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3325293230714.3276 - val_loss: 3580053659889.9580\n",
      "Epoch 544/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3324637414159.6660 - val_loss: 3579383817447.8765\n",
      "Epoch 545/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3323974933922.9634 - val_loss: 3578654537264.2476\n",
      "Epoch 546/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3323286702663.4277 - val_loss: 3577900869208.5737\n",
      "Epoch 547/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3322587457299.2676 - val_loss: 3577252320896.8999\n",
      "Epoch 548/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3321938747459.2266 - val_loss: 3576510641781.3784\n",
      "Epoch 549/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3321244850139.9858 - val_loss: 3575858598216.3711\n",
      "Epoch 550/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3320581035335.2480 - val_loss: 3575121995329.5303\n",
      "Epoch 551/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3319877916140.4326 - val_loss: 3574427422495.3247\n",
      "Epoch 552/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3319227669454.3003 - val_loss: 3573696291081.0015\n",
      "Epoch 553/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3318556867236.5845 - val_loss: 3572955303642.1943\n",
      "Epoch 554/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3317862317907.1323 - val_loss: 3572225277214.6050\n",
      "Epoch 555/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3317204058834.2021 - val_loss: 3571493419097.2939\n",
      "Epoch 556/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3316513644987.6934 - val_loss: 3570789347137.8901\n",
      "Epoch 557/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3315880914069.0981 - val_loss: 3570115693099.9268\n",
      "Epoch 558/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3315197136184.3623 - val_loss: 3569377884940.6021\n",
      "Epoch 559/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3314505510329.0522 - val_loss: 3568676043233.0352\n",
      "Epoch 560/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 3313872807929.7578 - val_loss: 3568002151753.8115\n",
      "Epoch 561/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3313218167365.9873 - val_loss: 3567237125599.5947\n",
      "Epoch 562/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3312536048456.3281 - val_loss: 3566506417081.4292\n",
      "Epoch 563/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3311858654346.5337 - val_loss: 3565841538415.2573\n",
      "Epoch 564/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3311204001551.4258 - val_loss: 3565167725837.3223\n",
      "Epoch 565/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3310528761632.2324 - val_loss: 3564469950495.6851\n",
      "Epoch 566/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3309833051985.2114 - val_loss: 3563818135891.8931\n",
      "Epoch 567/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3309204941562.0576 - val_loss: 3563034314639.6626\n",
      "Epoch 568/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 58us/step - loss: 3308556301452.9351 - val_loss: 3562349751208.1465\n",
      "Epoch 569/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3307883002905.2095 - val_loss: 3561581067942.3462\n",
      "Epoch 570/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3307221677146.9951 - val_loss: 3560941380909.0068\n",
      "Epoch 571/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3306547648621.9629 - val_loss: 3560214692300.8721\n",
      "Epoch 572/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3305874072041.7915 - val_loss: 3559477461153.3052\n",
      "Epoch 573/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3305205895443.6274 - val_loss: 3558747094049.1250\n",
      "Epoch 574/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3304578892786.3145 - val_loss: 3558060005407.6851\n",
      "Epoch 575/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3303916544605.0366 - val_loss: 3557382286478.5825\n",
      "Epoch 576/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3303269093332.7832 - val_loss: 3556700766277.1309\n",
      "Epoch 577/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3302599168206.4805 - val_loss: 3555958592369.4175\n",
      "Epoch 578/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3301932332076.8975 - val_loss: 3555274280389.6709\n",
      "Epoch 579/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3301292024797.1860 - val_loss: 3554610873564.3545\n",
      "Epoch 580/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3300621113357.9258 - val_loss: 3553960213721.4741\n",
      "Epoch 581/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 3299981423730.0444 - val_loss: 3553218970775.2236\n",
      "Epoch 582/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3299294816595.4927 - val_loss: 3552501649667.2407\n",
      "Epoch 583/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 3298648006669.4453 - val_loss: 3551832926222.4023\n",
      "Epoch 584/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3297976110638.2183 - val_loss: 3551073222586.8691\n",
      "Epoch 585/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3297347014025.0337 - val_loss: 3550437859270.3911\n",
      "Epoch 586/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3296694759063.8594 - val_loss: 3549819624907.4316\n",
      "Epoch 587/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3296020242453.1284 - val_loss: 3549123245086.2446\n",
      "Epoch 588/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 3295397079882.7290 - val_loss: 3548462989888.0898\n",
      "Epoch 589/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3294701456346.7856 - val_loss: 3547776933968.6528\n",
      "Epoch 590/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3294066002388.1831 - val_loss: 3547057985429.4233\n",
      "Epoch 591/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3293406856451.0615 - val_loss: 3546274855485.2095\n",
      "Epoch 592/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3292809134110.7319 - val_loss: 3545622970091.4766\n",
      "Epoch 593/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3292098866159.9141 - val_loss: 3544949422975.8198\n",
      "Epoch 594/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3291462262301.8921 - val_loss: 3544317623121.7329\n",
      "Epoch 595/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 3290822516864.6904 - val_loss: 3543542305511.1562\n",
      "Epoch 596/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 3290172411545.7803 - val_loss: 3542853188295.4712\n",
      "Epoch 597/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3289532511280.7388 - val_loss: 3542195716026.8691\n",
      "Epoch 598/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3288866484017.5195 - val_loss: 3541598076743.6514\n",
      "Epoch 599/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3288219308687.9365 - val_loss: 3540885449156.2305\n",
      "Epoch 600/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3287574023130.7856 - val_loss: 3540182016518.4810\n",
      "Epoch 601/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3286914547439.2534 - val_loss: 3539560410047.1899\n",
      "Epoch 602/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3286261556262.1748 - val_loss: 3538826558750.6050\n",
      "Epoch 603/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 3285623624812.2822 - val_loss: 3538124039745.5303\n",
      "Epoch 604/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 3284963136869.9800 - val_loss: 3537471145226.4414\n",
      "Epoch 605/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 3284348036595.1548 - val_loss: 3536791736187.4995\n",
      "Epoch 606/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3283679926808.3696 - val_loss: 3536136073855.4600\n",
      "Epoch 607/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3283039623032.1069 - val_loss: 3535405745464.5288\n",
      "Epoch 608/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3282399581522.7720 - val_loss: 3534696940533.9185\n",
      "Epoch 609/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3281770424367.5386 - val_loss: 3534021524121.3838\n",
      "Epoch 610/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3281130437929.9565 - val_loss: 3533346313810.8130\n",
      "Epoch 611/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 3280490339689.3413 - val_loss: 3532755477735.8765\n",
      "Epoch 612/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 3279831905997.1602 - val_loss: 3532012449555.8032\n",
      "Epoch 613/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3279194712730.0200 - val_loss: 3531333686475.0718\n",
      "Epoch 614/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3278590443335.6079 - val_loss: 3530648477085.3447\n",
      "Epoch 615/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3277889691396.3818 - val_loss: 3529995386419.1279\n",
      "Epoch 616/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3277257315017.5586 - val_loss: 3529368584998.5259\n",
      "Epoch 617/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3276660117503.0396 - val_loss: 3528713281777.9580\n",
      "Epoch 618/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3276000205232.6489 - val_loss: 3528100792829.8398\n",
      "Epoch 619/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3275353348286.6348 - val_loss: 3527447772216.1689\n",
      "Epoch 620/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3274711866737.0244 - val_loss: 3526797110529.8003\n",
      "Epoch 621/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3274070916666.7026 - val_loss: 3526081867770.2393\n",
      "Epoch 622/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 3273460881272.1069 - val_loss: 3525383385266.5879\n",
      "Epoch 623/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3272800214100.7529 - val_loss: 3524713858798.3574\n",
      "Epoch 624/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3272207502767.4487 - val_loss: 3523985644888.2139\n",
      "Epoch 625/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3271538403287.1841 - val_loss: 3523315734974.4697\n",
      "Epoch 626/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3270928782897.5791 - val_loss: 3522639514063.7524\n",
      "Epoch 627/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3270267586275.2490 - val_loss: 3522029331687.8765\n",
      "Epoch 628/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3269651215477.8857 - val_loss: 3521380430532.5908\n",
      "Epoch 629/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3269014387061.1060 - val_loss: 3520738724511.1450\n",
      "Epoch 630/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3268378271350.2461 - val_loss: 3520045440275.0830\n",
      "Epoch 631/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 64us/step - loss: 3267730521916.0835 - val_loss: 3519372779139.7808\n",
      "Epoch 632/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3267115769404.6235 - val_loss: 3518726450756.4106\n",
      "Epoch 633/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3266515412132.9443 - val_loss: 3518071047987.4883\n",
      "Epoch 634/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3265872258731.5469 - val_loss: 3517406649365.6035\n",
      "Epoch 635/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3265220668780.2222 - val_loss: 3516696038185.4062\n",
      "Epoch 636/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3264592694499.8486 - val_loss: 3516056263402.0366\n",
      "Epoch 637/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 3263967283698.9150 - val_loss: 3515354124892.8945\n",
      "Epoch 638/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3263349221166.1582 - val_loss: 3514692242263.4937\n",
      "Epoch 639/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3262744142966.8467 - val_loss: 3513982798748.6245\n",
      "Epoch 640/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3262115561975.2368 - val_loss: 3513283714530.4756\n",
      "Epoch 641/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3261458378952.7183 - val_loss: 3512590963431.1562\n",
      "Epoch 642/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3260814868107.3745 - val_loss: 3511886311510.4136\n",
      "Epoch 643/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3260192179793.2720 - val_loss: 3511341890040.0786\n",
      "Epoch 644/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3259562682864.9941 - val_loss: 3510634495377.8228\n",
      "Epoch 645/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3258922018294.9961 - val_loss: 3509923932128.3149\n",
      "Epoch 646/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3258298523204.3066 - val_loss: 3509224561800.8213\n",
      "Epoch 647/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3257684584845.1152 - val_loss: 3508602715307.3867\n",
      "Epoch 648/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3257058540626.8325 - val_loss: 3507923216306.2280\n",
      "Epoch 649/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3256417483701.3311 - val_loss: 3507302246575.7075\n",
      "Epoch 650/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3255800042723.3687 - val_loss: 3506681235551.0547\n",
      "Epoch 651/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3255192669119.8950 - val_loss: 3505962697003.5669\n",
      "Epoch 652/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3254552251266.4312 - val_loss: 3505298988952.3037\n",
      "Epoch 653/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3253964101300.1904 - val_loss: 3504630445063.2012\n",
      "Epoch 654/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3253324352052.2202 - val_loss: 3503993336700.9395\n",
      "Epoch 655/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3252682452485.8818 - val_loss: 3503374912090.0142\n",
      "Epoch 656/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3252083502617.0898 - val_loss: 3502765067441.1475\n",
      "Epoch 657/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3251462122134.1787 - val_loss: 3502148927649.3052\n",
      "Epoch 658/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3250847366733.9106 - val_loss: 3501463514738.4980\n",
      "Epoch 659/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 3250217565190.0024 - val_loss: 3500825738664.8662\n",
      "Epoch 660/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3249602215309.8354 - val_loss: 3500227531956.0283\n",
      "Epoch 661/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3248959898679.4619 - val_loss: 3499555272701.1196\n",
      "Epoch 662/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3248349640570.0278 - val_loss: 3498891353386.1265\n",
      "Epoch 663/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3247747963911.9233 - val_loss: 3498296008508.1294\n",
      "Epoch 664/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3247126049183.8423 - val_loss: 3497588679474.0479\n",
      "Epoch 665/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3246491105192.1260 - val_loss: 3496974404186.0142\n",
      "Epoch 666/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3245897902638.4580 - val_loss: 3496293742312.5962\n",
      "Epoch 667/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3245292318528.8853 - val_loss: 3495646468588.5571\n",
      "Epoch 668/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3244639676160.0601 - val_loss: 3494968983413.7383\n",
      "Epoch 669/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3244030180419.4663 - val_loss: 3494317166966.4585\n",
      "Epoch 670/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3243407733428.4307 - val_loss: 3493654498642.4531\n",
      "Epoch 671/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3242807318795.4648 - val_loss: 3492983404840.6865\n",
      "Epoch 672/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3242237327387.3706 - val_loss: 3492282200611.2856\n",
      "Epoch 673/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3241574451568.3037 - val_loss: 3491665744476.8945\n",
      "Epoch 674/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3240978800411.4307 - val_loss: 3491056048044.4668\n",
      "Epoch 675/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3240390642147.5488 - val_loss: 3490400383868.9395\n",
      "Epoch 676/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3239740891045.7246 - val_loss: 3489693114195.1729\n",
      "Epoch 677/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3239164935784.8013 - val_loss: 3489037759725.6372\n",
      "Epoch 678/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3238540596823.9941 - val_loss: 3488354269204.1631\n",
      "Epoch 679/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3237908957721.3301 - val_loss: 3487737741911.1338\n",
      "Epoch 680/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3237305809461.1807 - val_loss: 3487086212679.2910\n",
      "Epoch 681/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3236677662497.6733 - val_loss: 3486451153538.3403\n",
      "Epoch 682/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3236092059354.1255 - val_loss: 3485806741276.4443\n",
      "Epoch 683/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3235482015293.1040 - val_loss: 3485117792555.5669\n",
      "Epoch 684/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3234888355508.6704 - val_loss: 3484512291164.5347\n",
      "Epoch 685/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3234262052765.0815 - val_loss: 3483871478128.6978\n",
      "Epoch 686/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3233669948734.6045 - val_loss: 3483275733582.4922\n",
      "Epoch 687/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3233077845894.9927 - val_loss: 3482636294313.9468\n",
      "Epoch 688/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3232440094596.1118 - val_loss: 3482051144293.5356\n",
      "Epoch 689/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3231819834233.3076 - val_loss: 3481363951679.3701\n",
      "Epoch 690/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3231248634412.0571 - val_loss: 3480766252298.4414\n",
      "Epoch 691/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3230616073041.9321 - val_loss: 3480074438059.7471\n",
      "Epoch 692/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3230027690987.5918 - val_loss: 3479436285545.8564\n",
      "Epoch 693/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3229394735971.2188 - val_loss: 3478753419229.4346\n",
      "Epoch 694/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 59us/step - loss: 3228828607699.0425 - val_loss: 3478177269117.6597\n",
      "Epoch 695/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3228211651334.0620 - val_loss: 3477545027195.1392\n",
      "Epoch 696/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3227625334286.0454 - val_loss: 3476840550405.7607\n",
      "Epoch 697/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3226981218911.4370 - val_loss: 3476240257605.8511\n",
      "Epoch 698/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3226410738219.0967 - val_loss: 3475646590670.6724\n",
      "Epoch 699/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3225811065595.0181 - val_loss: 3475020000882.4980\n",
      "Epoch 700/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3225206692387.8940 - val_loss: 3474306141476.3657\n",
      "Epoch 701/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3224595568957.4038 - val_loss: 3473675434810.6890\n",
      "Epoch 702/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3224002676796.9834 - val_loss: 3473070583064.8438\n",
      "Epoch 703/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 3223392365920.6978 - val_loss: 3472425160940.1968\n",
      "Epoch 704/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3222759025340.5938 - val_loss: 3471779920214.7734\n",
      "Epoch 705/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3222193381404.8115 - val_loss: 3471076806500.4556\n",
      "Epoch 706/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3221598383733.2861 - val_loss: 3470407219934.5146\n",
      "Epoch 707/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3221003254559.5122 - val_loss: 3469757661391.3926\n",
      "Epoch 708/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 3220419561686.6440 - val_loss: 3469122070957.1870\n",
      "Epoch 709/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3219793931090.4121 - val_loss: 3468566205481.7666\n",
      "Epoch 710/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3219221819450.5830 - val_loss: 3467943328109.8174\n",
      "Epoch 711/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3218601340613.9575 - val_loss: 3467330143429.3110\n",
      "Epoch 712/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3218022551247.5610 - val_loss: 3466768424510.6499\n",
      "Epoch 713/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3217407216794.8604 - val_loss: 3466098781165.2769\n",
      "Epoch 714/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3216792108990.6943 - val_loss: 3465506154694.7510\n",
      "Epoch 715/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3216222032915.9277 - val_loss: 3464827292065.6650\n",
      "Epoch 716/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3215611718659.1211 - val_loss: 3464179362797.2769\n",
      "Epoch 717/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3215034993438.5518 - val_loss: 3463601822326.8184\n",
      "Epoch 718/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3214418062581.1357 - val_loss: 3463005587044.0957\n",
      "Epoch 719/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3213852043899.2881 - val_loss: 3462411582381.9072\n",
      "Epoch 720/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3213248422704.3188 - val_loss: 3461743498258.7231\n",
      "Epoch 721/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3212647553683.2979 - val_loss: 3461160835103.6851\n",
      "Epoch 722/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3212068642495.2349 - val_loss: 3460457411683.3755\n",
      "Epoch 723/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3211451391779.1138 - val_loss: 3459806768063.1899\n",
      "Epoch 724/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3210887300454.9399 - val_loss: 3459164392118.1885\n",
      "Epoch 725/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3210308134529.2905 - val_loss: 3458583063648.4951\n",
      "Epoch 726/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3209726990292.3027 - val_loss: 3457990831684.4106\n",
      "Epoch 727/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3209095407152.6191 - val_loss: 3457354801762.6553\n",
      "Epoch 728/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3208505371470.5708 - val_loss: 3456739141646.4023\n",
      "Epoch 729/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3207922483130.8530 - val_loss: 3456114182239.0547\n",
      "Epoch 730/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3207344096292.2539 - val_loss: 3455428592722.0928\n",
      "Epoch 731/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3206739685653.5483 - val_loss: 3454757295399.2461\n",
      "Epoch 732/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3206165290226.2544 - val_loss: 3454193073023.8198\n",
      "Epoch 733/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3205557223523.8789 - val_loss: 3453613346176.5400\n",
      "Epoch 734/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3205003383505.2417 - val_loss: 3452933542262.4585\n",
      "Epoch 735/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3204398057052.7964 - val_loss: 3452348176553.9468\n",
      "Epoch 736/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3203805708295.6831 - val_loss: 3451677132895.0547\n",
      "Epoch 737/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3203234924124.7964 - val_loss: 3451094622012.1294\n",
      "Epoch 738/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3202623015737.4424 - val_loss: 3450432370685.1196\n",
      "Epoch 739/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3202058622840.1069 - val_loss: 3449811223979.7471\n",
      "Epoch 740/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3201459187974.1826 - val_loss: 3449202936709.5811\n",
      "Epoch 741/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3200876284007.2407 - val_loss: 3448564449786.9590\n",
      "Epoch 742/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3200310213880.0171 - val_loss: 3447886451753.7666\n",
      "Epoch 743/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3199723791697.8120 - val_loss: 3447228262737.0127\n",
      "Epoch 744/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3199149550944.6978 - val_loss: 3446702598020.1406\n",
      "Epoch 745/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3198544223508.8281 - val_loss: 3446076421910.6836\n",
      "Epoch 746/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3197957654273.9810 - val_loss: 3445444412267.6567\n",
      "Epoch 747/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3197378607311.9214 - val_loss: 3444822138822.3911\n",
      "Epoch 748/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3196778975069.6968 - val_loss: 3444247077843.3530\n",
      "Epoch 749/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3196216927208.7109 - val_loss: 3443609959894.9536\n",
      "Epoch 750/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3195602092827.1904 - val_loss: 3443047327712.3149\n",
      "Epoch 751/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3195053221313.2153 - val_loss: 3442405194739.0381\n",
      "Epoch 752/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3194470131555.9390 - val_loss: 3441811260129.3950\n",
      "Epoch 753/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3193880658453.9688 - val_loss: 3441211698088.1465\n",
      "Epoch 754/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3193301010834.8774 - val_loss: 3440568190610.1826\n",
      "Epoch 755/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3192715877491.4854 - val_loss: 3439955880849.1025\n",
      "Epoch 756/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3192125905854.9346 - val_loss: 3439358726771.9380\n",
      "Epoch 757/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 73us/step - loss: 3191547019068.0835 - val_loss: 3438642402622.2896\n",
      "Epoch 758/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 3190963466855.6006 - val_loss: 3438064022627.3755\n",
      "Epoch 759/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3190389157232.0635 - val_loss: 3437391857390.3574\n",
      "Epoch 760/800\n",
      "4265/4265 [==============================] - 0s 53us/step - loss: 3189830417056.7427 - val_loss: 3436815150176.4951\n",
      "Epoch 761/800\n",
      "4265/4265 [==============================] - 0s 54us/step - loss: 3189254745083.4380 - val_loss: 3436182442588.8945\n",
      "Epoch 762/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3188662057905.4893 - val_loss: 3435601435623.5161\n",
      "Epoch 763/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3188110068137.9268 - val_loss: 3435005859940.8159\n",
      "Epoch 764/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 3187487709332.3779 - val_loss: 3434509484193.3052\n",
      "Epoch 765/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3186945716194.2285 - val_loss: 3433944341179.9492\n",
      "Epoch 766/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3186366563575.5366 - val_loss: 3433389443904.4502\n",
      "Epoch 767/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3185799020586.2563 - val_loss: 3432712784243.5781\n",
      "Epoch 768/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3185189758562.5586 - val_loss: 3432091995912.2812\n",
      "Epoch 769/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3184644249514.0464 - val_loss: 3431505401703.3359\n",
      "Epoch 770/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3184048791984.1689 - val_loss: 3430850854681.5640\n",
      "Epoch 771/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3183517228304.9863 - val_loss: 3430177634231.9888\n",
      "Epoch 772/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3182926497864.5083 - val_loss: 3429577197640.0112\n",
      "Epoch 773/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3182346037814.1411 - val_loss: 3428925033879.5835\n",
      "Epoch 774/800\n",
      "4265/4265 [==============================] - 0s 55us/step - loss: 3181777353991.3828 - val_loss: 3428370398748.0845\n",
      "Epoch 775/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3181181366626.3784 - val_loss: 3427762131756.2871\n",
      "Epoch 776/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3180623933391.0210 - val_loss: 3427184089857.0801\n",
      "Epoch 777/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3180053889892.1792 - val_loss: 3426614104738.0254\n",
      "Epoch 778/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3179485911184.5361 - val_loss: 3425979147357.6147\n",
      "Epoch 779/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3178920508894.5068 - val_loss: 3425380624674.9253\n",
      "Epoch 780/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 3178337534105.6597 - val_loss: 3424729547097.6538\n",
      "Epoch 781/800\n",
      "4265/4265 [==============================] - 0s 56us/step - loss: 3177790377222.4224 - val_loss: 3424183338918.7061\n",
      "Epoch 782/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3177203370763.3442 - val_loss: 3423600445696.3599\n",
      "Epoch 783/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3176654089100.0347 - val_loss: 3422995795628.1069\n",
      "Epoch 784/800\n",
      "4265/4265 [==============================] - 0s 52us/step - loss: 3176099463940.1416 - val_loss: 3422354701275.9941\n",
      "Epoch 785/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3175506054560.0825 - val_loss: 3421811207080.1465\n",
      "Epoch 786/800\n",
      "4265/4265 [==============================] - 0s 57us/step - loss: 3174930786589.7119 - val_loss: 3421214537840.3374\n",
      "Epoch 787/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3174394515504.9790 - val_loss: 3420593400352.4053\n",
      "Epoch 788/800\n",
      "4265/4265 [==============================] - 0s 60us/step - loss: 3173831787790.5859 - val_loss: 3419925767146.3965\n",
      "Epoch 789/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 3173261473450.5869 - val_loss: 3419311323732.2529\n",
      "Epoch 790/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 3172668012748.0796 - val_loss: 3418829241653.6484\n",
      "Epoch 791/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 3172071298386.7720 - val_loss: 3418204153570.8354\n",
      "Epoch 792/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3171545086010.1025 - val_loss: 3417629402297.7891\n",
      "Epoch 793/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 3170960426277.6348 - val_loss: 3417049853531.4541\n",
      "Epoch 794/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 3170391463777.0581 - val_loss: 3416379219864.3037\n",
      "Epoch 795/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3169826529719.8516 - val_loss: 3415772759490.7905\n",
      "Epoch 796/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3169268873861.8521 - val_loss: 3415234314240.0000\n",
      "Epoch 797/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 3168713678746.2002 - val_loss: 3414653942540.6021\n",
      "Epoch 798/800\n",
      "4265/4265 [==============================] - 0s 62us/step - loss: 3168143628086.8018 - val_loss: 3414069131721.9917\n",
      "Epoch 799/800\n",
      "4265/4265 [==============================] - 0s 59us/step - loss: 3167570236612.6367 - val_loss: 3413470000688.2476\n",
      "Epoch 800/800\n",
      "4265/4265 [==============================] - 0s 58us/step - loss: 3167034680477.2617 - val_loss: 3412826970304.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f7efd9b70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_train4 = model4.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train4'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f7efa7588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''# Create the plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(model_train1.history['val_loss'], 'r',\n",
    "         model_train2.history['val_loss'], 'y',\n",
    "         model_train3.history['val_loss'], 'g',\n",
    "         model_train4.history['val_loss'], 'b')\n",
    "plt.title(\"red:nadam , yellow:adam , green:adadelta , blue:adagrad\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.savefig(\"NeuralNetwith4HiddenLayer\")\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training now with more hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model 1\n",
    "model11 = Sequential()\n",
    "model11.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model11.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 2\n",
    "model22 = Sequential()\n",
    "model22.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model22.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 3\n",
    "model33 = Sequential()\n",
    "model33.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model33.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model33.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model33.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model33.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 4\n",
    "model44 = Sequential()\n",
    "model44.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model44.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model44.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model44.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model44.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model11 loss: mean_squared_error\n",
      "model22 loss: mean_squared_error\n",
      "model33 loss: mean_squared_error\n",
      "model44 loss: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model11.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "print(\"model11 loss:\",model11.loss)\n",
    "\n",
    "# Compile model\n",
    "model22.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(\"model22 loss:\",model22.loss)\n",
    "\n",
    "'''# Compile model\n",
    "model33.compile(loss='mean_squared_error', optimizer='adadelta')\n",
    "print(\"model33 loss:\",model33.loss)'''\n",
    "'''\n",
    "# Compile model\n",
    "model44.compile(loss='mean_squared_error', optimizer='adagrad')\n",
    "print(\"model44 loss:\",model44.loss)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4265 samples, validate on 2844 samples\n",
      "Epoch 1/800\n",
      "4265/4265 [==============================] - 1s 322us/step - loss: 123329748548337.4219 - val_loss: 72601296256954.8750\n",
      "Epoch 2/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 16958969235862.7188 - val_loss: 6902748310532.3203\n",
      "Epoch 3/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 6038039549958.2422 - val_loss: 5657160032129.2598\n",
      "Epoch 4/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 5062720277585.8721 - val_loss: 4987103125285.0859\n",
      "Epoch 5/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 4518132233817.9150 - val_loss: 4571601185773.2773\n",
      "Epoch 6/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 4166582945584.0786 - val_loss: 4296927676672.3604\n",
      "Epoch 7/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 3924954143686.6177 - val_loss: 4128594575705.6538\n",
      "Epoch 8/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 3736023189365.2261 - val_loss: 3969272213866.9365\n",
      "Epoch 9/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 3580287456744.5908 - val_loss: 3756497782326.0083\n",
      "Epoch 10/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 3438244955057.0093 - val_loss: 3624272769409.9805\n",
      "Epoch 11/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 3306506431963.6255 - val_loss: 3474935407640.4839\n",
      "Epoch 12/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 3186364204832.7129 - val_loss: 3363722172078.9873\n",
      "Epoch 13/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 3056555727787.7271 - val_loss: 3218547755471.7524\n",
      "Epoch 14/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 2947087354416.3789 - val_loss: 3095581956957.2544\n",
      "Epoch 15/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 2833890791204.0737 - val_loss: 2984443919116.6021\n",
      "Epoch 16/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 2722514936451.4517 - val_loss: 2901333385319.6963\n",
      "Epoch 17/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 2614914261020.8115 - val_loss: 2783886837002.4414\n",
      "Epoch 18/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 2506558825133.7080 - val_loss: 2688425289465.8789\n",
      "Epoch 19/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 2407773451427.9839 - val_loss: 2528325144166.9761\n",
      "Epoch 20/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 2319616363386.7476 - val_loss: 2435803624547.3755\n",
      "Epoch 21/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 2230665362222.8784 - val_loss: 2339998653622.9087\n",
      "Epoch 22/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 2146207992694.9065 - val_loss: 2261290692440.9341\n",
      "Epoch 23/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 2072635301398.2087 - val_loss: 2176649799210.4866\n",
      "Epoch 24/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 2003189370871.3567 - val_loss: 2133001318002.4978\n",
      "Epoch 25/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1935193201399.1768 - val_loss: 2030540065531.3193\n",
      "Epoch 26/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1876371969435.7610 - val_loss: 2008799226999.5386\n",
      "Epoch 27/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1825649633819.2505 - val_loss: 1907062899102.7849\n",
      "Epoch 28/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1779221387485.3665 - val_loss: 1875311248166.5261\n",
      "Epoch 29/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1723943076888.7297 - val_loss: 1804134536441.1589\n",
      "Epoch 30/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1673850163152.9414 - val_loss: 1774486211530.7117\n",
      "Epoch 31/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1626688951746.1760 - val_loss: 1718130094742.5034\n",
      "Epoch 32/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1581860705526.5762 - val_loss: 1644394088658.2729\n",
      "Epoch 33/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1535384031907.1436 - val_loss: 1598559199105.2603\n",
      "Epoch 34/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1492715645121.5156 - val_loss: 1573860943218.1379\n",
      "Epoch 35/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1451958851221.4583 - val_loss: 1546721734765.4570\n",
      "Epoch 36/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1414778140580.2842 - val_loss: 1469424402339.8257\n",
      "Epoch 37/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1376269813575.1277 - val_loss: 1429799656216.1238\n",
      "Epoch 38/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1336209996341.1809 - val_loss: 1400944576100.0957\n",
      "Epoch 39/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1304736641382.2200 - val_loss: 1360038858250.8018\n",
      "Epoch 40/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1270059196963.4138 - val_loss: 1335672548102.8411\n",
      "Epoch 41/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1242420909921.0581 - val_loss: 1303362382137.9690\n",
      "Epoch 42/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1211149695921.4893 - val_loss: 1268008395362.6555\n",
      "Epoch 43/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1190112833041.4070 - val_loss: 1243530746095.0774\n",
      "Epoch 44/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1165505010050.5510 - val_loss: 1226996351298.6104\n",
      "Epoch 45/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1142408084474.4778 - val_loss: 1197984021283.6455\n",
      "Epoch 46/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1124652115834.9880 - val_loss: 1187885113009.8677\n",
      "Epoch 47/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1107393489250.8586 - val_loss: 1163033414948.3657\n",
      "Epoch 48/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1089249473073.8193 - val_loss: 1175833447439.8425\n",
      "Epoch 49/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1078258380620.4099 - val_loss: 1144363008253.4795\n",
      "Epoch 50/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1066656302569.5513 - val_loss: 1124500961945.3840\n",
      "Epoch 51/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1049082585812.1228 - val_loss: 1149390260802.9705\n",
      "Epoch 52/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1047734347166.6420 - val_loss: 1106118757230.5374\n",
      "Epoch 53/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1034699715786.3990 - val_loss: 1087931807671.9888\n",
      "Epoch 54/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1026625597840.4764 - val_loss: 1081790469023.5048\n",
      "Epoch 55/800\n",
      "4265/4265 [==============================] - ETA: 0s - loss: 1017283199363.023 - 0s 95us/step - loss: 1020662259725.4452 - val_loss: 1075691559634.9928\n",
      "Epoch 56/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1013221300472.2571 - val_loss: 1064128234601.1365\n",
      "Epoch 57/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1003153521950.4319 - val_loss: 1061988889123.2856\n",
      "Epoch 58/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 997304047280.3489 - val_loss: 1106538016831.3699\n",
      "Epoch 59/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 995773155392.8253 - val_loss: 1044442060002.1155\n",
      "Epoch 60/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 990011596813.4452 - val_loss: 1037795729050.8242\n",
      "Epoch 61/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 982585507455.8499 - val_loss: 1036557401812.4331\n",
      "Epoch 62/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 983770149045.5109 - val_loss: 1042073460102.3010\n",
      "Epoch 63/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 974254097617.1217 - val_loss: 1026036541800.0562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 971737765406.3719 - val_loss: 1018148917622.4585\n",
      "Epoch 65/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 969369709763.1963 - val_loss: 1016343327359.4601\n",
      "Epoch 66/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 965305286168.1294 - val_loss: 1033272665043.3530\n",
      "Epoch 67/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 958879946804.3405 - val_loss: 1011364715426.3854\n",
      "Epoch 68/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 957780635983.1709 - val_loss: 1007491403620.4557\n",
      "Epoch 69/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 956215982068.9557 - val_loss: 1015436383319.8538\n",
      "Epoch 70/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 956755495718.2349 - val_loss: 1001268611190.0985\n",
      "Epoch 71/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 946091230190.4733 - val_loss: 999540992913.1027\n",
      "Epoch 72/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 944947860738.1008 - val_loss: 989946320282.4642\n",
      "Epoch 73/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 940380662675.7177 - val_loss: 989970799591.5162\n",
      "Epoch 74/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 941889910196.7306 - val_loss: 995292918514.6780\n",
      "Epoch 75/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 935226048604.1960 - val_loss: 980484878837.1984\n",
      "Epoch 76/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 931652831074.0183 - val_loss: 979488898657.2153\n",
      "Epoch 77/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 925927078445.4978 - val_loss: 997369690569.9916\n",
      "Epoch 78/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 924488242810.5679 - val_loss: 976705593862.4810\n",
      "Epoch 79/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 922738727106.4761 - val_loss: 967906777695.7750\n",
      "Epoch 80/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 917259735188.8582 - val_loss: 998754981965.7721\n",
      "Epoch 81/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 917496817991.9680 - val_loss: 970978519976.1462\n",
      "Epoch 82/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 914124890298.3129 - val_loss: 1014416394196.7932\n",
      "Epoch 83/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 913870950821.6047 - val_loss: 953120223134.0647\n",
      "Epoch 84/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 908081525551.3584 - val_loss: 946007697007.6174\n",
      "Epoch 85/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 903437692302.7958 - val_loss: 943813962548.9282\n",
      "Epoch 86/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 903913158410.6241 - val_loss: 939094192191.3699\n",
      "Epoch 87/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 896210602755.6615 - val_loss: 961512580051.3530\n",
      "Epoch 88/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 893491084758.8240 - val_loss: 956178122201.8340\n",
      "Epoch 89/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 889215819689.5662 - val_loss: 931760544098.2954\n",
      "Epoch 90/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 889898982980.3068 - val_loss: 928957379130.3291\n",
      "Epoch 91/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 884438121547.1494 - val_loss: 929058104828.3994\n",
      "Epoch 92/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 885055143486.5444 - val_loss: 920428256415.8650\n",
      "Epoch 93/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 878758542895.1783 - val_loss: 914042553748.7032\n",
      "Epoch 94/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 872693319917.6927 - val_loss: 914961903633.2827\n",
      "Epoch 95/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 872431452582.5651 - val_loss: 946693990552.6638\n",
      "Epoch 96/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 872294463457.7482 - val_loss: 910379040099.7356\n",
      "Epoch 97/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 863628471241.4987 - val_loss: 936941392569.0690\n",
      "Epoch 98/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 865536224877.6028 - val_loss: 909530285391.5724\n",
      "Epoch 99/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 862217702416.5664 - val_loss: 903555010819.2405\n",
      "Epoch 100/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 858347724433.1366 - val_loss: 893761643452.3094\n",
      "Epoch 101/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 855127789336.5496 - val_loss: 899763192245.8284\n",
      "Epoch 102/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 852914254681.6150 - val_loss: 887031948773.3558\n",
      "Epoch 103/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 850807842898.1121 - val_loss: 889512450517.5133\n",
      "Epoch 104/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 853345162209.9883 - val_loss: 883736105634.0253\n",
      "Epoch 105/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 846999626840.1144 - val_loss: 882820756763.7244\n",
      "Epoch 106/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 844939575607.1615 - val_loss: 877059625034.8917\n",
      "Epoch 107/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 841011314557.3890 - val_loss: 883584011602.4529\n",
      "Epoch 108/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 840982178162.2246 - val_loss: 881547053542.7960\n",
      "Epoch 109/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 838509028136.6359 - val_loss: 873960500781.3671\n",
      "Epoch 110/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 839402032290.5436 - val_loss: 866737973216.3151\n",
      "Epoch 111/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 831195574618.6954 - val_loss: 866706432288.0450\n",
      "Epoch 112/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 831410863513.3599 - val_loss: 862435090351.3474\n",
      "Epoch 113/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 832146456526.0604 - val_loss: 878049129732.6808\n",
      "Epoch 114/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 827505791847.0603 - val_loss: 859404069431.4486\n",
      "Epoch 115/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 830337092048.3413 - val_loss: 856760616801.5752\n",
      "Epoch 116/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 825664011458.2358 - val_loss: 856995933533.9747\n",
      "Epoch 117/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 824567850544.6190 - val_loss: 866461807784.5063\n",
      "Epoch 118/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 819623305459.9353 - val_loss: 861459082613.0183\n",
      "Epoch 119/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 823015789269.0831 - val_loss: 869733324486.0309\n",
      "Epoch 120/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 820811059855.9363 - val_loss: 864117427907.1505\n",
      "Epoch 121/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 819052596275.1400 - val_loss: 850343043406.1322\n",
      "Epoch 122/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 816877129538.8062 - val_loss: 845022223404.6470\n",
      "Epoch 123/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 816372109706.7142 - val_loss: 848586101317.8510\n",
      "Epoch 124/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 815982433203.8903 - val_loss: 859705613425.7778\n",
      "Epoch 125/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 816381199019.7871 - val_loss: 840058693692.4895\n",
      "Epoch 126/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 811239657144.5121 - val_loss: 841924656161.1251\n",
      "Epoch 127/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 810524754039.0865 - val_loss: 864802010280.5063\n",
      "Epoch 128/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 86us/step - loss: 808309589727.4072 - val_loss: 852098450375.8312\n",
      "Epoch 129/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 809809978225.6244 - val_loss: 839157254216.0112\n",
      "Epoch 130/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 806020057068.3124 - val_loss: 850625234877.7496\n",
      "Epoch 131/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 807069535398.3850 - val_loss: 841821839993.6990\n",
      "Epoch 132/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 806653264071.7581 - val_loss: 842804214264.0787\n",
      "Epoch 133/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 804551233711.7485 - val_loss: 838663137985.7103\n",
      "Epoch 134/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 807091931508.1454 - val_loss: 831656392816.3375\n",
      "Epoch 135/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 804543885205.3983 - val_loss: 831935454782.6498\n",
      "Epoch 136/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 800087935893.6384 - val_loss: 843365473693.3446\n",
      "Epoch 137/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 798370811776.2701 - val_loss: 830069818012.2644\n",
      "Epoch 138/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 800550642756.9069 - val_loss: 855121197997.9072\n",
      "Epoch 139/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 802310659510.1711 - val_loss: 833262669207.5837\n",
      "Epoch 140/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 799430659384.3621 - val_loss: 842569982654.8298\n",
      "Epoch 141/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 799428966173.8317 - val_loss: 825001432820.1182\n",
      "Epoch 142/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 797618476720.3489 - val_loss: 851992410685.2096\n",
      "Epoch 143/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 796516913504.4576 - val_loss: 820756091945.7665\n",
      "Epoch 144/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 793287806548.6331 - val_loss: 822023029888.1801\n",
      "Epoch 145/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 797116167635.4625 - val_loss: 839886540437.0632\n",
      "Epoch 146/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 793333003562.4366 - val_loss: 842303951568.1125\n",
      "Epoch 147/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 795498784162.4835 - val_loss: 819555002566.7511\n",
      "Epoch 148/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 791585150521.2625 - val_loss: 836775475404.5120\n",
      "Epoch 149/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 789047080518.7076 - val_loss: 826520217499.1842\n",
      "Epoch 150/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 791327353824.5477 - val_loss: 816923570538.9368\n",
      "Epoch 151/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 794154492228.6068 - val_loss: 815673433455.2573\n",
      "Epoch 152/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 790383065945.1348 - val_loss: 817131709741.0071\n",
      "Epoch 153/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 789939192307.3950 - val_loss: 824180942516.7483\n",
      "Epoch 154/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 788818485495.7767 - val_loss: 814838874803.3080\n",
      "Epoch 155/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 789645042030.8634 - val_loss: 815712560014.2222\n",
      "Epoch 156/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 787311055035.2732 - val_loss: 813432973672.0563\n",
      "Epoch 157/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 787016032169.5662 - val_loss: 858785300825.6541\n",
      "Epoch 158/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 789800862821.5597 - val_loss: 810799532242.2728\n",
      "Epoch 159/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 785028847749.0120 - val_loss: 816183140617.0015\n",
      "Epoch 160/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 782806212087.4767 - val_loss: 811809335633.0127\n",
      "Epoch 161/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 786902779538.5773 - val_loss: 826337098209.0352\n",
      "Epoch 162/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 784415052869.8673 - val_loss: 850961503826.8130\n",
      "Epoch 163/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 786467722190.7808 - val_loss: 812162187632.6976\n",
      "Epoch 164/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 784931191748.6969 - val_loss: 813660662465.7103\n",
      "Epoch 165/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 787016190208.9003 - val_loss: 809841775496.4613\n",
      "Epoch 166/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 783590589376.3751 - val_loss: 808673019561.2264\n",
      "Epoch 167/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 781543547077.8373 - val_loss: 822113686224.1125\n",
      "Epoch 168/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 781482024570.0878 - val_loss: 804268306426.2391\n",
      "Epoch 169/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 783604121722.9281 - val_loss: 804175153684.8833\n",
      "Epoch 170/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 781167056541.1415 - val_loss: 808555688091.5443\n",
      "Epoch 171/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 782398448201.8289 - val_loss: 806544015406.0872\n",
      "Epoch 172/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 778942581984.0074 - val_loss: 826494124170.2616\n",
      "Epoch 173/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 782419630697.0410 - val_loss: 808430987508.8383\n",
      "Epoch 174/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 780926048723.2225 - val_loss: 803604682484.1182\n",
      "Epoch 175/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 779166922775.5293 - val_loss: 805512728190.0197\n",
      "Epoch 176/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 778863776611.2188 - val_loss: 818901148051.2631\n",
      "Epoch 177/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 777527195567.8087 - val_loss: 801021128099.1055\n",
      "Epoch 178/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 780096614135.8969 - val_loss: 801002689024.7201\n",
      "Epoch 179/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 773871262647.2516 - val_loss: 803892327730.7679\n",
      "Epoch 180/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 777838838301.6516 - val_loss: 817699296436.0281\n",
      "Epoch 181/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 778785374638.2480 - val_loss: 829361199366.1210\n",
      "Epoch 182/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 776076305649.7744 - val_loss: 804112099665.0127\n",
      "Epoch 183/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 777263584590.9309 - val_loss: 799808012725.8284\n",
      "Epoch 184/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 775477405013.6534 - val_loss: 799959120306.9480\n",
      "Epoch 185/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 776076479055.1110 - val_loss: 812557980322.0253\n",
      "Epoch 186/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 773629173277.8917 - val_loss: 819873680492.0168\n",
      "Epoch 187/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 777467599434.3091 - val_loss: 811191681507.9156\n",
      "Epoch 188/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 775996960830.4244 - val_loss: 796752717538.8354\n",
      "Epoch 189/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 770213194981.7698 - val_loss: 812844974126.0872\n",
      "Epoch 190/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 770832805109.1357 - val_loss: 794133514565.4908\n",
      "Epoch 191/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 767950231500.6199 - val_loss: 794041132356.0507\n",
      "Epoch 192/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 84us/step - loss: 769334482762.0089 - val_loss: 788852718118.1660\n",
      "Epoch 193/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 763429405643.8997 - val_loss: 800245229974.1434\n",
      "Epoch 194/800\n",
      "4265/4265 [==============================] - 1s 132us/step - loss: 763256252778.5416 - val_loss: 807682012697.2040\n",
      "Epoch 195/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 763610503331.7439 - val_loss: 785189494127.2573\n",
      "Epoch 196/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 757291672767.8350 - val_loss: 787045210074.5542\n",
      "Epoch 197/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 759142738931.7552 - val_loss: 794801539097.9241\n",
      "Epoch 198/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 755965815565.2653 - val_loss: 785884906900.7032\n",
      "Epoch 199/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 754034560043.4569 - val_loss: 773730755193.6990\n",
      "Epoch 200/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 749445864344.5195 - val_loss: 775888129431.5837\n",
      "Epoch 201/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 748613865970.4347 - val_loss: 770173948439.7637\n",
      "Epoch 202/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 744243038656.9755 - val_loss: 765905606918.1210\n",
      "Epoch 203/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 741501449287.7880 - val_loss: 763796399428.0507\n",
      "Epoch 204/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 738333880774.2574 - val_loss: 801105838172.1744\n",
      "Epoch 205/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 738090394473.9414 - val_loss: 761784652910.8973\n",
      "Epoch 206/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 732861269942.5314 - val_loss: 754182667850.1716\n",
      "Epoch 207/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 726925090622.0043 - val_loss: 750519094149.5809\n",
      "Epoch 208/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 721817511991.2216 - val_loss: 765084611314.6780\n",
      "Epoch 209/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 721198008246.5314 - val_loss: 748256344614.1660\n",
      "Epoch 210/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 710730948905.7163 - val_loss: 739394771551.7750\n",
      "Epoch 211/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 709528033812.5281 - val_loss: 733161972023.0886\n",
      "Epoch 212/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 704010443418.9806 - val_loss: 752371610124.2419\n",
      "Epoch 213/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 700302635304.7560 - val_loss: 727150784828.8495\n",
      "Epoch 214/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 689977234866.8099 - val_loss: 737044093720.1238\n",
      "Epoch 215/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 690922628606.6794 - val_loss: 712537797754.4191\n",
      "Epoch 216/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 678566250134.4188 - val_loss: 708142251317.6484\n",
      "Epoch 217/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 673492087810.6411 - val_loss: 753991032106.1266\n",
      "Epoch 218/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 666455583712.5477 - val_loss: 688847147589.8510\n",
      "Epoch 219/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 660637530488.9473 - val_loss: 682491468798.5598\n",
      "Epoch 220/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 652999753074.9449 - val_loss: 672762083279.0323\n",
      "Epoch 221/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 647057787570.9900 - val_loss: 665893975692.4220\n",
      "Epoch 222/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 639131295165.8541 - val_loss: 673057745532.5795\n",
      "Epoch 223/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 633515512913.3918 - val_loss: 655961367553.4402\n",
      "Epoch 224/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 629717746785.7181 - val_loss: 651626383800.7089\n",
      "Epoch 225/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 619154031394.8737 - val_loss: 643254610118.7511\n",
      "Epoch 226/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 612556428112.0112 - val_loss: 635244298631.7412\n",
      "Epoch 227/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 607542235895.6567 - val_loss: 629011267284.4332\n",
      "Epoch 228/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 601463484555.2544 - val_loss: 623433536899.4205\n",
      "Epoch 229/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 597360087661.3628 - val_loss: 616847261672.9564\n",
      "Epoch 230/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 590243841578.8567 - val_loss: 609733798132.8383\n",
      "Epoch 231/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 586011705251.8040 - val_loss: 607438037210.9142\n",
      "Epoch 232/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 579712293350.4301 - val_loss: 606735685410.2053\n",
      "Epoch 233/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 575730433998.0604 - val_loss: 596851369284.0507\n",
      "Epoch 234/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 567718361042.3822 - val_loss: 592463751873.7103\n",
      "Epoch 235/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 565484185766.6250 - val_loss: 587183219506.0479\n",
      "Epoch 236/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 559820319328.3976 - val_loss: 579656178916.9957\n",
      "Epoch 237/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 556632251568.9491 - val_loss: 593526904978.9030\n",
      "Epoch 238/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 552670840470.4188 - val_loss: 586892388665.9691\n",
      "Epoch 239/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 549967895713.5830 - val_loss: 576118869353.4965\n",
      "Epoch 240/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 545077213438.0192 - val_loss: 567607950484.3431\n",
      "Epoch 241/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 540119003348.2429 - val_loss: 602750462680.7539\n",
      "Epoch 242/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 537316565673.8664 - val_loss: 557685027340.2419\n",
      "Epoch 243/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 530515693935.8237 - val_loss: 562439614135.6287\n",
      "Epoch 244/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 531866350890.4366 - val_loss: 549921996500.4332\n",
      "Epoch 245/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 530405917839.8162 - val_loss: 553092845804.1969\n",
      "Epoch 246/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 523068210631.4579 - val_loss: 541578172401.5978\n",
      "Epoch 247/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 518821933971.2375 - val_loss: 547330573555.3980\n",
      "Epoch 248/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 517677756970.1365 - val_loss: 535702037021.5247\n",
      "Epoch 249/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 517289165979.3406 - val_loss: 535616943042.0703\n",
      "Epoch 250/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 509452945956.8544 - val_loss: 537859711287.0886\n",
      "Epoch 251/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 506336117565.7642 - val_loss: 557868057682.0928\n",
      "Epoch 252/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 511525777109.3234 - val_loss: 525003021126.2111\n",
      "Epoch 253/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 498507986375.9381 - val_loss: 524483404801.4402\n",
      "Epoch 254/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 497776286024.9285 - val_loss: 532670536748.6469\n",
      "Epoch 255/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 497131373853.9517 - val_loss: 513400950559.3249\n",
      "Epoch 256/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 91us/step - loss: 492742318904.4821 - val_loss: 525815085490.9479\n",
      "Epoch 257/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 492638403125.1808 - val_loss: 514417925671.6062\n",
      "Epoch 258/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 487538962651.9259 - val_loss: 509725649647.7975\n",
      "Epoch 259/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 483493631091.9653 - val_loss: 503017203887.7075\n",
      "Epoch 260/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 482775932511.1972 - val_loss: 501659109056.2700\n",
      "Epoch 261/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 479659621620.1754 - val_loss: 519197095310.9423\n",
      "Epoch 262/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 478118435859.4476 - val_loss: 522064571834.1491\n",
      "Epoch 263/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 476228137179.9259 - val_loss: 506073982690.8355\n",
      "Epoch 264/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 472062316877.2502 - val_loss: 489114101590.0535\n",
      "Epoch 265/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 469414862212.2316 - val_loss: 487542473464.4388\n",
      "Epoch 266/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 465511413664.4427 - val_loss: 509220379116.5569\n",
      "Epoch 267/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 466866609229.0701 - val_loss: 487625235500.6470\n",
      "Epoch 268/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 459633542330.0727 - val_loss: 481469454459.8594\n",
      "Epoch 269/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 458830147377.5193 - val_loss: 483380402000.2925\n",
      "Epoch 270/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 455348245527.7693 - val_loss: 478430892544.7201\n",
      "Epoch 271/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 451014953708.6124 - val_loss: 469581823032.1688\n",
      "Epoch 272/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 449449429663.7824 - val_loss: 469736333254.3910\n",
      "Epoch 273/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 445619447125.6535 - val_loss: 473653601350.5710\n",
      "Epoch 274/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 445268508867.9165 - val_loss: 460862704356.2757\n",
      "Epoch 275/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 442190951288.3470 - val_loss: 459607044781.5471\n",
      "Epoch 276/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 437196332270.4131 - val_loss: 460440013217.6653\n",
      "Epoch 277/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 435143108098.5210 - val_loss: 460473371917.3221\n",
      "Epoch 278/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 432938213239.6268 - val_loss: 453035339286.3235\n",
      "Epoch 279/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 432393178047.8950 - val_loss: 458285438662.0309\n",
      "Epoch 280/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 427597458043.2881 - val_loss: 451592029565.6596\n",
      "Epoch 281/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 424632247941.1320 - val_loss: 460657292709.9860\n",
      "Epoch 282/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 422613695134.8221 - val_loss: 441437568355.7356\n",
      "Epoch 283/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 418902910958.2330 - val_loss: 435526484521.0464\n",
      "Epoch 284/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 416781181406.2668 - val_loss: 433856814424.2138\n",
      "Epoch 285/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 414732134620.4061 - val_loss: 430371524752.0225\n",
      "Epoch 286/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 411312814214.9327 - val_loss: 442145843168.3151\n",
      "Epoch 287/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 411332678783.4898 - val_loss: 425211644667.3193\n",
      "Epoch 288/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 407916908881.0917 - val_loss: 421713794596.7257\n",
      "Epoch 289/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 404880995468.9351 - val_loss: 429001093275.5443\n",
      "Epoch 290/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 402793647800.5120 - val_loss: 418812869365.5583\n",
      "Epoch 291/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 398838088590.4357 - val_loss: 432201489475.6906\n",
      "Epoch 292/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 400178115183.2834 - val_loss: 413244976086.2335\n",
      "Epoch 293/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 397472700310.8389 - val_loss: 410281369980.2194\n",
      "Epoch 294/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 393280453956.8469 - val_loss: 408996643069.4796\n",
      "Epoch 295/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 391678807905.5381 - val_loss: 411158039222.1885\n",
      "Epoch 296/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 386663126051.2938 - val_loss: 403573713647.7975\n",
      "Epoch 297/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 386920125137.2418 - val_loss: 402974308349.1196\n",
      "Epoch 298/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 385632071776.9979 - val_loss: 408026621436.3994\n",
      "Epoch 299/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 383371729816.5196 - val_loss: 399781928487.6062\n",
      "Epoch 300/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 382908890106.4778 - val_loss: 397891543120.6526\n",
      "Epoch 301/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 378424437090.8586 - val_loss: 420780671552.0900\n",
      "Epoch 302/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 377633081600.9003 - val_loss: 402169746727.2462\n",
      "Epoch 303/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 374343736681.3411 - val_loss: 387739540478.5598\n",
      "Epoch 304/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 372781558272.3602 - val_loss: 391480634418.4079\n",
      "Epoch 305/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 369982172716.2973 - val_loss: 386404440510.4698\n",
      "Epoch 306/800\n",
      "4265/4265 [==============================] - 1s 145us/step - loss: 369737016646.2874 - val_loss: 380415128836.6807\n",
      "Epoch 307/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 364309129947.3257 - val_loss: 391127983151.5274\n",
      "Epoch 308/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 363603162679.3416 - val_loss: 381056991439.3924\n",
      "Epoch 309/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 360648954500.4117 - val_loss: 378988505286.7510\n",
      "Epoch 310/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 357010134208.7953 - val_loss: 371496987375.7975\n",
      "Epoch 311/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 358083791508.2579 - val_loss: 382741775004.2644\n",
      "Epoch 312/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 355519631889.1667 - val_loss: 370331551009.4852\n",
      "Epoch 313/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 354511995926.8089 - val_loss: 389740310499.1955\n",
      "Epoch 314/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 351019781240.2870 - val_loss: 372540603932.0844\n",
      "Epoch 315/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 349338130334.5219 - val_loss: 375355594276.7257\n",
      "Epoch 316/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 348575015232.7653 - val_loss: 361424099336.6414\n",
      "Epoch 317/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 346128030251.5770 - val_loss: 359079508172.5120\n",
      "Epoch 318/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 342006795418.8605 - val_loss: 381217980922.9592\n",
      "Epoch 319/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 343601064406.1036 - val_loss: 359561659497.1364\n",
      "Epoch 320/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 81us/step - loss: 339894574418.2922 - val_loss: 353219372263.8762\n",
      "Epoch 321/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 337211858865.2493 - val_loss: 355618687554.9705\n",
      "Epoch 322/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 338383486588.0084 - val_loss: 360869069675.6568\n",
      "Epoch 323/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 336877078089.5887 - val_loss: 346703279203.3755\n",
      "Epoch 324/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 331818817377.0579 - val_loss: 346098541661.6146\n",
      "Epoch 325/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 331126227262.8445 - val_loss: 345723295752.6414\n",
      "Epoch 326/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 327383816785.9921 - val_loss: 373803714954.6216\n",
      "Epoch 327/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 327892423288.8872 - val_loss: 373927231395.8256\n",
      "Epoch 328/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 325717607341.1677 - val_loss: 335698127991.5387\n",
      "Epoch 329/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 323031030219.5395 - val_loss: 356318481907.7581\n",
      "Epoch 330/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 321832742183.0753 - val_loss: 339138608483.7356\n",
      "Epoch 331/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 318243658368.0900 - val_loss: 336665960459.5218\n",
      "Epoch 332/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 317223604276.8207 - val_loss: 329044990790.2110\n",
      "Epoch 333/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 313857011357.6215 - val_loss: 332073766318.6273\n",
      "Epoch 334/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 309758334809.3749 - val_loss: 322253766425.5640\n",
      "Epoch 335/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 307113462393.3674 - val_loss: 320039567774.7848\n",
      "Epoch 336/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 303538487591.7956 - val_loss: 316567354723.7356\n",
      "Epoch 337/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 301188904272.8516 - val_loss: 314116830030.8523\n",
      "Epoch 338/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 301060753699.4739 - val_loss: 333257867724.8720\n",
      "Epoch 339/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 295260064219.8658 - val_loss: 316553933900.3319\n",
      "Epoch 340/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 293486055115.7196 - val_loss: 311001387082.8917\n",
      "Epoch 341/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 290028614985.1686 - val_loss: 325306509948.5795\n",
      "Epoch 342/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 287688884523.6370 - val_loss: 298895605253.0408\n",
      "Epoch 343/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 286953019866.9055 - val_loss: 298382202266.4641\n",
      "Epoch 344/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 283617004472.2120 - val_loss: 302841933103.8875\n",
      "Epoch 345/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 279999999628.3348 - val_loss: 288755945086.0197\n",
      "Epoch 346/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 277207044033.5756 - val_loss: 287928958653.3896\n",
      "Epoch 347/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 275102504747.2769 - val_loss: 300781109409.3052\n",
      "Epoch 348/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 273281523372.0272 - val_loss: 280859889900.1969\n",
      "Epoch 349/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 270964213290.8568 - val_loss: 284230805979.2743\n",
      "Epoch 350/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 267960359029.6459 - val_loss: 285662465569.8453\n",
      "Epoch 351/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 266866102102.0136 - val_loss: 272139971887.8875\n",
      "Epoch 352/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 263434184040.8610 - val_loss: 272454511505.1027\n",
      "Epoch 353/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 259301590663.0528 - val_loss: 279330739979.1617\n",
      "Epoch 354/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 257391585865.3487 - val_loss: 264842868110.9423\n",
      "Epoch 355/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 256206816561.1592 - val_loss: 267332292475.4993\n",
      "Epoch 356/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 251396708945.9920 - val_loss: 267727747092.1631\n",
      "Epoch 357/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 249512996934.1074 - val_loss: 270385645893.4908\n",
      "Epoch 358/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 248204121507.4438 - val_loss: 262206208619.2968\n",
      "Epoch 359/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 244313079489.1555 - val_loss: 254619352857.5640\n",
      "Epoch 360/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 242612803878.3550 - val_loss: 258468340276.5682\n",
      "Epoch 361/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 240683070935.3041 - val_loss: 253588625531.8593\n",
      "Epoch 362/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 237464500581.7397 - val_loss: 255666296057.1590\n",
      "Epoch 363/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 235122528260.8019 - val_loss: 248684943783.4261\n",
      "Epoch 364/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 233915483445.0007 - val_loss: 247273775852.9171\n",
      "Epoch 365/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 234177646372.0741 - val_loss: 242204393541.1308\n",
      "Epoch 366/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 228421124910.3981 - val_loss: 256631695996.5795\n",
      "Epoch 367/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 227816023158.8464 - val_loss: 240603409227.9719\n",
      "Epoch 368/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 227220880678.1149 - val_loss: 236435717913.5640\n",
      "Epoch 369/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 226892019875.7440 - val_loss: 252354423051.8819\n",
      "Epoch 370/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 223450400398.7357 - val_loss: 235517569891.0155\n",
      "Epoch 371/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 220154242399.2572 - val_loss: 232480640453.6709\n",
      "Epoch 372/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 219187876150.2012 - val_loss: 231860643739.1842\n",
      "Epoch 373/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 217794778788.8244 - val_loss: 236113161289.4515\n",
      "Epoch 374/800\n",
      "4265/4265 [==============================] - 1s 154us/step - loss: 218266846958.7733 - val_loss: 238859190295.0436\n",
      "Epoch 375/800\n",
      "4265/4265 [==============================] - 1s 154us/step - loss: 216112408946.2246 - val_loss: 243348774393.5190\n",
      "Epoch 376/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 216072548489.3336 - val_loss: 229259043611.0042\n",
      "Epoch 377/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 211839968467.0424 - val_loss: 226911364028.3094\n",
      "Epoch 378/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 212307407947.6295 - val_loss: 229605225317.8959\n",
      "Epoch 379/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 211156066474.9468 - val_loss: 225210239670.1885\n",
      "Epoch 380/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 209963421118.8145 - val_loss: 223322237357.1871\n",
      "Epoch 381/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 208042946245.9573 - val_loss: 225004111990.0984\n",
      "Epoch 382/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 208845298859.1869 - val_loss: 222378244876.6020\n",
      "Epoch 383/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 206911114659.9240 - val_loss: 226604594067.9831\n",
      "Epoch 384/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 92us/step - loss: 206874713376.3526 - val_loss: 232418421165.1871\n",
      "Epoch 385/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 208257513595.4082 - val_loss: 217672124326.7061\n",
      "Epoch 386/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 203912793573.2296 - val_loss: 216597183889.8228\n",
      "Epoch 387/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 203142933626.2077 - val_loss: 223342420601.6990\n",
      "Epoch 388/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 203931631172.3067 - val_loss: 221549787985.7328\n",
      "Epoch 389/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 202324854345.5887 - val_loss: 212804228414.2897\n",
      "Epoch 390/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 202772396320.5927 - val_loss: 219835786633.1814\n",
      "Epoch 391/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 202016723480.8497 - val_loss: 228276548223.4599\n",
      "Epoch 392/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 199937667092.1679 - val_loss: 212846344212.1631\n",
      "Epoch 393/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 202024145924.5618 - val_loss: 210606099333.5809\n",
      "Epoch 394/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 199484439673.2474 - val_loss: 245516137686.5935\n",
      "Epoch 395/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 199661651091.6577 - val_loss: 224076478668.5120\n",
      "Epoch 396/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 199104119394.7986 - val_loss: 212183047051.3418\n",
      "Epoch 397/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 197627311613.7191 - val_loss: 222727947622.6160\n",
      "Epoch 398/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 196036418287.4936 - val_loss: 236092206936.9339\n",
      "Epoch 399/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 196876388651.1569 - val_loss: 204995677268.9733\n",
      "Epoch 400/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 192292457135.1484 - val_loss: 208762980400.9677\n",
      "Epoch 401/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 191175819548.2711 - val_loss: 231730789985.2152\n",
      "Epoch 402/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 192089683714.9412 - val_loss: 218386480276.3432\n",
      "Epoch 403/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 188122657652.5055 - val_loss: 201484425869.8622\n",
      "Epoch 404/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 187437664528.2664 - val_loss: 201816113290.2616\n",
      "Epoch 405/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 183059842468.4042 - val_loss: 205702567921.5977\n",
      "Epoch 406/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 179527228443.1306 - val_loss: 193185536653.8622\n",
      "Epoch 407/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 175478136150.1337 - val_loss: 208761261901.4121\n",
      "Epoch 408/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 169199192270.9608 - val_loss: 176713512058.4191\n",
      "Epoch 409/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 162447910742.4938 - val_loss: 174274795321.2489\n",
      "Epoch 410/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 157114514207.5123 - val_loss: 165803663233.2602\n",
      "Epoch 411/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 146193433463.3866 - val_loss: 159702593760.6751\n",
      "Epoch 412/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 134280930899.1925 - val_loss: 148076607646.4247\n",
      "Epoch 413/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 124822071862.1411 - val_loss: 127826363704.5288\n",
      "Epoch 414/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 111620033627.4757 - val_loss: 126895439180.6920\n",
      "Epoch 415/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 103509332499.0875 - val_loss: 109627723067.4093\n",
      "Epoch 416/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 96984768014.5257 - val_loss: 105747980426.2616\n",
      "Epoch 417/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 91347712430.7283 - val_loss: 98187646961.5977\n",
      "Epoch 418/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 86936604651.8321 - val_loss: 95822908973.3671\n",
      "Epoch 419/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 83157133245.2539 - val_loss: 98553184878.1772\n",
      "Epoch 420/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 81519463901.5465 - val_loss: 88369836994.0703\n",
      "Epoch 421/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 78807754939.7533 - val_loss: 83012112447.3699\n",
      "Epoch 422/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 76799414082.8061 - val_loss: 87351078560.5851\n",
      "Epoch 423/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 74597774638.2781 - val_loss: 78741607494.5710\n",
      "Epoch 424/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 73442920287.1372 - val_loss: 78794402008.0338\n",
      "Epoch 425/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 71379333946.8830 - val_loss: 83723174371.9156\n",
      "Epoch 426/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 71268712354.8436 - val_loss: 80552366563.9156\n",
      "Epoch 427/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 70799397388.3648 - val_loss: 75222163386.8692\n",
      "Epoch 428/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 68864701319.7130 - val_loss: 74939715739.5443\n",
      "Epoch 429/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 68513120855.2741 - val_loss: 79757875076.1406\n",
      "Epoch 430/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 67567407738.8080 - val_loss: 73208841223.2011\n",
      "Epoch 431/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 67022212189.3965 - val_loss: 74501351547.8594\n",
      "Epoch 432/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 65933049483.3744 - val_loss: 77495516630.9536\n",
      "Epoch 433/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 64813270291.3876 - val_loss: 70971760190.6498\n",
      "Epoch 434/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 65149750709.2108 - val_loss: 76863191762.9930\n",
      "Epoch 435/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 65297841739.9897 - val_loss: 68120790702.9873\n",
      "Epoch 436/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 63449614155.9297 - val_loss: 68552386263.3136\n",
      "Epoch 437/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 64560267227.5057 - val_loss: 80029211191.4487\n",
      "Epoch 438/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 63495922415.0134 - val_loss: 74965081363.0830\n",
      "Epoch 439/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 63086151948.4249 - val_loss: 66038409892.9058\n",
      "Epoch 440/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 62267827561.8213 - val_loss: 66336037771.3418\n",
      "Epoch 441/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 62544561726.0642 - val_loss: 67171744472.7539\n",
      "Epoch 442/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 61785375313.5118 - val_loss: 67105053343.1449\n",
      "Epoch 443/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 61441426685.2989 - val_loss: 69583313549.8622\n",
      "Epoch 444/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 60875466010.1102 - val_loss: 75557938868.7482\n",
      "Epoch 445/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 60748403093.2783 - val_loss: 70202947921.0127\n",
      "Epoch 446/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 61330703435.6295 - val_loss: 80392277866.2166\n",
      "Epoch 447/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 61009724456.0957 - val_loss: 87168851219.0830\n",
      "Epoch 448/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 94us/step - loss: 60894625855.3848 - val_loss: 65730181399.4037\n",
      "Epoch 449/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 60264868271.6886 - val_loss: 67759904828.4894\n",
      "Epoch 450/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 59636846761.9864 - val_loss: 68410253842.0028\n",
      "Epoch 451/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 59822373636.6218 - val_loss: 79141041533.6596\n",
      "Epoch 452/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 59172176782.4356 - val_loss: 74367152992.1350\n",
      "Epoch 453/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 59063599799.3116 - val_loss: 63363405342.9648\n",
      "Epoch 454/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 58173847658.1215 - val_loss: 78833908100.8608\n",
      "Epoch 455/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 58505682992.0188 - val_loss: 62663597503.9100\n",
      "Epoch 456/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 58199430626.8286 - val_loss: 65883547051.7468\n",
      "Epoch 457/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 57733401695.3172 - val_loss: 73328260229.9409\n",
      "Epoch 458/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 57789860342.7564 - val_loss: 62452113160.2813\n",
      "Epoch 459/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 57889209038.8408 - val_loss: 66739772702.6048\n",
      "Epoch 460/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 58556346718.5369 - val_loss: 63610460520.0563\n",
      "Epoch 461/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 57992861222.0549 - val_loss: 61011313665.4402\n",
      "Epoch 462/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 57127766701.2277 - val_loss: 68763157194.3516\n",
      "Epoch 463/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 57211523456.1501 - val_loss: 64666983387.9944\n",
      "Epoch 464/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 56561242647.8893 - val_loss: 71791215116.2419\n",
      "Epoch 465/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 56938223676.0234 - val_loss: 63069538785.0352\n",
      "Epoch 466/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 56161743354.1177 - val_loss: 66293731120.6076\n",
      "Epoch 467/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 57238083313.6544 - val_loss: 80972549585.1927\n",
      "Epoch 468/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 56183260665.8776 - val_loss: 61366673863.1111\n",
      "Epoch 469/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 55500512553.9564 - val_loss: 63060281053.0745\n",
      "Epoch 470/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 56240490044.1435 - val_loss: 60389283197.6596\n",
      "Epoch 471/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 55339716854.8164 - val_loss: 60679752547.0155\n",
      "Epoch 472/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 55259106303.2797 - val_loss: 62400879147.9269\n",
      "Epoch 473/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 56189552124.7587 - val_loss: 65164986002.1828\n",
      "Epoch 474/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 55154695941.3421 - val_loss: 65678515999.3249\n",
      "Epoch 475/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 55123002534.8652 - val_loss: 66582135878.5710\n",
      "Epoch 476/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 55232661459.3426 - val_loss: 63289755662.4023\n",
      "Epoch 477/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 55145859440.3039 - val_loss: 60330308920.5288\n",
      "Epoch 478/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 54748362732.3123 - val_loss: 82104522259.4430\n",
      "Epoch 479/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 54771121042.7573 - val_loss: 63483645825.2602\n",
      "Epoch 480/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 53960120734.4019 - val_loss: 66550363799.9437\n",
      "Epoch 481/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 54459904723.1625 - val_loss: 66583186879.9100\n",
      "Epoch 482/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 54583710359.6192 - val_loss: 61054031511.9437\n",
      "Epoch 483/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 54083124202.1515 - val_loss: 60536587772.3994\n",
      "Epoch 484/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 53482970994.8249 - val_loss: 59638403930.3741\n",
      "Epoch 485/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 53059595284.4080 - val_loss: 59486787684.8158\n",
      "Epoch 486/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 54239726004.4905 - val_loss: 60466010604.5570\n",
      "Epoch 487/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 54050223148.8975 - val_loss: 59349913116.0844\n",
      "Epoch 488/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 54336302590.1993 - val_loss: 59148564321.5752\n",
      "Epoch 489/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 52965903209.4612 - val_loss: 61904867604.5232\n",
      "Epoch 490/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 53174603342.8708 - val_loss: 59437948098.4304\n",
      "Epoch 491/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 53310418706.7873 - val_loss: 60579207464.6864\n",
      "Epoch 492/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 53248765964.4849 - val_loss: 81362138361.1589\n",
      "Epoch 493/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 53808162834.7273 - val_loss: 60313564950.6835\n",
      "Epoch 494/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 53230592560.8591 - val_loss: 67566210691.7806\n",
      "Epoch 495/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 52578745905.0992 - val_loss: 63541201394.3179\n",
      "Epoch 496/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 53184794616.3170 - val_loss: 60006191922.0478\n",
      "Epoch 497/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 51782250729.3712 - val_loss: 62254378287.8875\n",
      "Epoch 498/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 52688844711.1653 - val_loss: 61179198572.0169\n",
      "Epoch 499/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 51945927298.4910 - val_loss: 60132225473.3502\n",
      "Epoch 500/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 52633324616.2682 - val_loss: 70110915346.3629\n",
      "Epoch 501/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 52364427909.3721 - val_loss: 63136454007.8987\n",
      "Epoch 502/800\n",
      "4265/4265 [==============================] - 1s 135us/step - loss: 51746940624.0413 - val_loss: 61786838052.0056\n",
      "Epoch 503/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 51620753224.5684 - val_loss: 67017299596.4219\n",
      "Epoch 504/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 52358751362.3709 - val_loss: 66304232170.0366\n",
      "Epoch 505/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 52359099888.7540 - val_loss: 72694252827.7243\n",
      "Epoch 506/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 52284596028.0835 - val_loss: 60413935817.6315\n",
      "Epoch 507/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 51279051263.3998 - val_loss: 58495804129.3952\n",
      "Epoch 508/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 51742938134.8089 - val_loss: 56373039057.9128\n",
      "Epoch 509/800\n",
      "4265/4265 [==============================] - 1s 134us/step - loss: 52065518489.9601 - val_loss: 83948026394.6442\n",
      "Epoch 510/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 52150150812.1810 - val_loss: 62968233701.7159\n",
      "Epoch 511/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 51814066226.4197 - val_loss: 58577834963.3530\n",
      "Epoch 512/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 50851759693.4302 - val_loss: 60612821942.5485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 513/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 51312166525.4490 - val_loss: 67549897458.6779\n",
      "Epoch 514/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 51412873841.2042 - val_loss: 61849482973.0745\n",
      "Epoch 515/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 51461579855.4710 - val_loss: 58632401990.5710\n",
      "Epoch 516/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 50938954133.2783 - val_loss: 56724218910.2447\n",
      "Epoch 517/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 51219263481.2774 - val_loss: 59240499277.7721\n",
      "Epoch 518/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 50558792995.7140 - val_loss: 67607506120.1913\n",
      "Epoch 519/800\n",
      "4265/4265 [==============================] - 1s 126us/step - loss: 50759559958.8689 - val_loss: 56209440502.9986\n",
      "Epoch 520/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 50898449437.5315 - val_loss: 58717946184.3713\n",
      "Epoch 521/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 50966131389.0739 - val_loss: 56187548598.5485\n",
      "Epoch 522/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 50874512509.5691 - val_loss: 64409501897.6315\n",
      "Epoch 523/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 50965855672.0919 - val_loss: 59182393237.4233\n",
      "Epoch 524/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 50092625178.1102 - val_loss: 69786744788.7932\n",
      "Epoch 525/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 51198028568.7897 - val_loss: 57748907637.3783\n",
      "Epoch 526/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 50307813730.3784 - val_loss: 57252221267.8931\n",
      "Epoch 527/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 50503021826.1008 - val_loss: 57708737669.9409\n",
      "Epoch 528/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 50503032323.0012 - val_loss: 72173540290.0703\n",
      "Epoch 529/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 50495872733.0063 - val_loss: 56099940844.5570\n",
      "Epoch 530/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 50851736557.9930 - val_loss: 59452364359.2911\n",
      "Epoch 531/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 49731058305.0504 - val_loss: 59318414225.1027\n",
      "Epoch 532/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 49944566557.5916 - val_loss: 55711910950.8861\n",
      "Epoch 533/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 49504995998.8220 - val_loss: 61780756949.5134\n",
      "Epoch 534/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 49810414791.0378 - val_loss: 57637998891.5668\n",
      "Epoch 535/800\n",
      "4265/4265 [==============================] - 1s 129us/step - loss: 49369182579.6652 - val_loss: 59808939553.8453\n",
      "Epoch 536/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 50217525823.2647 - val_loss: 60850266025.5865\n",
      "Epoch 537/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 49917358606.2856 - val_loss: 58537533134.6723\n",
      "Epoch 538/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 50476242813.8692 - val_loss: 59051612819.6231\n",
      "Epoch 539/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 49213303453.8617 - val_loss: 75167290334.8748\n",
      "Epoch 540/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 49215718095.3210 - val_loss: 56026243313.9578\n",
      "Epoch 541/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 48893670296.9998 - val_loss: 56276687576.7539\n",
      "Epoch 542/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 49722363610.6054 - val_loss: 69293791995.3193\n",
      "Epoch 543/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 49588385692.6012 - val_loss: 56731583610.4191\n",
      "Epoch 544/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 50181211683.1737 - val_loss: 58240714806.7286\n",
      "Epoch 545/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 49427991291.9784 - val_loss: 55248825417.4515\n",
      "Epoch 546/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 49285017323.4120 - val_loss: 59210079357.2996\n",
      "Epoch 547/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 49632319762.6673 - val_loss: 62734441827.7356\n",
      "Epoch 548/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 49587979703.6117 - val_loss: 54872998671.4824\n",
      "Epoch 549/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 49385900239.9212 - val_loss: 56323936999.1561\n",
      "Epoch 550/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 49414634067.9128 - val_loss: 55137011876.1857\n",
      "Epoch 551/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 48860005370.9580 - val_loss: 58257545322.5767\n",
      "Epoch 552/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 49288072421.7698 - val_loss: 59548768489.3165\n",
      "Epoch 553/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 49006039888.0113 - val_loss: 67608598663.3812\n",
      "Epoch 554/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 48491621805.5278 - val_loss: 57376639917.9072\n",
      "Epoch 555/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 48529047114.0689 - val_loss: 57379097402.6892\n",
      "Epoch 556/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 49064444840.3658 - val_loss: 60705854373.2658\n",
      "Epoch 557/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 49291878824.4858 - val_loss: 65317752152.2138\n",
      "Epoch 558/800\n",
      "4265/4265 [==============================] - 1s 143us/step - loss: 48901284963.6389 - val_loss: 59759509538.5654\n",
      "Epoch 559/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 48610892058.8305 - val_loss: 57702370244.9508\n",
      "Epoch 560/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 48388886756.5693 - val_loss: 56919816517.4909\n",
      "Epoch 561/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 48359288463.2159 - val_loss: 56302561254.0760\n",
      "Epoch 562/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 48451453479.2553 - val_loss: 60846330144.0450\n",
      "Epoch 563/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 48482921062.6401 - val_loss: 60294905046.5935\n",
      "Epoch 564/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 48376198350.2406 - val_loss: 61033003252.8383\n",
      "Epoch 565/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 47932450639.7712 - val_loss: 61127359371.3418\n",
      "Epoch 566/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 48304319640.9397 - val_loss: 62830811922.3629\n",
      "Epoch 567/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 48536444750.3306 - val_loss: 56987919540.0281\n",
      "Epoch 568/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 48094451518.0042 - val_loss: 63049904433.3277\n",
      "Epoch 569/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 48552770600.5759 - val_loss: 54136799376.0225\n",
      "Epoch 570/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 48083989309.7641 - val_loss: 56353434150.1660\n",
      "Epoch 571/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 47576402221.0776 - val_loss: 53932133161.4065\n",
      "Epoch 572/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 48477941868.0422 - val_loss: 56800242150.7961\n",
      "Epoch 573/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 48069608372.8506 - val_loss: 53704955140.6807\n",
      "Epoch 574/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 47927795641.1723 - val_loss: 58602719914.6667\n",
      "Epoch 575/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 47710292169.1986 - val_loss: 54311272119.6287\n",
      "Epoch 576/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 47888655640.1895 - val_loss: 60925919024.6076\n",
      "Epoch 577/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 47872742951.9756 - val_loss: 56619115262.1997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 578/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 48115019077.8073 - val_loss: 57957110491.6343\n",
      "Epoch 579/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 48061028304.9416 - val_loss: 53581320347.5443\n",
      "Epoch 580/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 48185484896.6377 - val_loss: 58303548017.0577\n",
      "Epoch 581/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 47659012235.2544 - val_loss: 55271387689.0464\n",
      "Epoch 582/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 47592023919.4635 - val_loss: 53991650322.7229\n",
      "Epoch 583/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 47772731050.5866 - val_loss: 53066576667.0042\n",
      "Epoch 584/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 47703223279.9137 - val_loss: 53618648624.2475\n",
      "Epoch 585/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 47298789388.4849 - val_loss: 55394917432.1688\n",
      "Epoch 586/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 47870394833.7819 - val_loss: 59759947159.5837\n",
      "Epoch 587/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 47646191685.1470 - val_loss: 59923138591.6850\n",
      "Epoch 588/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 47878241866.0689 - val_loss: 59095444374.8636\n",
      "Epoch 589/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 47269677184.2101 - val_loss: 54100496798.7848\n",
      "Epoch 590/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 47373811784.9885 - val_loss: 58787019221.5134\n",
      "Epoch 591/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 47861113548.4399 - val_loss: 57901118788.0506\n",
      "Epoch 592/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 47235875686.5801 - val_loss: 66446409670.3910\n",
      "Epoch 593/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 47271836694.3287 - val_loss: 59726259875.4655\n",
      "Epoch 594/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 47281803746.1083 - val_loss: 59452938605.8172\n",
      "Epoch 595/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 47495826689.6206 - val_loss: 70680758309.4458\n",
      "Epoch 596/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 46936755798.5538 - val_loss: 56091162598.0760\n",
      "Epoch 597/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 47489127097.2324 - val_loss: 61093645304.7989\n",
      "Epoch 598/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 47131195989.8335 - val_loss: 54074254049.3952\n",
      "Epoch 599/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 47122415526.6851 - val_loss: 56680974068.1181\n",
      "Epoch 600/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 47491189481.7313 - val_loss: 54575644366.6723\n",
      "Epoch 601/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 46646173358.9083 - val_loss: 53598055439.8425\n",
      "Epoch 602/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 47112089036.2598 - val_loss: 58919871080.4163\n",
      "Epoch 603/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 47139575818.0839 - val_loss: 52987989474.4754\n",
      "Epoch 604/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 47013979241.4012 - val_loss: 57822971238.6160\n",
      "Epoch 605/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 47330517309.8842 - val_loss: 54323989145.3840\n",
      "Epoch 606/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 47207706505.8739 - val_loss: 54140433052.2644\n",
      "Epoch 607/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 46153835859.7327 - val_loss: 60851736702.7398\n",
      "Epoch 608/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 47071205274.6804 - val_loss: 52918408331.7018\n",
      "Epoch 609/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 46462052597.3758 - val_loss: 62506592333.7721\n",
      "Epoch 610/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 47120603996.9763 - val_loss: 55240799872.9001\n",
      "Epoch 611/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 46743081874.2771 - val_loss: 54092812408.9789\n",
      "Epoch 612/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 46911169623.1540 - val_loss: 72810805919.1449\n",
      "Epoch 613/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 47390508115.3125 - val_loss: 56966422597.1308\n",
      "Epoch 614/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 46402573796.7494 - val_loss: 63618563623.6062\n",
      "Epoch 615/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 46763625067.9222 - val_loss: 58966468040.5513\n",
      "Epoch 616/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 47000676514.7836 - val_loss: 60399750102.2335\n",
      "Epoch 617/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 46566909275.4157 - val_loss: 56341318285.8622\n",
      "Epoch 618/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 46344806518.1261 - val_loss: 53323993888.7651\n",
      "Epoch 619/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 47008983246.0005 - val_loss: 55145138368.9902\n",
      "Epoch 620/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 46603404257.9883 - val_loss: 60811522243.8706\n",
      "Epoch 621/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 46644428057.6300 - val_loss: 52424044895.4149\n",
      "Epoch 622/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 46021121521.2342 - val_loss: 58768838252.7370\n",
      "Epoch 623/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 46388681515.9972 - val_loss: 80612208225.2152\n",
      "Epoch 624/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 45858282763.2244 - val_loss: 55907717932.2869\n",
      "Epoch 625/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 45902204980.8206 - val_loss: 51809120999.1561\n",
      "Epoch 626/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 46242391297.1404 - val_loss: 54228389041.1477\n",
      "Epoch 627/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 46647306775.4091 - val_loss: 61844086461.3896\n",
      "Epoch 628/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 46548774909.1189 - val_loss: 59797871113.3615\n",
      "Epoch 629/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 46345166878.2518 - val_loss: 62521794738.5879\n",
      "Epoch 630/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 46338919649.9283 - val_loss: 52301906242.6104\n",
      "Epoch 631/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 46261886573.6028 - val_loss: 53754777173.6934\n",
      "Epoch 632/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 45843964703.9925 - val_loss: 55981920152.3038\n",
      "Epoch 633/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 46375999310.8108 - val_loss: 55456200781.7721\n",
      "Epoch 634/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 46818077714.4872 - val_loss: 55588661308.4894\n",
      "Epoch 635/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 45901306939.7834 - val_loss: 70786977930.2616\n",
      "Epoch 636/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 46618435833.6975 - val_loss: 62405075652.5907\n",
      "Epoch 637/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 45948979610.5604 - val_loss: 55093718129.7778\n",
      "Epoch 638/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 46159326056.5008 - val_loss: 56168895293.5696\n",
      "Epoch 639/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 45790843551.3022 - val_loss: 53160618822.2110\n",
      "Epoch 640/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 46378038512.8141 - val_loss: 58037127119.0323\n",
      "Epoch 641/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 46058607624.4033 - val_loss: 73320726873.6540\n",
      "Epoch 642/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 46048598302.6720 - val_loss: 54482210602.8467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 643/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 45958250956.9801 - val_loss: 52047378089.2264\n",
      "Epoch 644/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 45815223137.2980 - val_loss: 52033612257.0352\n",
      "Epoch 645/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 46518311778.7386 - val_loss: 61469528167.6962\n",
      "Epoch 646/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 45840786603.6671 - val_loss: 55245490570.6217\n",
      "Epoch 647/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 46231324143.3135 - val_loss: 55183171198.0197\n",
      "Epoch 648/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 46040479529.3562 - val_loss: 56074833896.9564\n",
      "Epoch 649/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 46061223870.6945 - val_loss: 53270449369.4740\n",
      "Epoch 650/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 45387425943.2591 - val_loss: 57681522227.1280\n",
      "Epoch 651/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 46256976572.1135 - val_loss: 54512134964.9283\n",
      "Epoch 652/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 45605240232.7259 - val_loss: 54672899404.6920\n",
      "Epoch 653/800\n",
      "4265/4265 [==============================] - 1s 146us/step - loss: 45668516458.4816 - val_loss: 61223780449.9353\n",
      "Epoch 654/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 46466736654.2856 - val_loss: 59926409158.3910\n",
      "Epoch 655/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 46066757707.8696 - val_loss: 52063818661.2658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f7caa4860>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_train11 = model11.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train11'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_train22 = model22.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train22'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4265 samples, validate on 2844 samples\n",
      "Epoch 1/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132773082073613.8125 - val_loss: 133088626495470.7188\n",
      "Epoch 2/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132773078372005.2969 - val_loss: 133088611051465.2812\n",
      "Epoch 3/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132773064655205.2500 - val_loss: 133088611051465.2812\n",
      "Epoch 4/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132773063435759.3125 - val_loss: 133088610107599.3906\n",
      "Epoch 5/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132773063726852.8594 - val_loss: 133088610107599.3906\n",
      "Epoch 6/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132773062027495.9375 - val_loss: 133088608219867.6406\n",
      "Epoch 7/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132773052368303.9375 - val_loss: 133088593247795.1094\n",
      "Epoch 8/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132773044805772.2188 - val_loss: 133088590416197.4688\n",
      "Epoch 9/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132773041706019.2969 - val_loss: 133088583997909.5312\n",
      "Epoch 10/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132773037888366.7500 - val_loss: 133088582015791.1719\n",
      "Epoch 11/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132773032432329.5469 - val_loss: 133088567787013.0625\n",
      "Epoch 12/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132773019775660.7500 - val_loss: 133088566465600.8125\n",
      "Epoch 13/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132773019805163.4688 - val_loss: 133088566465600.8125\n",
      "Epoch 14/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132773019895638.4844 - val_loss: 133088564955415.4219\n",
      "Epoch 15/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132773016614935.5312 - val_loss: 133088548945090.4375\n",
      "Epoch 16/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 132773003222665.4531 - val_loss: 133088547623678.1875\n",
      "Epoch 17/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132773000728701.8125 - val_loss: 133088541205390.2188\n",
      "Epoch 18/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772994478057.9062 - val_loss: 133088540072751.1719\n",
      "Epoch 19/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772992456137.8594 - val_loss: 133088523767468.1094\n",
      "Epoch 20/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132772977787383.2344 - val_loss: 133088521690963.1719\n",
      "Epoch 21/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 132772976750854.1875 - val_loss: 133088521690963.1719\n",
      "Epoch 22/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 132772976137197.5000 - val_loss: 133088520652710.6875\n",
      "Epoch 23/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 132772971432496.3750 - val_loss: 133088505763226.4688\n",
      "Epoch 24/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 132772958653882.9688 - val_loss: 133088503875494.6875\n",
      "Epoch 25/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132772956085179.0938 - val_loss: 133088497079660.3906\n",
      "Epoch 26/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132772950220037.4688 - val_loss: 133088495191928.6094\n",
      "Epoch 27/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132772944457171.9531 - val_loss: 133088480503015.8906\n",
      "Epoch 28/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 132772933265805.1250 - val_loss: 133088477765804.8281\n",
      "Epoch 29/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772931853608.0469 - val_loss: 133088477577031.6719\n",
      "Epoch 30/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132772931873276.5312 - val_loss: 133088476350006.0312\n",
      "Epoch 31/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132772922282924.2031 - val_loss: 133088460799815.6719\n",
      "Epoch 32/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132772915157032.8125 - val_loss: 133088459006470.4688\n",
      "Epoch 33/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772909577084.1875 - val_loss: 133088453532048.3906\n",
      "Epoch 34/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772905600116.9219 - val_loss: 133088452399409.3281\n",
      "Epoch 35/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772892276686.5469 - val_loss: 133088435150260.3906\n",
      "Epoch 36/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772888262349.1562 - val_loss: 133088435055873.8125\n",
      "Epoch 37/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772888598680.2188 - val_loss: 133088433262528.6094\n",
      "Epoch 38/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772882344102.6250 - val_loss: 133088418962960.5625\n",
      "Epoch 39/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132772870944249.8750 - val_loss: 133088416225749.5312\n",
      "Epoch 40/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132772867525867.5312 - val_loss: 133088409347326.9219\n",
      "Epoch 41/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772862502536.9688 - val_loss: 133088407742754.9219\n",
      "Epoch 42/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772850813557.5312 - val_loss: 133088393584766.7188\n",
      "Epoch 43/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132772846158027.6094 - val_loss: 133088391319488.6094\n",
      "Epoch 44/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772844757631.6094 - val_loss: 133088389714916.6406\n",
      "Epoch 45/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772838943628.0469 - val_loss: 133088373881566.5312\n",
      "Epoch 46/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772828208570.0156 - val_loss: 133088371993834.7500\n",
      "Epoch 47/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132772823004289.4062 - val_loss: 133088366708185.8281\n",
      "Epoch 48/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772819711785.3594 - val_loss: 133088364159747.9375\n",
      "Epoch 49/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132772804098943.5469 - val_loss: 133088347299943.6875\n",
      "Epoch 50/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772801746592.9844 - val_loss: 133088347205557.1094\n",
      "Epoch 51/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772801842968.5469 - val_loss: 133088345789758.3125\n",
      "Epoch 52/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772790777480.0156 - val_loss: 133088332056509.7500\n",
      "Epoch 53/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132772783816803.8750 - val_loss: 133088330451937.7500\n",
      "Epoch 54/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772777577961.0781 - val_loss: 133088320824505.7812\n",
      "Epoch 55/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772769978059.2500 - val_loss: 133088306489542.7500\n",
      "Epoch 56/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132772759260702.8438 - val_loss: 133088304413037.8125\n",
      "Epoch 57/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132772758342184.7031 - val_loss: 133088304413037.8125\n",
      "Epoch 58/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772756564153.8281 - val_loss: 133088287447048.6406\n",
      "Epoch 59/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132772741789189.4062 - val_loss: 133088284615451.0000\n",
      "Epoch 60/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132772737556531.8594 - val_loss: 133088279707348.4375\n",
      "Epoch 61/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772732946239.4531 - val_loss: 133088278008389.8281\n",
      "Epoch 62/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772719968974.3594 - val_loss: 133088259650198.5000\n",
      "Epoch 63/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132772714809931.2812 - val_loss: 133088259650198.5000\n",
      "Epoch 64/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132772714135302.2969 - val_loss: 133088257856853.3281\n",
      "Epoch 65/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772704324662.9844 - val_loss: 133088244383167.9219\n",
      "Epoch 66/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132772695591856.5312 - val_loss: 133088236171534.7812\n",
      "Epoch 67/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132772689431687.6562 - val_loss: 133088233056777.3594\n",
      "Epoch 68/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772680150130.5156 - val_loss: 133088218922385.8281\n",
      "Epoch 69/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132772672390913.9844 - val_loss: 133088217317813.8281\n",
      "Epoch 70/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772671065258.2344 - val_loss: 133088215241308.8906\n",
      "Epoch 71/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132772664145885.9062 - val_loss: 133088200458009.5625\n",
      "Epoch 72/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772652730298.3750 - val_loss: 133088198287118.0625\n",
      "Epoch 73/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132772647895785.2500 - val_loss: 133088190075484.8906\n",
      "Epoch 74/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132772640935109.1250 - val_loss: 133088174430907.9375\n",
      "Epoch 75/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772628921599.7031 - val_loss: 133088173203882.3125\n",
      "Epoch 76/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772627649048.8438 - val_loss: 133088173203882.3125\n",
      "Epoch 77/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772622196945.3594 - val_loss: 133088156615439.4688\n",
      "Epoch 78/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772610602374.6406 - val_loss: 133088153972615.0312\n",
      "Epoch 79/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132772604943752.0781 - val_loss: 133088148026259.9688\n",
      "Epoch 80/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132772598665572.2969 - val_loss: 133088131626590.3281\n",
      "Epoch 81/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772585369677.7969 - val_loss: 133088130116404.9219\n",
      "Epoch 82/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772584075491.6094 - val_loss: 133088130116404.9219\n",
      "Epoch 83/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132772580308977.1094 - val_loss: 133088113256600.6719\n",
      "Epoch 84/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772566698386.8750 - val_loss: 133088111746415.2500\n",
      "Epoch 85/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132772561165642.6094 - val_loss: 133088106271993.1719\n",
      "Epoch 86/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772554387883.3594 - val_loss: 133088088362138.1094\n",
      "Epoch 87/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132772541432253.6250 - val_loss: 133088087135112.4688\n",
      "Epoch 88/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772540934641.0000 - val_loss: 133088087135112.4688\n",
      "Epoch 89/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772535632017.9844 - val_loss: 133088068847711.0625\n",
      "Epoch 90/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772522564277.8750 - val_loss: 133088066959979.3125\n",
      "Epoch 91/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772516754207.9844 - val_loss: 133088058311808.1875\n",
      "Epoch 92/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772509315587.7344 - val_loss: 133088044354391.5000\n",
      "Epoch 93/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772498407447.0469 - val_loss: 133088042761617.8281\n",
      "Epoch 94/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772497081791.2969 - val_loss: 133088040968272.6719\n",
      "Epoch 95/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772489363878.5625 - val_loss: 133088025583258.8281\n",
      "Epoch 96/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772479655515.3594 - val_loss: 133088023884300.2188\n",
      "Epoch 97/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772471514730.2500 - val_loss: 133088015955826.8594\n",
      "Epoch 98/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132772462237106.8125 - val_loss: 133088000134275.0625\n",
      "Epoch 99/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132772454202531.5000 - val_loss: 133087998907249.4219\n",
      "Epoch 100/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772452684124.6094 - val_loss: 133087997680223.7812\n",
      "Epoch 101/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772442124116.0938 - val_loss: 133087981953058.5625\n",
      "Epoch 102/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772434071839.1406 - val_loss: 133087974319543.2812\n",
      "Epoch 103/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772427197704.3438 - val_loss: 133087973092517.6094\n",
      "Epoch 104/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772414065058.2500 - val_loss: 133087954805116.2188\n",
      "Epoch 105/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772409120401.6094 - val_loss: 133087954710729.6094\n",
      "Epoch 106/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772408746700.4375 - val_loss: 133087953011771.0625\n",
      "Epoch 107/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772392775892.2500 - val_loss: 133087937272807.5312\n",
      "Epoch 108/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772388098726.9844 - val_loss: 133087930854519.5312\n",
      "Epoch 109/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772381195089.4375 - val_loss: 133087913428395.7500\n",
      "Epoch 110/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772367621869.3281 - val_loss: 133087911163117.6406\n",
      "Epoch 111/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132772365440634.5625 - val_loss: 133087910596798.1094\n",
      "Epoch 112/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132772358599936.1875 - val_loss: 133087894480288.2188\n",
      "Epoch 113/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132772348450998.9531 - val_loss: 133087892403783.3125\n",
      "Epoch 114/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132772339838170.2500 - val_loss: 133087885230402.6094\n",
      "Epoch 115/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132772329813144.4531 - val_loss: 133087870234733.4375\n",
      "Epoch 116/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132772323029484.6719 - val_loss: 133087868641959.7812\n",
      "Epoch 117/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132772321935917.0156 - val_loss: 133087866848614.6094\n",
      "Epoch 118/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772307890653.2969 - val_loss: 133087848938759.5625\n",
      "Epoch 119/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132772301663611.5781 - val_loss: 133087843086791.1094\n",
      "Epoch 120/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132772294881918.6562 - val_loss: 133087826132600.2812\n",
      "Epoch 121/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132772280177760.7656 - val_loss: 133087824433641.6719\n",
      "Epoch 122/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772278938646.3281 - val_loss: 133087823218414.3594\n",
      "Epoch 123/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 77us/step - loss: 132772271618036.9531 - val_loss: 133087808033972.0312\n",
      "Epoch 124/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132772260359797.2812 - val_loss: 133087799633565.6875\n",
      "Epoch 125/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772253849529.4062 - val_loss: 133087798312153.4688\n",
      "Epoch 126/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772241118120.3750 - val_loss: 133087781534937.4688\n",
      "Epoch 127/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772236381949.6562 - val_loss: 133087781346164.3125\n",
      "Epoch 128/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772233874218.0781 - val_loss: 133087764014427.0781\n",
      "Epoch 129/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132772217726393.5312 - val_loss: 133087761749148.9688\n",
      "Epoch 130/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772212529980.3125 - val_loss: 133087753820675.5781\n",
      "Epoch 131/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132772204446233.8125 - val_loss: 133087738754216.5000\n",
      "Epoch 132/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772192019686.2500 - val_loss: 133087736960871.3281\n",
      "Epoch 133/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132772191767929.6719 - val_loss: 133087735828232.2812\n",
      "Epoch 134/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772180014044.2188 - val_loss: 133087719888697.2500\n",
      "Epoch 135/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772170844597.4531 - val_loss: 133087711688862.4219\n",
      "Epoch 136/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772165187941.7344 - val_loss: 133087696787579.8594\n",
      "Epoch 137/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772149531829.2656 - val_loss: 133087695300991.0781\n",
      "Epoch 138/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772147537445.0938 - val_loss: 133087692941326.4219\n",
      "Epoch 139/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132772140895398.3750 - val_loss: 133087677579909.2188\n",
      "Epoch 140/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132772129910550.6250 - val_loss: 133087669002528.0625\n",
      "Epoch 141/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772121948748.7188 - val_loss: 133087668247435.3281\n",
      "Epoch 142/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772109746421.8594 - val_loss: 133087649759462.4375\n",
      "Epoch 143/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772104848969.5781 - val_loss: 133087649570689.2812\n",
      "Epoch 144/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772102022608.5781 - val_loss: 133087633171019.6094\n",
      "Epoch 145/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132772087414826.2500 - val_loss: 133087631094514.6719\n",
      "Epoch 146/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132772080733442.5781 - val_loss: 133087623284024.5312\n",
      "Epoch 147/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772069233280.5625 - val_loss: 133087608300153.6875\n",
      "Epoch 148/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132772060946948.6875 - val_loss: 133087606884354.8906\n",
      "Epoch 149/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132772060170043.5938 - val_loss: 133087605185396.3125\n",
      "Epoch 150/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772044203169.1094 - val_loss: 133087587181154.6719\n",
      "Epoch 151/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132772038375397.5781 - val_loss: 133087580857253.2812\n",
      "Epoch 152/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132772028952227.3906 - val_loss: 133087565212676.3281\n",
      "Epoch 153/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132772016989856.0312 - val_loss: 133087563041784.8125\n",
      "Epoch 154/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132772017361590.3594 - val_loss: 133087560965279.8594\n",
      "Epoch 155/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132772003239619.5625 - val_loss: 133087544671795.1094\n",
      "Epoch 156/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771995618082.3906 - val_loss: 133087539102986.4375\n",
      "Epoch 157/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771986999353.1406 - val_loss: 133087520909971.6094\n",
      "Epoch 158/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771974525601.2344 - val_loss: 133087519588559.3906\n",
      "Epoch 159/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771973125205.2344 - val_loss: 133087518172760.5781\n",
      "Epoch 160/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771960844204.4375 - val_loss: 133087502056250.6875\n",
      "Epoch 161/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771953025982.4531 - val_loss: 133087494611508.5625\n",
      "Epoch 162/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132771945007141.9375 - val_loss: 133087478211838.9219\n",
      "Epoch 163/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132771931624706.0938 - val_loss: 133087476135333.9688\n",
      "Epoch 164/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771929703095.3125 - val_loss: 133087474058829.0625\n",
      "Epoch 165/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132771918918866.0938 - val_loss: 133087457281613.0625\n",
      "Epoch 166/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132771909320646.3750 - val_loss: 133087451807190.9375\n",
      "Epoch 167/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771902373738.1875 - val_loss: 133087434003520.8125\n",
      "Epoch 168/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132771887130663.8594 - val_loss: 133087432115789.0625\n",
      "Epoch 169/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132771886312454.9688 - val_loss: 133087429756124.3594\n",
      "Epoch 170/800\n",
      "4265/4265 [==============================] - 0s 63us/step - loss: 132771874792624.4688 - val_loss: 133087416365027.1875\n",
      "Epoch 171/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771866903595.9375 - val_loss: 133087408377562.1875\n",
      "Epoch 172/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132771858686103.7344 - val_loss: 133087390821656.8281\n",
      "Epoch 173/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771844282873.6406 - val_loss: 133087388745151.9219\n",
      "Epoch 174/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771843563007.1562 - val_loss: 133087387164176.5625\n",
      "Epoch 175/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771832505386.0156 - val_loss: 133087369148136.5781\n",
      "Epoch 176/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771821597245.3438 - val_loss: 133087363591126.2188\n",
      "Epoch 177/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771815533452.0469 - val_loss: 133087347108868.3281\n",
      "Epoch 178/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771800725051.1719 - val_loss: 133087345409909.7188\n",
      "Epoch 179/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771798518247.3906 - val_loss: 133087343239018.2188\n",
      "Epoch 180/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132771787918901.9062 - val_loss: 133087327205096.5781\n",
      "Epoch 181/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771778104328.8750 - val_loss: 133087320892993.5312\n",
      "Epoch 182/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771770181863.9375 - val_loss: 133087303266298.2188\n",
      "Epoch 183/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132771757216399.9375 - val_loss: 133087301756112.8281\n",
      "Epoch 184/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771755119739.6406 - val_loss: 133087300434700.6094\n",
      "Epoch 185/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771742055933.2500 - val_loss: 133087282996778.4688\n",
      "Epoch 186/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771734754992.3594 - val_loss: 133087277050423.4375\n",
      "Epoch 187/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132771726262141.3906 - val_loss: 133087261134485.0625\n",
      "Epoch 188/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771714417780.9219 - val_loss: 133087258869206.9375\n",
      "Epoch 189/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132771712425363.5938 - val_loss: 133087256981475.1875\n",
      "Epoch 190/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132771697811680.7344 - val_loss: 133087239071620.1406\n",
      "Epoch 191/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771689820376.0781 - val_loss: 133087232936491.9219\n",
      "Epoch 192/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771680802376.6250 - val_loss: 133087216631208.8594\n",
      "Epoch 193/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771668902944.4219 - val_loss: 133087214554703.9375\n",
      "Epoch 194/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771669803760.9375 - val_loss: 133087212006266.0625\n",
      "Epoch 195/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132771653252732.5000 - val_loss: 133087196255504.1875\n",
      "Epoch 196/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771646073736.1875 - val_loss: 133087188161854.3125\n",
      "Epoch 197/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771634740756.2969 - val_loss: 133087172139731.0000\n",
      "Epoch 198/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771625948944.3750 - val_loss: 133087170830117.0781\n",
      "Epoch 199/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771623059644.2188 - val_loss: 133087154607422.3125\n",
      "Epoch 200/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771607942448.2031 - val_loss: 133087152353942.5000\n",
      "Epoch 201/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771601371208.0312 - val_loss: 133087144047922.7812\n",
      "Epoch 202/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771588142186.3594 - val_loss: 133087128769093.8281\n",
      "Epoch 203/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771581867940.2812 - val_loss: 133087128769093.8281\n",
      "Epoch 204/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132771576486643.3281 - val_loss: 133087110493490.7812\n",
      "Epoch 205/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771563709996.7656 - val_loss: 133087102671202.3125\n",
      "Epoch 206/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771556060923.7344 - val_loss: 133087101066630.3125\n",
      "Epoch 207/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132771541315462.0312 - val_loss: 133087084100641.1094\n",
      "Epoch 208/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771538243244.9844 - val_loss: 133087081929749.6094\n",
      "Epoch 209/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771529154439.0000 - val_loss: 133087067311626.8125\n",
      "Epoch 210/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771518307270.6250 - val_loss: 133087059099993.6719\n",
      "Epoch 211/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771510652297.0312 - val_loss: 133087042157601.1094\n",
      "Epoch 212/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771494620516.5312 - val_loss: 133087040281667.6875\n",
      "Epoch 213/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132771494602814.9062 - val_loss: 133087038016389.5781\n",
      "Epoch 214/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132771482270676.0625 - val_loss: 133087020672854.0625\n",
      "Epoch 215/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771473238908.6719 - val_loss: 133087014726499.0312\n",
      "Epoch 216/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771463823605.8594 - val_loss: 133086998893148.8906\n",
      "Epoch 217/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771451611444.7500 - val_loss: 133086996816643.9375\n",
      "Epoch 218/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771450238584.6406 - val_loss: 133086995400845.1406\n",
      "Epoch 219/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132771435113521.2188 - val_loss: 133086977868536.4375\n",
      "Epoch 220/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132771427202857.3594 - val_loss: 133086970411996.0000\n",
      "Epoch 221/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132771415016265.2812 - val_loss: 133086954684830.7812\n",
      "Epoch 222/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771406910883.4375 - val_loss: 133086952136392.9219\n",
      "Epoch 223/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771403793428.8906 - val_loss: 133086935819311.5312\n",
      "Epoch 224/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771389226950.3750 - val_loss: 133086927713863.3125\n",
      "Epoch 225/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771383125786.9531 - val_loss: 133086926014904.6875\n",
      "Epoch 226/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132771368311485.5625 - val_loss: 133086909910193.1406\n",
      "Epoch 227/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771364002120.9219 - val_loss: 133086907928074.8125\n",
      "Epoch 228/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771355924274.9531 - val_loss: 133086892472271.0312\n",
      "Epoch 229/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771343472158.3750 - val_loss: 133086884366822.8125\n",
      "Epoch 230/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771335681472.2500 - val_loss: 133086867577808.4688\n",
      "Epoch 231/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132771321185800.2812 - val_loss: 133086865312530.3594\n",
      "Epoch 232/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132771319222885.6719 - val_loss: 133086863707958.3906\n",
      "Epoch 233/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132771306351830.4062 - val_loss: 133086847685835.0781\n",
      "Epoch 234/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132771297674095.7031 - val_loss: 133086840323681.2188\n",
      "Epoch 235/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771287680539.5000 - val_loss: 133086824218969.6719\n",
      "Epoch 236/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771277163801.6406 - val_loss: 133086823086330.5781\n",
      "Epoch 237/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771272699056.0000 - val_loss: 133086805459635.3125\n",
      "Epoch 238/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771258687228.7031 - val_loss: 133086804326996.2500\n",
      "Epoch 239/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132771250788365.9219 - val_loss: 133086794805749.1875\n",
      "Epoch 240/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771236910284.3281 - val_loss: 133086778783625.9219\n",
      "Epoch 241/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132771232890046.3906 - val_loss: 133086776707120.9688\n",
      "Epoch 242/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771224349991.0625 - val_loss: 133086762289569.6719\n",
      "Epoch 243/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132771212434824.0781 - val_loss: 133086753712188.4688\n",
      "Epoch 244/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771205727871.3750 - val_loss: 133086737690065.1875\n",
      "Epoch 245/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771189536776.1562 - val_loss: 133086734764080.9688\n",
      "Epoch 246/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132771188451075.9062 - val_loss: 133086732970735.8125\n",
      "Epoch 247/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771175033236.7969 - val_loss: 133086716559267.8281\n",
      "Epoch 248/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771165759547.0625 - val_loss: 133086708371231.3281\n",
      "Epoch 249/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132771153700800.1406 - val_loss: 133086693552537.0312\n",
      "Epoch 250/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132771146262179.8594 - val_loss: 133086690933309.1875\n",
      "Epoch 251/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132771141091335.6875 - val_loss: 133086675182547.3594\n",
      "Epoch 252/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771127984258.6094 - val_loss: 133086666805737.6719\n",
      "Epoch 253/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771120053926.2656 - val_loss: 133086664823619.3281\n",
      "Epoch 254/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132771105219956.3750 - val_loss: 133086646630604.5312\n",
      "Epoch 255/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132771102169374.6719 - val_loss: 133086645120419.1094\n",
      "Epoch 256/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771090092926.1094 - val_loss: 133086629298867.3125\n",
      "Epoch 257/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132771080713026.5625 - val_loss: 133086623446898.8594\n",
      "Epoch 258/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771070750939.9219 - val_loss: 133086607047229.1875\n",
      "Epoch 259/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132771059433694.7969 - val_loss: 133086605159497.4375\n",
      "Epoch 260/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132771056326074.4844 - val_loss: 133086587721575.3281\n",
      "Epoch 261/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132771040070073.2969 - val_loss: 133086586305776.5312\n",
      "Epoch 262/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132771033829263.6406 - val_loss: 133086577622210.4375\n",
      "Epoch 263/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132771020383888.6562 - val_loss: 133086561529297.1875\n",
      "Epoch 264/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132771014554150.2969 - val_loss: 133086559264019.0781\n",
      "Epoch 265/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132771006122271.6250 - val_loss: 133086544362736.5312\n",
      "Epoch 266/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770994216938.8750 - val_loss: 133086535690968.7500\n",
      "Epoch 267/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132770985700485.7344 - val_loss: 133086520223366.6719\n",
      "Epoch 268/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132770973403750.1562 - val_loss: 133086518052475.1406\n",
      "Epoch 269/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770970307930.9375 - val_loss: 133086516353516.5625\n",
      "Epoch 270/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770955239906.1094 - val_loss: 133086499399325.6875\n",
      "Epoch 271/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132770947525927.0625 - val_loss: 133086491116902.6094\n",
      "Epoch 272/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770934853523.4844 - val_loss: 133086474233501.6875\n",
      "Epoch 273/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770928791697.0156 - val_loss: 133086474139115.1094\n",
      "Epoch 274/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770919826802.4688 - val_loss: 133086457350100.8125\n",
      "Epoch 275/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770908033580.0469 - val_loss: 133086447640080.5625\n",
      "Epoch 276/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770900390407.5625 - val_loss: 133086431511772.3594\n",
      "Epoch 277/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132770885314515.3438 - val_loss: 133086430851066.2188\n",
      "Epoch 278/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770882814651.1406 - val_loss: 133086428975132.8125\n",
      "Epoch 279/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770869030978.2656 - val_loss: 133086410038823.6094\n",
      "Epoch 280/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770859920536.9531 - val_loss: 133086402594081.4688\n",
      "Epoch 281/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770847999469.3906 - val_loss: 133086386748933.0625\n",
      "Epoch 282/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770840617887.7188 - val_loss: 133086386654546.4375\n",
      "Epoch 283/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132770834023045.3750 - val_loss: 133086369393599.1875\n",
      "Epoch 284/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770821488321.1562 - val_loss: 133086361004991.1875\n",
      "Epoch 285/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770812901061.4844 - val_loss: 133086345478397.4688\n",
      "Epoch 286/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132770798562737.3750 - val_loss: 133086343213119.3906\n",
      "Epoch 287/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770796322497.1562 - val_loss: 133086341608547.3906\n",
      "Epoch 288/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132770781659643.0781 - val_loss: 133086324736944.7812\n",
      "Epoch 289/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770773446084.5781 - val_loss: 133086316902857.9688\n",
      "Epoch 290/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132770762577280.8750 - val_loss: 133086299087389.5312\n",
      "Epoch 291/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770753769734.1875 - val_loss: 133086299087389.5312\n",
      "Epoch 292/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770746154097.5625 - val_loss: 133086282782106.4688\n",
      "Epoch 293/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770735080741.6250 - val_loss: 133086274476086.7188\n",
      "Epoch 294/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770724550235.8438 - val_loss: 133086257049962.9375\n",
      "Epoch 295/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132770711152065.2188 - val_loss: 133086255539777.5312\n",
      "Epoch 296/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132770710200110.6406 - val_loss: 133086253463272.5781\n",
      "Epoch 297/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770693731689.8281 - val_loss: 133086235930963.8906\n",
      "Epoch 298/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770686873289.7969 - val_loss: 133086229323902.7188\n",
      "Epoch 299/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770673656069.2188 - val_loss: 133086212641073.3281\n",
      "Epoch 300/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770665739504.8125 - val_loss: 133086212641073.3281\n",
      "Epoch 301/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770658609679.7188 - val_loss: 133086194837403.1875\n",
      "Epoch 302/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132770646607971.4062 - val_loss: 133086186908929.8125\n",
      "Epoch 303/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770637973507.3594 - val_loss: 133086170981193.0781\n",
      "Epoch 304/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770624056088.7812 - val_loss: 133086169093461.3281\n",
      "Epoch 305/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132770621554257.7500 - val_loss: 133086166639410.0625\n",
      "Epoch 306/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 77us/step - loss: 132770607023182.5156 - val_loss: 133086149685219.1875\n",
      "Epoch 307/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132770598077956.4531 - val_loss: 133086142700611.6875\n",
      "Epoch 308/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132770584386725.4219 - val_loss: 133086124401411.9375\n",
      "Epoch 309/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132770579168676.8750 - val_loss: 133086123363159.5000\n",
      "Epoch 310/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132770569059076.6250 - val_loss: 133086107553406.0312\n",
      "Epoch 311/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132770558217808.7812 - val_loss: 133086100757571.6875\n",
      "Epoch 312/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132770549005091.3594 - val_loss: 133086084440490.3125\n",
      "Epoch 313/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132770536728024.2656 - val_loss: 133086082552758.5625\n",
      "Epoch 314/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 132770533647939.8281 - val_loss: 133086063899609.1094\n",
      "Epoch 315/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132770517747938.1719 - val_loss: 133086062672583.4688\n",
      "Epoch 316/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 132770510690886.4688 - val_loss: 133086054366563.7188\n",
      "Epoch 317/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770496852141.8281 - val_loss: 133086038356238.7812\n",
      "Epoch 318/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770493193804.0000 - val_loss: 133086036657280.1875\n",
      "Epoch 319/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770480810527.0938 - val_loss: 133086018853610.0312\n",
      "Epoch 320/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770471139534.0000 - val_loss: 133086011774615.9375\n",
      "Epoch 321/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770459411217.5781 - val_loss: 133085997156493.1406\n",
      "Epoch 322/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132770448577817.1562 - val_loss: 133085994702441.8594\n",
      "Epoch 323/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770444927346.7031 - val_loss: 133085977535881.1875\n",
      "Epoch 324/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770429890791.4531 - val_loss: 133085970386097.1406\n",
      "Epoch 325/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770422214182.5312 - val_loss: 133085951827334.3125\n",
      "Epoch 326/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770406943572.3281 - val_loss: 133085950411535.4688\n",
      "Epoch 327/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770404518448.3750 - val_loss: 133085948335030.5625\n",
      "Epoch 328/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132770390740676.0469 - val_loss: 133085932784840.1875\n",
      "Epoch 329/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770381303737.8906 - val_loss: 133085923534954.5781\n",
      "Epoch 330/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770368519223.9375 - val_loss: 133085908279722.3125\n",
      "Epoch 331/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132770362221375.6875 - val_loss: 133085908279722.3125\n",
      "Epoch 332/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132770353860303.5625 - val_loss: 133085889237228.1875\n",
      "Epoch 333/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770340959745.5625 - val_loss: 133085881308754.8125\n",
      "Epoch 334/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132770330846211.6094 - val_loss: 133085865286631.5312\n",
      "Epoch 335/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770318537674.9531 - val_loss: 133085863210126.5781\n",
      "Epoch 336/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132770315550032.3594 - val_loss: 133085846338524.0000\n",
      "Epoch 337/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132770299673632.9062 - val_loss: 133085844167632.4688\n",
      "Epoch 338/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132770292724757.8438 - val_loss: 133085838315664.0312\n",
      "Epoch 339/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132770278266455.9844 - val_loss: 133085820040060.9375\n",
      "Epoch 340/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132770274018063.6562 - val_loss: 133085818435488.9375\n",
      "Epoch 341/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770262777525.6406 - val_loss: 133085800159885.8594\n",
      "Epoch 342/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770252593185.1406 - val_loss: 133085792325799.0625\n",
      "Epoch 343/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132770240510836.0156 - val_loss: 133085779040886.8125\n",
      "Epoch 344/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132770231048328.8438 - val_loss: 133085777153155.0625\n",
      "Epoch 345/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132770224825220.8438 - val_loss: 133085760092779.3125\n",
      "Epoch 346/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132770211014012.0625 - val_loss: 133085751987331.0625\n",
      "Epoch 347/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770202576232.8594 - val_loss: 133085733983089.4219\n",
      "Epoch 348/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132770187958616.2812 - val_loss: 133085732000971.0781\n",
      "Epoch 349/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132770186042906.0625 - val_loss: 133085730207625.9219\n",
      "Epoch 350/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132770171073223.6406 - val_loss: 133085715034981.8906\n",
      "Epoch 351/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770163203863.5781 - val_loss: 133085705136188.4688\n",
      "Epoch 352/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132770149376920.0312 - val_loss: 133085690435477.4219\n",
      "Epoch 353/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132770143808772.5000 - val_loss: 133085688465157.4219\n",
      "Epoch 354/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132770134346265.3438 - val_loss: 133085669988982.8125\n",
      "Epoch 355/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132770121955121.0469 - val_loss: 133085663948241.1875\n",
      "Epoch 356/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132770111216129.3125 - val_loss: 133085648386252.5312\n",
      "Epoch 357/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132770100398463.6719 - val_loss: 133085645755226.3906\n",
      "Epoch 358/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132770094651332.9375 - val_loss: 133085628989808.6875\n",
      "Epoch 359/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132770080714245.8906 - val_loss: 133085620117469.4375\n",
      "Epoch 360/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132770073484111.5312 - val_loss: 133085603812186.3906\n",
      "Epoch 361/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132770057759159.3750 - val_loss: 133085601924454.6094\n",
      "Epoch 362/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132770056126675.2812 - val_loss: 133085600508655.8125\n",
      "Epoch 363/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770040380087.7969 - val_loss: 133085582881960.5000\n",
      "Epoch 364/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132770032760517.4844 - val_loss: 133085575425420.0625\n",
      "Epoch 365/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132770018302215.6250 - val_loss: 133085559025750.4219\n",
      "Epoch 366/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132770013927945.0000 - val_loss: 133085557609951.5781\n",
      "Epoch 367/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 66us/step - loss: 132770002889992.3438 - val_loss: 133085540655760.7500\n",
      "Epoch 368/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769991568813.5156 - val_loss: 133085533104833.6875\n",
      "Epoch 369/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132769979498265.5156 - val_loss: 133085515761298.1875\n",
      "Epoch 370/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769969418167.9688 - val_loss: 133085513873566.4219\n",
      "Epoch 371/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769964473511.3594 - val_loss: 133085497662669.9375\n",
      "Epoch 372/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769950764578.7031 - val_loss: 133085490123541.2500\n",
      "Epoch 373/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769942924721.3750 - val_loss: 133085472414257.6875\n",
      "Epoch 374/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769926790664.7656 - val_loss: 133085471281618.6406\n",
      "Epoch 375/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769924585827.8125 - val_loss: 133085468733180.7812\n",
      "Epoch 376/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132769909690885.6406 - val_loss: 133085451566620.0781\n",
      "Epoch 377/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769901349482.0000 - val_loss: 133085445065743.8281\n",
      "Epoch 378/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769887618914.0156 - val_loss: 133085427439048.5625\n",
      "Epoch 379/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769881706568.0312 - val_loss: 133085425268157.0312\n",
      "Epoch 380/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132769870782692.5625 - val_loss: 133085408691512.5312\n",
      "Epoch 381/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769859601159.9844 - val_loss: 133085400491677.6875\n",
      "Epoch 382/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769849182764.5312 - val_loss: 133085386439874.4375\n",
      "Epoch 383/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769839136103.4219 - val_loss: 133085384174596.3281\n",
      "Epoch 384/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769832942498.1250 - val_loss: 133085366359127.8594\n",
      "Epoch 385/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769818936571.3750 - val_loss: 133085359197545.5000\n",
      "Epoch 386/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132769809430793.5312 - val_loss: 133085342148968.0625\n",
      "Epoch 387/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769795096403.1406 - val_loss: 133085340450009.4688\n",
      "Epoch 388/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769794769906.3125 - val_loss: 133085324711045.9375\n",
      "Epoch 389/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132769777211851.5469 - val_loss: 133085322823314.1875\n",
      "Epoch 390/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132769769696524.1875 - val_loss: 133085314989227.3906\n",
      "Epoch 391/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132769755350332.6875 - val_loss: 133085296878800.8281\n",
      "Epoch 392/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132769752851451.9062 - val_loss: 133085294625321.0625\n",
      "Epoch 393/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132769738616387.3438 - val_loss: 133085277376172.1094\n",
      "Epoch 394/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769728996532.3125 - val_loss: 133085269282522.1875\n",
      "Epoch 395/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132769716847310.3594 - val_loss: 133085253909306.6875\n",
      "Epoch 396/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132769707972890.8281 - val_loss: 133085253909306.6875\n",
      "Epoch 397/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132769699641321.4375 - val_loss: 133085236565771.1719\n",
      "Epoch 398/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769687616010.9375 - val_loss: 133085227516457.0625\n",
      "Epoch 399/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769678137768.9688 - val_loss: 133085211010602.4688\n",
      "Epoch 400/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769665492901.2344 - val_loss: 133085209594803.6719\n",
      "Epoch 401/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769661677215.5312 - val_loss: 133085193018159.1719\n",
      "Epoch 402/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769645379910.5312 - val_loss: 133085184617752.8281\n",
      "Epoch 403/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769638029798.4219 - val_loss: 133085182848004.3281\n",
      "Epoch 404/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769621612515.6719 - val_loss: 133085165787628.5625\n",
      "Epoch 405/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769620141313.1406 - val_loss: 133085164560602.9219\n",
      "Epoch 406/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132769605051652.9844 - val_loss: 133085147299655.6719\n",
      "Epoch 407/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769596218537.2656 - val_loss: 133085139465568.8594\n",
      "Epoch 408/800\n",
      "4265/4265 [==============================] - 0s 61us/step - loss: 132769583005250.3750 - val_loss: 133085123526033.8281\n",
      "Epoch 409/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769577466605.5625 - val_loss: 133085120529259.6719\n",
      "Epoch 410/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132769566192631.1250 - val_loss: 133085104306564.8594\n",
      "Epoch 411/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769556305284.7188 - val_loss: 133085098938327.6719\n",
      "Epoch 412/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769543650582.7500 - val_loss: 133085080556539.6719\n",
      "Epoch 413/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132769533122043.7969 - val_loss: 133085079518287.2188\n",
      "Epoch 414/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132769527498824.5000 - val_loss: 133085061419658.9688\n",
      "Epoch 415/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132769514527459.9688 - val_loss: 133085053420395.6719\n",
      "Epoch 416/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132769503943849.2656 - val_loss: 133085036631381.3281\n",
      "Epoch 417/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769491049191.8125 - val_loss: 133085035132994.2500\n",
      "Epoch 418/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769488602432.5312 - val_loss: 133085017588887.2188\n",
      "Epoch 419/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132769472965988.5312 - val_loss: 133085010981826.0781\n",
      "Epoch 420/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769464496739.7656 - val_loss: 133085009577825.5781\n",
      "Epoch 421/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769448474793.5156 - val_loss: 133084991561785.6094\n",
      "Epoch 422/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769445911990.1719 - val_loss: 133084989874625.3281\n",
      "Epoch 423/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132769431758549.7969 - val_loss: 133084973864300.3906\n",
      "Epoch 424/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769422551732.9062 - val_loss: 133084965086347.6875\n",
      "Epoch 425/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769410201892.4375 - val_loss: 133084948025971.9375\n",
      "Epoch 426/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132769403654254.4531 - val_loss: 133084946421399.9375\n",
      "Epoch 427/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132769392455020.2188 - val_loss: 133084930304890.0625\n",
      "Epoch 428/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769380866350.0312 - val_loss: 133084922942736.1875\n",
      "Epoch 429/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769370310275.2031 - val_loss: 133084905516612.4219\n",
      "Epoch 430/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769359982354.7969 - val_loss: 133084903628880.6719\n",
      "Epoch 431/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132769352897767.2188 - val_loss: 133084887040437.8281\n",
      "Epoch 432/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132769340463352.2656 - val_loss: 133084879583897.3906\n",
      "Epoch 433/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769330817928.1875 - val_loss: 133084862806681.3906\n",
      "Epoch 434/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769316465836.1406 - val_loss: 133084860824563.0312\n",
      "Epoch 435/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769313729950.1562 - val_loss: 133084844708053.1719\n",
      "Epoch 436/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132769297739473.4688 - val_loss: 133084835481764.1875\n",
      "Epoch 437/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132769289933052.5781 - val_loss: 133084833499645.8281\n",
      "Epoch 438/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132769273208941.4844 - val_loss: 133084817489320.8594\n",
      "Epoch 439/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769271869517.7969 - val_loss: 133084815979135.4375\n",
      "Epoch 440/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769257548895.3125 - val_loss: 133084800877281.3906\n",
      "Epoch 441/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769248330277.3281 - val_loss: 133084791450420.9219\n",
      "Epoch 442/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769233352727.5312 - val_loss: 133084773717540.7188\n",
      "Epoch 443/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769229116136.2969 - val_loss: 133084772112968.7188\n",
      "Epoch 444/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769216402428.8906 - val_loss: 133084753672189.1094\n",
      "Epoch 445/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769205104852.2500 - val_loss: 133084746498808.4375\n",
      "Epoch 446/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132769193978391.4062 - val_loss: 133084730193525.3906\n",
      "Epoch 447/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132769185078402.8438 - val_loss: 133084730193525.3906\n",
      "Epoch 448/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132769177094965.6094 - val_loss: 133084713782057.4219\n",
      "Epoch 449/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769164768727.2969 - val_loss: 133084705582222.5781\n",
      "Epoch 450/800\n",
      "4265/4265 [==============================] - 0s 64us/step - loss: 132769154456541.6562 - val_loss: 133084688722418.3125\n",
      "Epoch 451/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132769142071297.9219 - val_loss: 133084686173980.4375\n",
      "Epoch 452/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132769137044033.6562 - val_loss: 133084670140058.8281\n",
      "Epoch 453/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132769122619168.2344 - val_loss: 133084662412156.9375\n",
      "Epoch 454/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132769112942274.5938 - val_loss: 133084644313528.6875\n",
      "Epoch 455/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132769098548878.7344 - val_loss: 133084642520183.5312\n",
      "Epoch 456/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132769096385345.6094 - val_loss: 133084641293157.8906\n",
      "Epoch 457/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132769080921984.2656 - val_loss: 133084624622126.8125\n",
      "Epoch 458/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132769071846946.2188 - val_loss: 133084616127333.8906\n",
      "Epoch 459/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132769057292268.7812 - val_loss: 133084599444504.4688\n",
      "Epoch 460/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132769054314460.4688 - val_loss: 133084597662957.6406\n",
      "Epoch 461/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132769041498476.9375 - val_loss: 133084580590783.5625\n",
      "Epoch 462/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132769030248104.6719 - val_loss: 133084573240428.0312\n",
      "Epoch 463/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 132769017325911.3281 - val_loss: 133084556380623.7500\n",
      "Epoch 464/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132769011165742.4688 - val_loss: 133084553643412.6875\n",
      "Epoch 465/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132769001646196.6719 - val_loss: 133084538458970.3906\n",
      "Epoch 466/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132768989673991.0781 - val_loss: 133084531391774.6094\n",
      "Epoch 467/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132768976745897.2031 - val_loss: 133084514508373.6875\n",
      "Epoch 468/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132768967348296.0312 - val_loss: 133084512903801.6875\n",
      "Epoch 469/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132768960861630.3438 - val_loss: 133084496692905.2188\n",
      "Epoch 470/800\n",
      "4265/4265 [==============================] - 0s 65us/step - loss: 132768946444632.2812 - val_loss: 133084487914952.5625\n",
      "Epoch 471/800\n",
      "4265/4265 [==============================] - 0s 66us/step - loss: 132768936848379.4375 - val_loss: 133084468990441.6719\n",
      "Epoch 472/800\n",
      "4265/4265 [==============================] - 0s 68us/step - loss: 132768923440374.5781 - val_loss: 133084467775214.3594\n",
      "Epoch 473/800\n",
      "4265/4265 [==============================] - 0s 67us/step - loss: 132768919782036.7344 - val_loss: 133084452767746.8906\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 132768717719843.4688 - val_loss: 133084252467609.0312\n",
      "Epoch 493/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 132768706410465.7500 - val_loss: 133084251334969.9688\n",
      "Epoch 494/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132768700588594.7812 - val_loss: 133084234368980.8125\n",
      "Epoch 495/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132768686816722.9844 - val_loss: 133084225036506.9219\n",
      "Epoch 496/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 132768677074923.3438 - val_loss: 133084210040837.7812\n",
      "Epoch 497/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132768662998190.0625 - val_loss: 133084207976131.1719\n",
      "Epoch 498/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 132768660240668.7500 - val_loss: 133084190066276.0781\n",
      "Epoch 499/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132768644582589.4375 - val_loss: 133084182527147.3906\n",
      "Epoch 500/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132768637429162.1719 - val_loss: 133084181016961.9688\n",
      "Epoch 501/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132768619511174.1562 - val_loss: 133084162823947.1719\n",
      "Epoch 502/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 132768618047839.0156 - val_loss: 133084160936215.4219\n",
      "Epoch 503/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132768602067196.5781 - val_loss: 133084145279840.1406\n",
      "Epoch 504/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 132768594020820.1875 - val_loss: 133084137646324.8281\n",
      "Epoch 505/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 132768579005900.2656 - val_loss: 133084119842654.6875\n",
      "Epoch 506/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 132768574716204.1094 - val_loss: 133084118426855.8906\n",
      "Epoch 507/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 105us/step - loss: 132768562535512.5938 - val_loss: 133084101272093.5312\n",
      "Epoch 508/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132768551639173.0156 - val_loss: 133084092977872.1094\n",
      "Epoch 509/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132768537780759.8906 - val_loss: 133084077333295.1719\n",
      "Epoch 510/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132768531117077.8438 - val_loss: 133084075445563.4219\n",
      "Epoch 511/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132768520596406.2969 - val_loss: 133084057830666.4375\n",
      "Epoch 512/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 132768509934121.6562 - val_loss: 133084049335873.5312\n",
      "Epoch 513/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132768497641319.7812 - val_loss: 133084034528977.5625\n",
      "Epoch 514/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132768488011630.5156 - val_loss: 133084032841817.3125\n",
      "Epoch 515/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132768479821674.1875 - val_loss: 133084017197240.3281\n",
      "Epoch 516/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132768466720497.6562 - val_loss: 133084007758581.5625\n",
      "Epoch 517/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132768457757569.9531 - val_loss: 133083991925231.4375\n",
      "Epoch 518/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132768443790980.1719 - val_loss: 133083989565566.7188\n",
      "Epoch 519/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132768438698809.9219 - val_loss: 133083970912417.3125\n",
      "Epoch 520/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132768424738120.6875 - val_loss: 133083963172717.0781\n",
      "Epoch 521/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132768415462464.1094 - val_loss: 133083945734794.9688\n",
      "Epoch 522/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132768400827145.8906 - val_loss: 133083944130222.9688\n",
      "Epoch 523/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132768398140431.1250 - val_loss: 133083928485646.0625\n",
      "Epoch 524/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132768382879655.1719 - val_loss: 133083921312265.3594\n",
      "Epoch 525/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 132768373415181.1406 - val_loss: 133083904263687.9219\n",
      "Epoch 526/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 132768357890847.5000 - val_loss: 133083902375956.1719\n",
      "Epoch 527/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132768356386208.5625 - val_loss: 133083901243317.1094\n",
      "Epoch 528/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132768340281654.6875 - val_loss: 133083882590167.6719\n",
      "Epoch 529/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 132768331790770.5625 - val_loss: 133083874862265.7812\n",
      "Epoch 530/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 132768315865199.8906 - val_loss: 133083858179436.3906\n",
      "Epoch 531/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 132768313520716.7188 - val_loss: 133083856291704.6094\n",
      "Epoch 532/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 132768298313045.6562 - val_loss: 133083840552741.0781\n",
      "Epoch 533/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 132768288868240.1094 - val_loss: 133083832529881.1094\n",
      "Epoch 534/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 132768274093275.6875 - val_loss: 133083815174547.2812\n",
      "Epoch 535/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 132768269734739.8594 - val_loss: 133083814242479.6875\n",
      "Epoch 536/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 132768257276722.7188 - val_loss: 133083796816355.9219\n",
      "Epoch 537/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 132768247721773.6719 - val_loss: 133083788887882.5312\n",
      "Epoch 538/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132768233973504.0625 - val_loss: 133083771178598.9688\n",
      "Epoch 539/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 132768226892850.1719 - val_loss: 133083769385253.8125\n",
      "Epoch 540/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 132768216500023.7656 - val_loss: 133083751852945.1094\n",
      "Epoch 541/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 132768204142315.8906 - val_loss: 133083744962724.1875\n",
      "Epoch 542/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132768192545778.3125 - val_loss: 133083729884466.7812\n",
      "Epoch 543/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132768183344861.9688 - val_loss: 133083728657441.1094\n",
      "Epoch 544/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132768175219811.6406 - val_loss: 133083711207720.6875\n",
      "Epoch 545/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 132768161998657.3594 - val_loss: 133083703196659.0312\n",
      "Epoch 546/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132768151346206.9688 - val_loss: 133083686148081.5781\n",
      "Epoch 547/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 132768140034862.4062 - val_loss: 133083683505257.1406\n",
      "Epoch 548/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 132768134075312.0469 - val_loss: 133083668710159.4688\n",
      "Epoch 549/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132768119973009.7344 - val_loss: 133083659082727.5312\n",
      "Epoch 550/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132768108878018.4688 - val_loss: 133083642777444.4375\n",
      "Epoch 551/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132768096471139.4062 - val_loss: 133083641078485.8906\n",
      "Epoch 552/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132768092616116.7188 - val_loss: 133083623746748.6719\n",
      "Epoch 553/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 132768077849019.6875 - val_loss: 133083616478981.4219\n",
      "Epoch 554/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132768068213429.8750 - val_loss: 133083600079311.7500\n",
      "Epoch 555/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132768053302752.9062 - val_loss: 133083597259512.4375\n",
      "Epoch 556/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132768049805696.6250 - val_loss: 133083580659271.3125\n",
      "Epoch 557/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132768034074843.9219 - val_loss: 133083573297117.4375\n",
      "Epoch 558/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132768026270389.8750 - val_loss: 133083571786932.0312\n",
      "Epoch 559/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 132768009394831.4531 - val_loss: 133083552744437.9219\n",
      "Epoch 560/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132768007884291.9688 - val_loss: 133083551139865.9219\n",
      "Epoch 561/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132767992814300.2812 - val_loss: 133083535223927.5312\n",
      "Epoch 562/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132767983550444.7812 - val_loss: 133083526080226.8281\n",
      "Epoch 563/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132767968840386.3438 - val_loss: 133083510152490.1094\n",
      "Epoch 564/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132767963885895.4844 - val_loss: 133083508925464.4688\n",
      "Epoch 565/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132767950163194.8906 - val_loss: 133083489977356.9688\n",
      "Epoch 566/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 132767941074388.9062 - val_loss: 133083483464682.3906\n",
      "Epoch 567/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132767926382032.0938 - val_loss: 133083465661012.2500\n",
      "Epoch 568/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 79us/step - loss: 132767920568028.5312 - val_loss: 133083464339600.0312\n",
      "Epoch 569/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132767909156374.6875 - val_loss: 133083447479795.7812\n",
      "Epoch 570/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767897831262.1719 - val_loss: 133083439928868.7188\n",
      "Epoch 571/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132767885707609.2656 - val_loss: 133083422856694.6406\n",
      "Epoch 572/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767877954293.2500 - val_loss: 133083420131281.9219\n",
      "Epoch 573/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132767867543765.2031 - val_loss: 133083404203545.1875\n",
      "Epoch 574/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767855929526.0000 - val_loss: 133083396180685.2188\n",
      "Epoch 575/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767843455774.0625 - val_loss: 133083380264746.8281\n",
      "Epoch 576/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132767833892957.6406 - val_loss: 133083379698427.3125\n",
      "Epoch 577/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 132767825529918.6719 - val_loss: 133083361127866.1719\n",
      "Epoch 578/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132767813551812.5156 - val_loss: 133083354143258.6406\n",
      "Epoch 579/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767801709418.9062 - val_loss: 133083337283454.3906\n",
      "Epoch 580/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767791601785.4844 - val_loss: 133083335678882.3906\n",
      "Epoch 581/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132767784261507.6406 - val_loss: 133083318712893.1875\n",
      "Epoch 582/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767769982188.9688 - val_loss: 133083311067579.5781\n",
      "Epoch 583/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767760777338.9219 - val_loss: 133083292780178.1875\n",
      "Epoch 584/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132767747056605.1875 - val_loss: 133083291269992.7812\n",
      "Epoch 585/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132767743679526.6562 - val_loss: 133083274964709.7188\n",
      "Epoch 586/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132767727789359.2500 - val_loss: 133083266281143.6094\n",
      "Epoch 587/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132767718842166.3281 - val_loss: 133083250648365.0000\n",
      "Epoch 588/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132767703233258.2188 - val_loss: 133083248666246.6719\n",
      "Epoch 589/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132767700835670.1250 - val_loss: 133083231322711.1406\n",
      "Epoch 590/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132767685085148.9375 - val_loss: 133083223111077.9688\n",
      "Epoch 591/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767677386904.6875 - val_loss: 133083206145088.8125\n",
      "Epoch 592/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767661720958.0000 - val_loss: 133083205106836.3281\n",
      "Epoch 593/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132767659140453.0156 - val_loss: 133083203502264.3281\n",
      "Epoch 594/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132767641550928.6719 - val_loss: 133083185710392.5312\n",
      "Epoch 595/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132767634063137.2031 - val_loss: 133083179764037.4688\n",
      "Epoch 596/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767618251643.7031 - val_loss: 133083161099089.7188\n",
      "Epoch 597/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 132767616013370.3438 - val_loss: 133083159400131.1719\n",
      "Epoch 598/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132767601551134.7812 - val_loss: 133083142528528.5625\n",
      "Epoch 599/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132767591437600.8438 - val_loss: 133083134423080.3281\n",
      "Epoch 600/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132767576959630.5000 - val_loss: 133083117645864.3281\n",
      "Epoch 601/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132767572673868.0469 - val_loss: 133083116041292.3281\n",
      "Epoch 602/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767559442879.5312 - val_loss: 133083099547236.0781\n",
      "Epoch 603/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767550733675.2656 - val_loss: 133083090580510.2500\n",
      "Epoch 604/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132767535622379.7656 - val_loss: 133083074287025.5000\n",
      "Epoch 605/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132767528783648.2344 - val_loss: 133083072210520.5781\n",
      "Epoch 606/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132767517120237.8125 - val_loss: 133083055256329.7188\n",
      "Epoch 607/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132767505985909.5938 - val_loss: 133083048082949.0625\n",
      "Epoch 608/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132767493188611.1094 - val_loss: 133083031305733.0625\n",
      "Epoch 609/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132767486413802.1562 - val_loss: 133083029229228.1094\n",
      "Epoch 610/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767475269639.6875 - val_loss: 133083011508146.2188\n",
      "Epoch 611/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132767464886647.5156 - val_loss: 133083003296513.0781\n",
      "Epoch 612/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132767450920057.7188 - val_loss: 133082987935095.9219\n",
      "Epoch 613/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132767441848953.3594 - val_loss: 133082986141750.7188\n",
      "Epoch 614/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132767433222356.7188 - val_loss: 133082969848265.9688\n",
      "Epoch 615/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132767421897244.2188 - val_loss: 133082961731019.4375\n",
      "Epoch 616/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132767409317282.4844 - val_loss: 133082945236963.1875\n",
      "Epoch 617/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132767398289164.0625 - val_loss: 133082943443618.0312\n",
      "Epoch 618/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 132767391788730.4219 - val_loss: 133082926300654.0000\n",
      "Epoch 619/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132767377513345.4688 - val_loss: 133082917522701.3281\n",
      "Epoch 620/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132767367942661.6406 - val_loss: 133082901311804.8281\n",
      "Epoch 621/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132767355885881.5625 - val_loss: 133082899435871.4219\n",
      "Epoch 622/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767349334309.8750 - val_loss: 133082883685109.5625\n",
      "Epoch 623/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132767334103036.6406 - val_loss: 133082874718383.6875\n",
      "Epoch 624/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132767325248285.5938 - val_loss: 133082858129940.8906\n",
      "Epoch 625/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132767311496082.2812 - val_loss: 133082856525368.8906\n",
      "Epoch 626/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 132767307981324.3750 - val_loss: 133082839866136.1094\n",
      "Epoch 627/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132767292100991.1875 - val_loss: 133082830710637.0781\n",
      "Epoch 628/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132767283918902.2656 - val_loss: 133082814771102.0625\n",
      "Epoch 629/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 82us/step - loss: 132767268854811.1406 - val_loss: 133082812895168.6094\n",
      "Epoch 630/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132767266272339.3125 - val_loss: 133082795445448.1875\n",
      "Epoch 631/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132767250753906.2188 - val_loss: 133082788283865.8281\n",
      "Epoch 632/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767241584459.4531 - val_loss: 133082770775153.7812\n",
      "Epoch 633/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132767225273386.5000 - val_loss: 133082768981808.6094\n",
      "Epoch 634/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132767223080350.6406 - val_loss: 133082766810917.0781\n",
      "Epoch 635/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132767207123310.3906 - val_loss: 133082750682608.8906\n",
      "Epoch 636/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132767198229222.3594 - val_loss: 133082741066975.2188\n",
      "Epoch 637/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132767182435430.5156 - val_loss: 133082725693759.7188\n",
      "Epoch 638/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 132767179416318.3750 - val_loss: 133082724183574.3281\n",
      "Epoch 639/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132767164633486.5625 - val_loss: 133082707229383.4688\n",
      "Epoch 640/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132767155814138.7812 - val_loss: 133082698923363.7188\n",
      "Epoch 641/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 132767140405849.2031 - val_loss: 133082682818652.1719\n",
      "Epoch 642/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 132767136027644.8906 - val_loss: 133082680364600.8906\n",
      "Epoch 643/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 132767123290335.2812 - val_loss: 133082663964931.2188\n",
      "Epoch 644/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 132767112669354.4688 - val_loss: 133082655670709.8281\n",
      "Epoch 645/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 132767097721307.3906 - val_loss: 133082639164855.2812\n",
      "Epoch 646/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 132767092336076.7500 - val_loss: 133082636710803.9688\n",
      "Epoch 647/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132767080365837.9844 - val_loss: 133082620417319.2500\n",
      "Epoch 648/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 132767069760591.9531 - val_loss: 133082610707299.0312\n",
      "Epoch 649/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132767056637780.0938 - val_loss: 133082594390217.6094\n",
      "Epoch 650/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132767050532682.9688 - val_loss: 133082592880032.2188\n",
      "Epoch 651/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132767038194643.5938 - val_loss: 133082577046682.1094\n",
      "Epoch 652/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132767027563828.5156 - val_loss: 133082567702409.9219\n",
      "Epoch 653/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132767013481194.6875 - val_loss: 133082551774673.1875\n",
      "Epoch 654/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132767006868650.7031 - val_loss: 133082549792554.8281\n",
      "Epoch 655/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132766996011648.0938 - val_loss: 133082532743977.4219\n",
      "Epoch 656/800\n",
      "4265/4265 [==============================] - ETA: 0s - loss: 132395007568470.640 - 0s 83us/step - loss: 132766983903729.9531 - val_loss: 133082524272781.1406\n",
      "Epoch 657/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132766971547988.9375 - val_loss: 133082507106220.4688\n",
      "Epoch 658/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 132766963139712.4531 - val_loss: 133082505230287.0312\n",
      "Epoch 659/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132766953875856.9531 - val_loss: 133082490411592.7188\n",
      "Epoch 660/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 132766941166083.2344 - val_loss: 133082482778077.4375\n",
      "Epoch 661/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 132766928871314.5156 - val_loss: 133082464962608.9688\n",
      "Epoch 662/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 132766918980034.4062 - val_loss: 133082462720927.5000\n",
      "Epoch 663/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 132766911445038.5781 - val_loss: 133082449046670.5781\n",
      "Epoch 664/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 132766898522845.2500 - val_loss: 133082438015238.1094\n",
      "Epoch 665/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132766887795654.6250 - val_loss: 133082422181888.0000\n",
      "Epoch 666/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 132766876163713.7656 - val_loss: 133082419822223.3125\n",
      "Epoch 667/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 132766868255016.7656 - val_loss: 133082402761847.5312\n",
      "Epoch 668/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132766855350525.0625 - val_loss: 133082395022147.3281\n",
      "Epoch 669/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132766843716617.3594 - val_loss: 133082379200595.5312\n",
      "Epoch 670/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132766832452477.1562 - val_loss: 133082376652157.6719\n",
      "Epoch 671/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 132766826481125.7031 - val_loss: 133082358931075.7812\n",
      "Epoch 672/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 132766811796636.2969 - val_loss: 133082351663308.5312\n",
      "Epoch 673/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132766801690969.7344 - val_loss: 133082335086664.0312\n",
      "Epoch 674/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132766788320334.9844 - val_loss: 133082333576478.6094\n",
      "Epoch 675/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132766783045247.8438 - val_loss: 133082316893649.1875\n",
      "Epoch 676/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132766768140471.4375 - val_loss: 133082307832536.7500\n",
      "Epoch 677/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 132766758554052.8281 - val_loss: 133082291810413.4375\n",
      "Epoch 678/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132766745716433.9688 - val_loss: 133082290111454.8906\n",
      "Epoch 679/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 132766740815048.0000 - val_loss: 133082272956692.5312\n",
      "Epoch 680/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132766725408725.2656 - val_loss: 133082265712521.9219\n",
      "Epoch 681/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132766716345488.2969 - val_loss: 133082247897053.4375\n",
      "Epoch 682/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 132766702282522.9531 - val_loss: 133082246575641.1875\n",
      "Epoch 683/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132766699277178.7500 - val_loss: 133082229314693.9375\n",
      "Epoch 684/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132766684093109.8750 - val_loss: 133082220170993.2188\n",
      "Epoch 685/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 132766674644370.6406 - val_loss: 133082203570752.0781\n",
      "Epoch 686/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 132766658791573.3438 - val_loss: 133082202284734.8281\n",
      "Epoch 687/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 132766656470692.3438 - val_loss: 133082186333401.4688\n",
      "Epoch 688/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 132766639941299.2344 - val_loss: 133082177284087.3594\n",
      "Epoch 689/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 132766632530214.8281 - val_loss: 133082160978804.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 690/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 132766614926922.5469 - val_loss: 133082159291644.0625\n",
      "Epoch 691/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 132766613188228.6562 - val_loss: 133082156837592.7500\n",
      "Epoch 692/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 132766597272492.2031 - val_loss: 133082140048578.4375\n",
      "Epoch 693/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 132766588394138.9844 - val_loss: 133082131282424.0781\n",
      "Epoch 694/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132766572602313.9688 - val_loss: 133082114894552.7500\n",
      "Epoch 695/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132766570049344.8906 - val_loss: 133082113006821.0000\n",
      "Epoch 696/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132766553759907.2656 - val_loss: 133082095663285.4688\n",
      "Epoch 697/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132766545585685.7344 - val_loss: 133082088867451.1406\n",
      "Epoch 698/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132766529384756.2812 - val_loss: 133082072184621.7188\n",
      "Epoch 699/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 132766525679214.0781 - val_loss: 133082070485663.1406\n",
      "Epoch 700/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 132766511993883.6094 - val_loss: 133082052564009.7812\n",
      "Epoch 701/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 132766501675797.4219 - val_loss: 133082044470359.8594\n",
      "Epoch 702/800\n",
      "4265/4265 [==============================] - 1s 138us/step - loss: 132766486080657.2500 - val_loss: 133082028070690.1875\n",
      "Epoch 703/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 132766482406584.6250 - val_loss: 133082026843664.5625\n",
      "Epoch 704/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 132766469360479.8438 - val_loss: 133082009405742.4375\n",
      "Epoch 705/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 132766459488868.2344 - val_loss: 133082001961000.3281\n",
      "Epoch 706/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 132766444066810.7188 - val_loss: 133081985360759.1719\n",
      "Epoch 707/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132766439712208.5781 - val_loss: 133081983473027.4219\n",
      "Epoch 708/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 132766426581529.3438 - val_loss: 133081964914264.5781\n",
      "Epoch 709/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132766415283952.7031 - val_loss: 133081958602161.5000\n",
      "Epoch 710/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132766400489319.7812 - val_loss: 133081941824945.5000\n",
      "Epoch 711/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 132766395210298.9375 - val_loss: 133081939842827.1719\n",
      "Epoch 712/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132766382614602.4219 - val_loss: 133081923053812.8281\n",
      "Epoch 713/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132766373075388.1719 - val_loss: 133081914665204.8281\n",
      "Epoch 714/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132766358882610.8438 - val_loss: 133081897805400.5781\n",
      "Epoch 715/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 132766352907325.7031 - val_loss: 133081896106441.9688\n",
      "Epoch 716/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 132766340533883.0469 - val_loss: 133081878184788.6094\n",
      "Epoch 717/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132766329588372.2656 - val_loss: 133081871967072.1406\n",
      "Epoch 718/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 132766316123328.7969 - val_loss: 133081854139805.3281\n",
      "Epoch 719/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132766309221658.1094 - val_loss: 133081851697552.3906\n",
      "Epoch 720/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 132766297682159.1406 - val_loss: 133081835227092.8125\n",
      "Epoch 721/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132766287773177.3906 - val_loss: 133081827204232.8281\n",
      "Epoch 722/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132766273737747.9375 - val_loss: 133081809955083.8906\n",
      "Epoch 723/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 132766265638266.6250 - val_loss: 133081808161738.7188\n",
      "Epoch 724/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 132766255265108.6875 - val_loss: 133081790617631.6875\n",
      "Epoch 725/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 132766243212262.2969 - val_loss: 133081783739209.0781\n",
      "Epoch 726/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132766230246798.3125 - val_loss: 133081767540110.9375\n",
      "Epoch 727/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132766221781483.2344 - val_loss: 133081765180446.2500\n",
      "Epoch 728/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 132766212800853.8906 - val_loss: 133081747553750.9375\n",
      "Epoch 729/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132766199721312.7031 - val_loss: 133081739625277.5625\n",
      "Epoch 730/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 132766187882852.7812 - val_loss: 133081724157675.4688\n",
      "Epoch 731/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 132766178931726.1719 - val_loss: 133081722364330.3125\n",
      "Epoch 732/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 132766169585263.0469 - val_loss: 133081706908526.5312\n",
      "Epoch 733/800\n",
      "4265/4265 [==============================] - 1s 141us/step - loss: 132766157292461.1562 - val_loss: 133081698614305.1094\n",
      "Epoch 734/800\n",
      "4265/4265 [==============================] - 1s 147us/step - loss: 132766145507106.1562 - val_loss: 133081681376954.5312\n",
      "Epoch 735/800\n",
      "4265/4265 [==============================] - 1s 153us/step - loss: 132766134838920.9688 - val_loss: 133081678828516.6406\n",
      "Epoch 736/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 132766127593051.8438 - val_loss: 133081662417048.6719\n",
      "Epoch 737/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 132766113543854.4375 - val_loss: 133081653839667.4688\n",
      "Epoch 738/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 132766101516577.0781 - val_loss: 133081638667023.4688\n",
      "Epoch 739/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 132766090942800.6094 - val_loss: 133081636118585.6094\n",
      "Epoch 740/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132766083567119.4844 - val_loss: 133081618786848.4219\n",
      "Epoch 741/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132766069962429.7969 - val_loss: 133081611247719.6875\n",
      "Epoch 742/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132766060197027.9844 - val_loss: 133081593892385.8281\n",
      "Epoch 743/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132766048437242.0000 - val_loss: 133081591627107.7188\n",
      "Epoch 744/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 132766041657515.9062 - val_loss: 133081575793757.6094\n",
      "Epoch 745/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 132766027264120.0469 - val_loss: 133081565906762.5312\n",
      "Epoch 746/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 132766017146652.3906 - val_loss: 133081551394824.6406\n",
      "Epoch 747/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 132766004232326.4531 - val_loss: 133081550545345.3281\n",
      "Epoch 748/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 132765998772355.5781 - val_loss: 133081531396666.3281\n",
      "Epoch 749/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 132765983836109.5781 - val_loss: 133081523562579.5312\n",
      "Epoch 750/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132765973791415.3125 - val_loss: 133081506690976.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 751/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 132765960641067.5781 - val_loss: 133081504897631.7812\n",
      "Epoch 752/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132765956278598.0469 - val_loss: 133081488320987.2812\n",
      "Epoch 753/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132765939818044.6250 - val_loss: 133081480321723.9375\n",
      "Epoch 754/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132765931171779.4844 - val_loss: 133081462883801.8281\n",
      "Epoch 755/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132765917738205.6094 - val_loss: 133081461279229.8281\n",
      "Epoch 756/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 132765912854521.2812 - val_loss: 133081444867761.8906\n",
      "Epoch 757/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 132765897760927.4219 - val_loss: 133081437328633.1719\n",
      "Epoch 758/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 132765888231547.4062 - val_loss: 133081420079484.2188\n",
      "Epoch 759/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 132765873800781.4375 - val_loss: 133081418286139.0625\n",
      "Epoch 760/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132765870301758.3125 - val_loss: 133081400765628.6719\n",
      "Epoch 761/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 132765854738087.7188 - val_loss: 133081392837155.2812\n",
      "Epoch 762/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 132765845566674.0938 - val_loss: 133081375682392.9375\n",
      "Epoch 763/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 132765829894826.8281 - val_loss: 133081373889047.7812\n",
      "Epoch 764/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132765827042896.7812 - val_loss: 133081357017445.1719\n",
      "Epoch 765/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 132765811052420.1250 - val_loss: 133081348994585.1875\n",
      "Epoch 766/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132765802193735.3750 - val_loss: 133081332134780.9375\n",
      "Epoch 767/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132765787049003.4531 - val_loss: 133081330435822.3594\n",
      "Epoch 768/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132765782985494.8594 - val_loss: 133081314885632.0000\n",
      "Epoch 769/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132765766865206.2031 - val_loss: 133081307051545.1875\n",
      "Epoch 770/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132765758832597.7500 - val_loss: 133081289896782.8594\n",
      "Epoch 771/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132765743247291.8125 - val_loss: 133081288197824.2812\n",
      "Epoch 772/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132765741162432.6094 - val_loss: 133081270665515.5625\n",
      "Epoch 773/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132765725126718.4219 - val_loss: 133081261250453.4219\n",
      "Epoch 774/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 132765716466685.3594 - val_loss: 133081245499691.5625\n",
      "Epoch 775/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 132765700629622.8438 - val_loss: 133081243706346.3906\n",
      "Epoch 776/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 132765698588034.3125 - val_loss: 133081242774278.8281\n",
      "Epoch 777/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132765681635768.8125 - val_loss: 133081224675650.6094\n",
      "Epoch 778/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 132765673668066.3438 - val_loss: 133081216935950.4219\n",
      "Epoch 779/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132765657591048.3438 - val_loss: 133081199403641.6875\n",
      "Epoch 780/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 132765655864155.5312 - val_loss: 133081198271002.6406\n",
      "Epoch 781/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132765638807647.0781 - val_loss: 133081181305013.4688\n",
      "Epoch 782/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132765629827017.7344 - val_loss: 133081174332204.2812\n",
      "Epoch 783/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 132765614920274.4844 - val_loss: 133081157094853.6719\n",
      "Epoch 784/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 132765610811528.3750 - val_loss: 133081155490281.6719\n",
      "Epoch 785/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 132765595942155.2344 - val_loss: 133081137391653.4375\n",
      "Epoch 786/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 132765586068576.7656 - val_loss: 133081129463180.0625\n",
      "Epoch 787/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132765571185435.6719 - val_loss: 133081112886535.5625\n",
      "Epoch 788/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 132765569224487.9219 - val_loss: 133081111281963.5625\n",
      "Epoch 789/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 132765553505436.2969 - val_loss: 133081093655268.2812\n",
      "Epoch 790/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 132765543944586.7188 - val_loss: 133081086387501.0000\n",
      "Epoch 791/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 132765529376141.3594 - val_loss: 133081069043965.4688\n",
      "Epoch 792/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132765523621143.2188 - val_loss: 133081066684300.7812\n",
      "Epoch 793/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 132765510809093.4062 - val_loss: 133081051334681.9219\n",
      "Epoch 794/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 132765501395757.4375 - val_loss: 133081042084796.3125\n",
      "Epoch 795/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132765485952064.5781 - val_loss: 133081024835647.3906\n",
      "Epoch 796/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132765480566833.9375 - val_loss: 133081023042302.1875\n",
      "Epoch 797/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 132765468372374.4688 - val_loss: 133081007964044.7812\n",
      "Epoch 798/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 132765456653892.3125 - val_loss: 133080998997318.9375\n",
      "Epoch 799/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 132765443458307.0625 - val_loss: 133080982408876.1094\n",
      "Epoch 800/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 132765437069983.7812 - val_loss: 133080981370623.6406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f7ca85400>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_train33 = model33.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train33'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4265 samples, validate on 2844 samples\n",
      "Epoch 1/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1533697076131.0837 - val_loss: 1638758704429.0071\n",
      "Epoch 2/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1532882912845.4302 - val_loss: 1637941339805.7046\n",
      "Epoch 3/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1531743208369.0090 - val_loss: 1636642065838.6272\n",
      "Epoch 4/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1530692518037.8184 - val_loss: 1635542253959.7412\n",
      "Epoch 5/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1529863512745.8665 - val_loss: 1634603068512.4951\n",
      "Epoch 6/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1528923704593.9470 - val_loss: 1633466319032.3489\n",
      "Epoch 7/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1527981345516.6125 - val_loss: 1632499506888.9114\n",
      "Epoch 8/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1527042853293.7678 - val_loss: 1631575276554.0815\n",
      "Epoch 9/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1525795425058.3936 - val_loss: 1630376585322.5767\n",
      "Epoch 10/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1525116302949.4397 - val_loss: 1629541171660.8721\n",
      "Epoch 11/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1524186835656.1182 - val_loss: 1628371140216.2588\n",
      "Epoch 12/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1523045800576.0901 - val_loss: 1627762658642.4529\n",
      "Epoch 13/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1522222309742.8633 - val_loss: 1626742824464.5625\n",
      "Epoch 14/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1521183301368.8572 - val_loss: 1625557335557.0408\n",
      "Epoch 15/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1520245346433.6506 - val_loss: 1624505899569.6877\n",
      "Epoch 16/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 1519478791863.5518 - val_loss: 1623464132677.1309\n",
      "Epoch 17/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1518551634633.5588 - val_loss: 1622293330654.5146\n",
      "Epoch 18/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1517473926721.4255 - val_loss: 1621116717588.8833\n",
      "Epoch 19/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1516494942116.7644 - val_loss: 1620695024860.3545\n",
      "Epoch 20/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1515592837432.6021 - val_loss: 1619246240878.8972\n",
      "Epoch 21/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1514684505460.8657 - val_loss: 1618096166114.1152\n",
      "Epoch 22/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1513796204794.1777 - val_loss: 1617221539249.5078\n",
      "Epoch 23/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1512732659695.6736 - val_loss: 1616436587939.1055\n",
      "Epoch 24/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1511979194193.6919 - val_loss: 1615213540912.2476\n",
      "Epoch 25/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1511045761949.3215 - val_loss: 1614090967855.1675\n",
      "Epoch 26/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1510122276333.3928 - val_loss: 1613207152124.3994\n",
      "Epoch 27/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1509255294304.6978 - val_loss: 1612088575565.0520\n",
      "Epoch 28/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1508216446044.4360 - val_loss: 1611183593641.9465\n",
      "Epoch 29/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1507405734147.3013 - val_loss: 1610452505918.2898\n",
      "Epoch 30/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1506395747833.8777 - val_loss: 1609314479638.3235\n",
      "Epoch 31/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1505380828511.2571 - val_loss: 1608255074805.1982\n",
      "Epoch 32/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1504514576444.5037 - val_loss: 1607447002652.0845\n",
      "Epoch 33/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1503602446796.0198 - val_loss: 1606344531910.3911\n",
      "Epoch 34/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1502695574717.4341 - val_loss: 1605078470039.5837\n",
      "Epoch 35/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1501735781896.2834 - val_loss: 1604125980323.4656\n",
      "Epoch 36/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1500952106818.3259 - val_loss: 1603193656523.0718\n",
      "Epoch 37/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1499967184483.0386 - val_loss: 1602259450890.0815\n",
      "Epoch 38/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1499121370459.4158 - val_loss: 1601230113900.0168\n",
      "Epoch 39/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1498170172319.7224 - val_loss: 1600291859229.8848\n",
      "Epoch 40/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1497234107418.1704 - val_loss: 1599761406052.8157\n",
      "Epoch 41/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1496362235343.6211 - val_loss: 1598383361316.3657\n",
      "Epoch 42/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1495292881496.7146 - val_loss: 1597578229895.3811\n",
      "Epoch 43/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1494596300658.8247 - val_loss: 1596704067221.0632\n",
      "Epoch 44/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1493670463675.7534 - val_loss: 1595652272859.6343\n",
      "Epoch 45/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1492753321575.8406 - val_loss: 1594411624946.3179\n",
      "Epoch 46/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1491832949388.5750 - val_loss: 1593559855674.3291\n",
      "Epoch 47/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1490900318696.3508 - val_loss: 1592670578978.9255\n",
      "Epoch 48/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1490065525599.8574 - val_loss: 1591929297135.0774\n",
      "Epoch 49/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1489339973993.1011 - val_loss: 1590807215736.2588\n",
      "Epoch 50/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1488194888649.9790 - val_loss: 1589584044642.6555\n",
      "Epoch 51/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1487367933735.9155 - val_loss: 1588691988335.9775\n",
      "Epoch 52/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1486468527291.0330 - val_loss: 1587774716246.7734\n",
      "Epoch 53/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1485451459217.3767 - val_loss: 1586606781032.4163\n",
      "Epoch 54/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1484742659580.2786 - val_loss: 1585532370759.6511\n",
      "Epoch 55/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1483791736307.6353 - val_loss: 1584697877367.1787\n",
      "Epoch 56/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1482886862116.6743 - val_loss: 1583679173984.8552\n",
      "Epoch 57/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1481999603725.9253 - val_loss: 1582674584347.0042\n",
      "Epoch 58/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1481216145170.3071 - val_loss: 1581824599387.0942\n",
      "Epoch 59/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1480324517272.3997 - val_loss: 1580890073724.5793\n",
      "Epoch 60/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1479279633046.6589 - val_loss: 1580091876768.2251\n",
      "Epoch 61/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1478544545652.2654 - val_loss: 1579172036147.1279\n",
      "Epoch 62/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1477676052653.8279 - val_loss: 1578154150904.7988\n",
      "Epoch 63/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1476865115206.8276 - val_loss: 1577188106574.1323\n",
      "Epoch 64/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 70us/step - loss: 1475849096992.2327 - val_loss: 1576585206160.3826\n",
      "Epoch 65/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1475029844040.9885 - val_loss: 1575472641215.5500\n",
      "Epoch 66/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1474261028993.6506 - val_loss: 1574473935438.4922\n",
      "Epoch 67/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1473146781788.1960 - val_loss: 1573249424313.4290\n",
      "Epoch 68/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1472433591500.3198 - val_loss: 1572363445986.8354\n",
      "Epoch 69/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1471499911986.2395 - val_loss: 1571425985860.0505\n",
      "Epoch 70/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1470594531762.0896 - val_loss: 1570436600942.8972\n",
      "Epoch 71/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1469699667538.2322 - val_loss: 1569516623719.3362\n",
      "Epoch 72/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1468826515291.2957 - val_loss: 1569189356291.9607\n",
      "Epoch 73/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1468045039994.8679 - val_loss: 1567802267771.8594\n",
      "Epoch 74/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1467176877119.6250 - val_loss: 1566914831246.2222\n",
      "Epoch 75/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1466388081951.3923 - val_loss: 1565771031530.3967\n",
      "Epoch 76/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1465368001465.1724 - val_loss: 1565074264023.6736\n",
      "Epoch 77/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1464667049583.5237 - val_loss: 1564168739930.7341\n",
      "Epoch 78/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1463796784116.7156 - val_loss: 1563131535118.0422\n",
      "Epoch 79/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1462765285461.4734 - val_loss: 1562377793322.8467\n",
      "Epoch 80/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1462081803003.4980 - val_loss: 1561362498549.9185\n",
      "Epoch 81/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1461168071203.4138 - val_loss: 1560195045776.3826\n",
      "Epoch 82/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1460323283152.4016 - val_loss: 1559270962920.5964\n",
      "Epoch 83/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1459528064534.2087 - val_loss: 1558575724587.2068\n",
      "Epoch 84/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1458587502249.6265 - val_loss: 1557653395072.9001\n",
      "Epoch 85/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1457664355945.7612 - val_loss: 1556776009428.4331\n",
      "Epoch 86/800\n",
      "4265/4265 [==============================] - 1s 132us/step - loss: 1456932868314.0051 - val_loss: 1555795915506.6780\n",
      "Epoch 87/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1456033449146.0728 - val_loss: 1554653886276.7708\n",
      "Epoch 88/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1455295487694.6006 - val_loss: 1553724857711.2573\n",
      "Epoch 89/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1454346062484.2578 - val_loss: 1552938323028.9734\n",
      "Epoch 90/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1453618348066.5735 - val_loss: 1551920497700.0056\n",
      "Epoch 91/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1452647802869.9160 - val_loss: 1551073672478.6047\n",
      "Epoch 92/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1451878816153.5999 - val_loss: 1550254702545.9128\n",
      "Epoch 93/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1450964314363.1379 - val_loss: 1549223263979.4768\n",
      "Epoch 94/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1450204083686.6702 - val_loss: 1548362119464.6863\n",
      "Epoch 95/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1449313415011.4587 - val_loss: 1547570669674.5767\n",
      "Epoch 96/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1448556004457.4014 - val_loss: 1546819584714.3516\n",
      "Epoch 97/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1447614629710.8108 - val_loss: 1545987165983.3250\n",
      "Epoch 98/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1446825793699.7441 - val_loss: 1544886014441.6765\n",
      "Epoch 99/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1446039815150.4731 - val_loss: 1543932521499.3643\n",
      "Epoch 100/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1445185729469.4939 - val_loss: 1543182960550.7061\n",
      "Epoch 101/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1444399443723.1042 - val_loss: 1542185157345.3953\n",
      "Epoch 102/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1443518184926.5071 - val_loss: 1541309700726.8186\n",
      "Epoch 103/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 1442692888311.4167 - val_loss: 1540372867385.9690\n",
      "Epoch 104/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1441885096738.3936 - val_loss: 1539533771172.5457\n",
      "Epoch 105/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1441026612723.6353 - val_loss: 1538864161009.9578\n",
      "Epoch 106/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1440088109882.8831 - val_loss: 1537536690893.2322\n",
      "Epoch 107/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1439310028828.8113 - val_loss: 1536693170861.5471\n",
      "Epoch 108/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1438515963444.4604 - val_loss: 1535815161380.7258\n",
      "Epoch 109/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1437776062204.2185 - val_loss: 1534906781724.8044\n",
      "Epoch 110/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1436998511876.5020 - val_loss: 1534173647545.0688\n",
      "Epoch 111/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1436111736638.9646 - val_loss: 1533178450110.1096\n",
      "Epoch 112/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1435280734258.4197 - val_loss: 1532360246884.0957\n",
      "Epoch 113/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1434488767670.2312 - val_loss: 1531333378508.8721\n",
      "Epoch 114/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1433809311750.2424 - val_loss: 1530434034156.5569\n",
      "Epoch 115/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1432867031177.8137 - val_loss: 1529566705108.0732\n",
      "Epoch 116/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1432015432976.0261 - val_loss: 1528788561614.6724\n",
      "Epoch 117/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1431323970476.6875 - val_loss: 1528050665705.3164\n",
      "Epoch 118/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1430494225634.4084 - val_loss: 1527168550960.9675\n",
      "Epoch 119/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1429648044655.2837 - val_loss: 1526358954796.2869\n",
      "Epoch 120/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1428789565465.4500 - val_loss: 1525353608959.6399\n",
      "Epoch 121/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1427981129473.0203 - val_loss: 1524692519214.4473\n",
      "Epoch 122/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1427033513793.8457 - val_loss: 1523884348250.3740\n",
      "Epoch 123/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1426280288815.8987 - val_loss: 1523112351314.8130\n",
      "Epoch 124/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1425552300437.7585 - val_loss: 1521835553707.0266\n",
      "Epoch 125/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1424804445954.4609 - val_loss: 1520913593074.6780\n",
      "Epoch 126/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1424047029607.4204 - val_loss: 1520090789424.2476\n",
      "Epoch 127/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 101us/step - loss: 1423368843779.7214 - val_loss: 1519425350153.3616\n",
      "Epoch 128/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1422402744735.8425 - val_loss: 1518373548786.6780\n",
      "Epoch 129/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1421592204861.1040 - val_loss: 1517472285547.6567\n",
      "Epoch 130/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1420886770609.0090 - val_loss: 1516752191464.9563\n",
      "Epoch 131/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1420078116711.3005 - val_loss: 1515956444872.9114\n",
      "Epoch 132/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1419233353814.4338 - val_loss: 1515181251144.7314\n",
      "Epoch 133/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1418448144840.6584 - val_loss: 1514336173549.9971\n",
      "Epoch 134/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1417728612358.2424 - val_loss: 1513403280296.1462\n",
      "Epoch 135/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1416767136114.7048 - val_loss: 1512422599407.7974\n",
      "Epoch 136/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1416137839743.4897 - val_loss: 1511719696957.2095\n",
      "Epoch 137/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1415281778901.2034 - val_loss: 1510793784439.5386\n",
      "Epoch 138/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1414442138783.6624 - val_loss: 1510038766213.2207\n",
      "Epoch 139/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1413882375416.9773 - val_loss: 1509245226460.7146\n",
      "Epoch 140/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1412977903415.7617 - val_loss: 1508172107735.6736\n",
      "Epoch 141/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1412196077113.7427 - val_loss: 1507259638670.2222\n",
      "Epoch 142/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1411459321601.5007 - val_loss: 1506449144173.8171\n",
      "Epoch 143/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1410714086191.8389 - val_loss: 1505648893203.0830\n",
      "Epoch 144/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1409813680741.6797 - val_loss: 1504929722707.8931\n",
      "Epoch 145/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1409148364801.4407 - val_loss: 1504139336426.0366\n",
      "Epoch 146/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1408370348976.0486 - val_loss: 1503235061111.8987\n",
      "Epoch 147/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1407583591063.1389 - val_loss: 1502487710136.7087\n",
      "Epoch 148/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1406650509194.3540 - val_loss: 1501466969884.4443\n",
      "Epoch 149/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1405980456530.2322 - val_loss: 1500615729324.8269\n",
      "Epoch 150/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1405170577608.7183 - val_loss: 1499724948158.8298\n",
      "Epoch 151/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1404441170011.2356 - val_loss: 1499096515804.3545\n",
      "Epoch 152/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 1403625811327.4297 - val_loss: 1498096949302.7285\n",
      "Epoch 153/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1402888700630.0435 - val_loss: 1497287835025.8228\n",
      "Epoch 154/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1402187874073.5100 - val_loss: 1496544665265.8677\n",
      "Epoch 155/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1401338798600.5234 - val_loss: 1495764019606.1436\n",
      "Epoch 156/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1400547135122.0972 - val_loss: 1495133924609.8003\n",
      "Epoch 157/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1399827980136.2607 - val_loss: 1494085161680.1125\n",
      "Epoch 158/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1399077863639.1240 - val_loss: 1493524960136.4614\n",
      "Epoch 159/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1398277288417.1479 - val_loss: 1492443842148.0957\n",
      "Epoch 160/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1397542695746.5659 - val_loss: 1491686420566.4136\n",
      "Epoch 161/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1396816927402.3464 - val_loss: 1490745597364.3882\n",
      "Epoch 162/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1396105690179.4663 - val_loss: 1490064410281.2266\n",
      "Epoch 163/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1395255485305.3074 - val_loss: 1489183656399.7524\n",
      "Epoch 164/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1394584694120.3806 - val_loss: 1488435978021.0857\n",
      "Epoch 165/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1393776014077.4189 - val_loss: 1487740126727.9211\n",
      "Epoch 166/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 1393154242814.0193 - val_loss: 1486953218554.9592\n",
      "Epoch 167/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1392325892513.2832 - val_loss: 1486010795389.6597\n",
      "Epoch 168/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1391567688351.3022 - val_loss: 1485197020590.6272\n",
      "Epoch 169/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1390770901663.3022 - val_loss: 1484264172133.5359\n",
      "Epoch 170/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1390113315006.6345 - val_loss: 1483546903011.9155\n",
      "Epoch 171/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1389304983226.4329 - val_loss: 1482901299119.3474\n",
      "Epoch 172/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1388543656354.7236 - val_loss: 1481881457751.8538\n",
      "Epoch 173/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1387857327406.2781 - val_loss: 1481134968791.6736\n",
      "Epoch 174/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1387078806070.1411 - val_loss: 1480365509233.0576\n",
      "Epoch 175/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1386102607321.7051 - val_loss: 1479491229497.2490\n",
      "Epoch 176/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1385487771833.8325 - val_loss: 1478655975850.3066\n",
      "Epoch 177/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1384869695871.1897 - val_loss: 1477857321155.8706\n",
      "Epoch 178/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1384129184569.9226 - val_loss: 1477080773017.0239\n",
      "Epoch 179/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1383368099858.7275 - val_loss: 1476335094085.4910\n",
      "Epoch 180/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1382621978775.7393 - val_loss: 1475601760809.0464\n",
      "Epoch 181/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1382006078792.6882 - val_loss: 1474926093313.4402\n",
      "Epoch 182/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1381012507487.8574 - val_loss: 1473957888783.4824\n",
      "Epoch 183/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1380437514446.0005 - val_loss: 1473336715347.5330\n",
      "Epoch 184/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1379843225147.1831 - val_loss: 1472517467969.8904\n",
      "Epoch 185/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1378917155953.0842 - val_loss: 1471553402070.5935\n",
      "Epoch 186/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1378344469040.8591 - val_loss: 1471056246527.6399\n",
      "Epoch 187/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1377402748098.2358 - val_loss: 1470059160254.8298\n",
      "Epoch 188/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1376816330625.7107 - val_loss: 1469330832785.8228\n",
      "Epoch 189/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1376103421189.9421 - val_loss: 1468710618148.0056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1375419335677.8394 - val_loss: 1467747981873.6877\n",
      "Epoch 191/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1374528892007.0002 - val_loss: 1466891401097.9016\n",
      "Epoch 192/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1373825551172.9670 - val_loss: 1466252105252.7258\n",
      "Epoch 193/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1373234498369.1255 - val_loss: 1465473116335.7075\n",
      "Epoch 194/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1372500538639.5461 - val_loss: 1464708276189.4346\n",
      "Epoch 195/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1371808382810.8154 - val_loss: 1464040410335.2349\n",
      "Epoch 196/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1371039110455.8818 - val_loss: 1463159678022.5710\n",
      "Epoch 197/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1370270662702.0981 - val_loss: 1462278922113.2603\n",
      "Epoch 198/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1369588348797.8691 - val_loss: 1461561359877.0408\n",
      "Epoch 199/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1368822131742.0117 - val_loss: 1460704500734.5598\n",
      "Epoch 200/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1368183194018.7236 - val_loss: 1459981541318.3911\n",
      "Epoch 201/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1367519530784.2327 - val_loss: 1459286173572.1406\n",
      "Epoch 202/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1366757466359.7769 - val_loss: 1458591722039.4487\n",
      "Epoch 203/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1366117159648.6077 - val_loss: 1457747448758.5486\n",
      "Epoch 204/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1365357413492.9255 - val_loss: 1457019059901.3896\n",
      "Epoch 205/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1364644869391.5461 - val_loss: 1456241005264.1125\n",
      "Epoch 206/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1363897003829.3608 - val_loss: 1455493845642.9817\n",
      "Epoch 207/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1363215647457.5681 - val_loss: 1454847834155.2068\n",
      "Epoch 208/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1362634891027.7478 - val_loss: 1454098527054.8523\n",
      "Epoch 209/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1361876581977.4351 - val_loss: 1453304797791.7749\n",
      "Epoch 210/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1361100017593.8926 - val_loss: 1452456292968.4163\n",
      "Epoch 211/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1360382430385.9094 - val_loss: 1451641943001.1140\n",
      "Epoch 212/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1359742777329.1143 - val_loss: 1450901091796.0732\n",
      "Epoch 213/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1359087248948.4604 - val_loss: 1450251621187.3306\n",
      "Epoch 214/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1358386379059.8003 - val_loss: 1449524392805.8960\n",
      "Epoch 215/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1357620635999.0171 - val_loss: 1448751677831.7412\n",
      "Epoch 216/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1356932436424.1782 - val_loss: 1448038396359.1111\n",
      "Epoch 217/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1356302099774.6042 - val_loss: 1447220972191.1448\n",
      "Epoch 218/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1355599538857.3860 - val_loss: 1446475980143.2573\n",
      "Epoch 219/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1354905328317.5540 - val_loss: 1445869856206.3123\n",
      "Epoch 220/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1354250716150.1560 - val_loss: 1445059725430.0984\n",
      "Epoch 221/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1353477141744.8140 - val_loss: 1444215905647.2573\n",
      "Epoch 222/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1352772232718.0454 - val_loss: 1443722713050.5542\n",
      "Epoch 223/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1352092518188.2373 - val_loss: 1442919074854.8860\n",
      "Epoch 224/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1351396900107.2244 - val_loss: 1442316347544.6638\n",
      "Epoch 225/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1350839498233.8777 - val_loss: 1441383112508.1294\n",
      "Epoch 226/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1350069441982.3342 - val_loss: 1440655973904.5625\n",
      "Epoch 227/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1349420811815.0151 - val_loss: 1439865359214.5374\n",
      "Epoch 228/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1348699103752.5234 - val_loss: 1439117565433.5190\n",
      "Epoch 229/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1347988880916.2881 - val_loss: 1438411210314.1716\n",
      "Epoch 230/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1347302290592.1426 - val_loss: 1437725737121.3052\n",
      "Epoch 231/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1346691686716.9238 - val_loss: 1436976715024.2026\n",
      "Epoch 232/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1345949564244.2129 - val_loss: 1436296883457.8003\n",
      "Epoch 233/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1345339434010.6504 - val_loss: 1435604023961.3840\n",
      "Epoch 234/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1344647190856.9285 - val_loss: 1434828423375.3923\n",
      "Epoch 235/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1343879597406.7771 - val_loss: 1434016641386.9368\n",
      "Epoch 236/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1343370945359.0508 - val_loss: 1433305211698.0479\n",
      "Epoch 237/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1342715606287.5461 - val_loss: 1432603489632.8552\n",
      "Epoch 238/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1341867359472.3340 - val_loss: 1432236072141.9521\n",
      "Epoch 239/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1341261367180.2747 - val_loss: 1431266133442.7905\n",
      "Epoch 240/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1340635247468.8225 - val_loss: 1430603080014.1323\n",
      "Epoch 241/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1339894217856.9302 - val_loss: 1429825412324.9958\n",
      "Epoch 242/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1339260464822.5913 - val_loss: 1429146893322.0815\n",
      "Epoch 243/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1338655599396.3142 - val_loss: 1428395729644.9170\n",
      "Epoch 244/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1337963028332.1023 - val_loss: 1427725582474.2617\n",
      "Epoch 245/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1337339300000.8628 - val_loss: 1427040183758.3123\n",
      "Epoch 246/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1336600685340.6313 - val_loss: 1426218916102.1208\n",
      "Epoch 247/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1335956256461.1602 - val_loss: 1425490564483.4207\n",
      "Epoch 248/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1335215823242.2339 - val_loss: 1424986061903.2124\n",
      "Epoch 249/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1334665890244.8169 - val_loss: 1424144604918.9985\n",
      "Epoch 250/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1334041132099.4663 - val_loss: 1423368138262.3235\n",
      "Epoch 251/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1333345494488.2644 - val_loss: 1422686626172.2195\n",
      "Epoch 252/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1332706298723.6990 - val_loss: 1422023050299.0493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1332008818744.1821 - val_loss: 1421379171911.2913\n",
      "Epoch 254/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1331359510167.8594 - val_loss: 1420811598443.2969\n",
      "Epoch 255/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1330791959557.0420 - val_loss: 1419913736439.7188\n",
      "Epoch 256/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1330043858679.8967 - val_loss: 1419299299662.1323\n",
      "Epoch 257/800\n",
      "4265/4265 [==============================] - 0s 69us/step - loss: 1329299535565.1602 - val_loss: 1418740979213.6821\n",
      "Epoch 258/800\n",
      "4265/4265 [==============================] - 0s 70us/step - loss: 1328737898959.8613 - val_loss: 1417826418526.6948\n",
      "Epoch 259/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1328100532579.3389 - val_loss: 1417076808909.9521\n",
      "Epoch 260/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1327524550535.4729 - val_loss: 1416608581565.7498\n",
      "Epoch 261/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1326819212443.8208 - val_loss: 1415809228926.7397\n",
      "Epoch 262/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1326209057440.2625 - val_loss: 1415199943960.8440\n",
      "Epoch 263/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1325550377861.3120 - val_loss: 1414370518317.0071\n",
      "Epoch 264/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1324889284836.5693 - val_loss: 1413707634489.2490\n",
      "Epoch 265/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1324250941285.8599 - val_loss: 1412988380852.7483\n",
      "Epoch 266/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1323691141916.6313 - val_loss: 1412346982878.1548\n",
      "Epoch 267/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1322977358777.6526 - val_loss: 1411786604544.0000\n",
      "Epoch 268/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1322196209547.7947 - val_loss: 1411565550970.7793\n",
      "Epoch 269/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1321853250762.8792 - val_loss: 1410583602131.3530\n",
      "Epoch 270/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1321142756255.7224 - val_loss: 1409656379999.7749\n",
      "Epoch 271/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1320426150320.6489 - val_loss: 1408798087360.9902\n",
      "Epoch 272/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1319828105570.8586 - val_loss: 1408250272213.5134\n",
      "Epoch 273/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1319242736851.0422 - val_loss: 1407797648705.1702\n",
      "Epoch 274/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1318615091587.5115 - val_loss: 1406786882460.6245\n",
      "Epoch 275/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1317888537979.1082 - val_loss: 1406091702934.5034\n",
      "Epoch 276/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1317337309566.2292 - val_loss: 1405569597440.0000\n",
      "Epoch 277/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1316707143848.0657 - val_loss: 1404960619599.2124\n",
      "Epoch 278/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1316056213282.6335 - val_loss: 1404206062613.6033\n",
      "Epoch 279/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1315449328195.1062 - val_loss: 1403514450974.2446\n",
      "Epoch 280/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1314774466648.3545 - val_loss: 1402866265092.3206\n",
      "Epoch 281/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1313967012697.1348 - val_loss: 1402051514903.7637\n",
      "Epoch 282/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1313499836378.5454 - val_loss: 1401470304601.6541\n",
      "Epoch 283/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1312927544265.0186 - val_loss: 1400976857050.5542\n",
      "Epoch 284/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1312358793338.9280 - val_loss: 1400185281903.2573\n",
      "Epoch 285/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1311651369081.9678 - val_loss: 1399554221472.2251\n",
      "Epoch 286/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1311008953586.7349 - val_loss: 1398885237726.8748\n",
      "Epoch 287/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1310420283281.5569 - val_loss: 1398155009621.6934\n",
      "Epoch 288/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1309742038771.0950 - val_loss: 1397575590083.8706\n",
      "Epoch 289/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1309170697428.9634 - val_loss: 1396909686069.6484\n",
      "Epoch 290/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1308625712776.0132 - val_loss: 1396218431882.6216\n",
      "Epoch 291/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1307945764406.3811 - val_loss: 1395626339839.2798\n",
      "Epoch 292/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 1307287783089.3093 - val_loss: 1395023326051.0154\n",
      "Epoch 293/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1306602010205.7566 - val_loss: 1394200949168.0676\n",
      "Epoch 294/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1306118068841.0410 - val_loss: 1393626508522.7566\n",
      "Epoch 295/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1305502399962.6655 - val_loss: 1392887260046.2222\n",
      "Epoch 296/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1304793429179.0330 - val_loss: 1392368594869.1084\n",
      "Epoch 297/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1304307560579.5713 - val_loss: 1391533246835.5781\n",
      "Epoch 298/800\n",
      "4265/4265 [==============================] - 1s 139us/step - loss: 1303643555205.6721 - val_loss: 1390890069710.6724\n",
      "Epoch 299/800\n",
      "4265/4265 [==============================] - 1s 140us/step - loss: 1302995531211.7795 - val_loss: 1390218519092.5681\n",
      "Epoch 300/800\n",
      "4265/4265 [==============================] - 1s 151us/step - loss: 1302413624360.0957 - val_loss: 1389600844965.6260\n",
      "Epoch 301/800\n",
      "4265/4265 [==============================] - 1s 210us/step - loss: 1301864834132.2727 - val_loss: 1388986861317.4009\n",
      "Epoch 302/800\n",
      "4265/4265 [==============================] - 1s 136us/step - loss: 1301250657292.7251 - val_loss: 1388443312738.6555\n",
      "Epoch 303/800\n",
      "4265/4265 [==============================] - 1s 150us/step - loss: 1300601727642.0200 - val_loss: 1387838006941.7046\n",
      "Epoch 304/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1299920797430.9363 - val_loss: 1387015467463.1111\n",
      "Epoch 305/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1299471824908.0046 - val_loss: 1386482564659.1279\n",
      "Epoch 306/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1298712643883.1567 - val_loss: 1385757954768.1125\n",
      "Epoch 307/800\n",
      "4265/4265 [==============================] - 1s 152us/step - loss: 1298250599240.8083 - val_loss: 1385125501397.5134\n",
      "Epoch 308/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1297551070596.7117 - val_loss: 1384645685703.1111\n",
      "Epoch 309/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1297001404683.4644 - val_loss: 1383894849798.1208\n",
      "Epoch 310/800\n",
      "4265/4265 [==============================] - 1s 144us/step - loss: 1296488318284.0496 - val_loss: 1383265031324.9846\n",
      "Epoch 311/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1295701557912.5796 - val_loss: 1382516563933.4346\n",
      "Epoch 312/800\n",
      "4265/4265 [==============================] - 1s 146us/step - loss: 1295200731155.2075 - val_loss: 1382024527572.4331\n",
      "Epoch 313/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1294603086505.3860 - val_loss: 1381264246372.0957\n",
      "Epoch 314/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1293989437088.5029 - val_loss: 1380726840479.8650\n",
      "Epoch 315/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1293550052321.0278 - val_loss: 1380200717103.1675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1292692498241.3655 - val_loss: 1379360366396.1294\n",
      "Epoch 317/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1292256646010.2676 - val_loss: 1378724613181.9297\n",
      "Epoch 318/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1291698667404.7551 - val_loss: 1378105600498.3179\n",
      "Epoch 319/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1291045841412.2017 - val_loss: 1377522109248.4500\n",
      "Epoch 320/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1290579017359.2161 - val_loss: 1376988568044.5569\n",
      "Epoch 321/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1289761221058.8960 - val_loss: 1376660733007.2124\n",
      "Epoch 322/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1289415224015.8010 - val_loss: 1375816287175.8313\n",
      "Epoch 323/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1288724258176.3904 - val_loss: 1375072370434.5205\n",
      "Epoch 324/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1288175625573.0195 - val_loss: 1374444235987.7131\n",
      "Epoch 325/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1287533653740.3723 - val_loss: 1373906734418.4529\n",
      "Epoch 326/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1287036080249.9678 - val_loss: 1373186841091.6006\n",
      "Epoch 327/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1286506244347.1379 - val_loss: 1372529825270.6384\n",
      "Epoch 328/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1285789947944.8159 - val_loss: 1371871894526.5598\n",
      "Epoch 329/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1285395127273.1912 - val_loss: 1371296614356.7932\n",
      "Epoch 330/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1284663375192.5347 - val_loss: 1370651144485.8059\n",
      "Epoch 331/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1284125109892.6519 - val_loss: 1370083124709.3560\n",
      "Epoch 332/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1283603938932.3254 - val_loss: 1369559398604.5120\n",
      "Epoch 333/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1282913632555.6372 - val_loss: 1368808995919.2124\n",
      "Epoch 334/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1282365602145.8982 - val_loss: 1368363221890.7004\n",
      "Epoch 335/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1281883639307.6445 - val_loss: 1367593175549.8396\n",
      "Epoch 336/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1281255655584.8628 - val_loss: 1367031055672.5288\n",
      "Epoch 337/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1280671595380.0254 - val_loss: 1366410318037.1533\n",
      "Epoch 338/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1279999959141.7998 - val_loss: 1365964785321.2266\n",
      "Epoch 339/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1279527770540.8074 - val_loss: 1365250669301.5583\n",
      "Epoch 340/800\n",
      "4265/4265 [==============================] - 1s 132us/step - loss: 1278950083930.9353 - val_loss: 1364571661036.9170\n",
      "Epoch 341/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1278464558085.5222 - val_loss: 1364249200135.9211\n",
      "Epoch 342/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1277789760338.4124 - val_loss: 1363592682425.4290\n",
      "Epoch 343/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1277411519706.2451 - val_loss: 1362953285372.7595\n",
      "Epoch 344/800\n",
      "4265/4265 [==============================] - 1s 139us/step - loss: 1276697028409.6826 - val_loss: 1362195759818.3516\n",
      "Epoch 345/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1276131156688.2815 - val_loss: 1361580590991.6624\n",
      "Epoch 346/800\n",
      "4265/4265 [==============================] - 1s 140us/step - loss: 1275550187623.7205 - val_loss: 1361013547157.7834\n",
      "Epoch 347/800\n",
      "4265/4265 [==============================] - 1s 136us/step - loss: 1274950593888.6978 - val_loss: 1360427837270.0535\n",
      "Epoch 348/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1274359205045.7510 - val_loss: 1359855393490.9929\n",
      "Epoch 349/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1273731942902.7563 - val_loss: 1359655960345.5640\n",
      "Epoch 350/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1273374479840.9077 - val_loss: 1358787398003.5781\n",
      "Epoch 351/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1272704026231.6868 - val_loss: 1358149663408.4275\n",
      "Epoch 352/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1272181516877.1902 - val_loss: 1357474581773.3220\n",
      "Epoch 353/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1271708011655.6531 - val_loss: 1356961240220.9846\n",
      "Epoch 354/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1271093503356.0686 - val_loss: 1356388287086.1772\n",
      "Epoch 355/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1270542550394.1477 - val_loss: 1355684458098.4978\n",
      "Epoch 356/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1270032382182.9702 - val_loss: 1355037512146.6328\n",
      "Epoch 357/800\n",
      "4265/4265 [==============================] - 1s 132us/step - loss: 1269527846810.2002 - val_loss: 1354468971953.5078\n",
      "Epoch 358/800\n",
      "4265/4265 [==============================] - 1s 135us/step - loss: 1268956413333.5186 - val_loss: 1353865382443.9268\n",
      "Epoch 359/800\n",
      "4265/4265 [==============================] - 1s 138us/step - loss: 1268373654437.0044 - val_loss: 1353266000880.1575\n",
      "Epoch 360/800\n",
      "4265/4265 [==============================] - 1s 142us/step - loss: 1267824811595.9897 - val_loss: 1352757929307.0942\n",
      "Epoch 361/800\n",
      "4265/4265 [==============================] - 1s 150us/step - loss: 1267211702294.0886 - val_loss: 1352329600574.6499\n",
      "Epoch 362/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1266678719687.5181 - val_loss: 1351593239369.0913\n",
      "Epoch 363/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1266114323594.0537 - val_loss: 1350951573535.6851\n",
      "Epoch 364/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1265687798802.9675 - val_loss: 1350372382037.3333\n",
      "Epoch 365/800\n",
      "4265/4265 [==============================] - 1s 142us/step - loss: 1265107461065.9790 - val_loss: 1349825396643.8257\n",
      "Epoch 366/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1264392706949.3120 - val_loss: 1349200563146.7117\n",
      "Epoch 367/800\n",
      "4265/4265 [==============================] - 1s 136us/step - loss: 1263977936374.0361 - val_loss: 1348654955760.5176\n",
      "Epoch 368/800\n",
      "4265/4265 [==============================] - 1s 143us/step - loss: 1263448437570.5659 - val_loss: 1348048657613.9521\n",
      "Epoch 369/800\n",
      "4265/4265 [==============================] - 1s 143us/step - loss: 1262884401886.6870 - val_loss: 1347470231531.8369\n",
      "Epoch 370/800\n",
      "4265/4265 [==============================] - 1s 150us/step - loss: 1262283484355.1963 - val_loss: 1346940464685.3672\n",
      "Epoch 371/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1261865800063.1897 - val_loss: 1346322309961.0913\n",
      "Epoch 372/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1261281048403.6128 - val_loss: 1345757985456.4275\n",
      "Epoch 373/800\n",
      "4265/4265 [==============================] - 1s 129us/step - loss: 1260699444492.6650 - val_loss: 1345345102757.2659\n",
      "Epoch 374/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1260314419816.5608 - val_loss: 1344613197077.9634\n",
      "Epoch 375/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1259648853118.5295 - val_loss: 1344080006136.7988\n",
      "Epoch 376/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1259229211097.9453 - val_loss: 1343567019640.2588\n",
      "Epoch 377/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1258574703327.1672 - val_loss: 1343096010656.9451\n",
      "Epoch 378/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 90us/step - loss: 1258062294537.7239 - val_loss: 1342497175871.7300\n",
      "Epoch 379/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1257589236144.1689 - val_loss: 1341826698440.1912\n",
      "Epoch 380/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1257102019761.9094 - val_loss: 1341236368126.1997\n",
      "Epoch 381/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1256535125248.6602 - val_loss: 1340704000025.9241\n",
      "Epoch 382/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1255887472697.1423 - val_loss: 1340107705198.5374\n",
      "Epoch 383/800\n",
      "4265/4265 [==============================] - 1s 126us/step - loss: 1255413458307.7515 - val_loss: 1339590064171.2068\n",
      "Epoch 384/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1254921695279.0586 - val_loss: 1339038594400.8552\n",
      "Epoch 385/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1254407932683.3445 - val_loss: 1338457600247.7188\n",
      "Epoch 386/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1253870452263.4954 - val_loss: 1337881292886.4136\n",
      "Epoch 387/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1253330753453.8879 - val_loss: 1337333141302.3684\n",
      "Epoch 388/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1252808888610.9937 - val_loss: 1336818235642.5991\n",
      "Epoch 389/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1252270337672.4932 - val_loss: 1336293453311.2798\n",
      "Epoch 390/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1251791107186.0444 - val_loss: 1335766306430.0198\n",
      "Epoch 391/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1251318785623.0339 - val_loss: 1335265288318.7397\n",
      "Epoch 392/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1250828752222.2969 - val_loss: 1334676577601.1702\n",
      "Epoch 393/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1250253751989.1506 - val_loss: 1334060853239.3586\n",
      "Epoch 394/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1249751784213.9084 - val_loss: 1333489218198.5034\n",
      "Epoch 395/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1249142716355.0161 - val_loss: 1332894533289.2266\n",
      "Epoch 396/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1248695050280.3357 - val_loss: 1332342067758.8074\n",
      "Epoch 397/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1248233269610.5417 - val_loss: 1331791701961.2715\n",
      "Epoch 398/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1247621312144.1765 - val_loss: 1331310996138.6667\n",
      "Epoch 399/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1247104719345.9546 - val_loss: 1330842041999.3025\n",
      "Epoch 400/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1246605807714.9185 - val_loss: 1330199742743.4036\n",
      "Epoch 401/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1246075115764.4155 - val_loss: 1329728751365.4009\n",
      "Epoch 402/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1245675683924.5132 - val_loss: 1329142057792.4500\n",
      "Epoch 403/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1245169184776.6433 - val_loss: 1328602171026.1829\n",
      "Epoch 404/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1244602636213.5708 - val_loss: 1328105488614.4360\n",
      "Epoch 405/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1243980531746.0933 - val_loss: 1327454483209.7214\n",
      "Epoch 406/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1243663391763.4475 - val_loss: 1326984053542.5261\n",
      "Epoch 407/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1243080575597.8430 - val_loss: 1326488411411.0830\n",
      "Epoch 408/800\n",
      "4265/4265 [==============================] - 1s 135us/step - loss: 1242525871003.4009 - val_loss: 1325965878411.7019\n",
      "Epoch 409/800\n",
      "4265/4265 [==============================] - 1s 129us/step - loss: 1242064031351.2065 - val_loss: 1325348602386.0029\n",
      "Epoch 410/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1241535289715.1851 - val_loss: 1324945637099.4768\n",
      "Epoch 411/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1241094454366.3567 - val_loss: 1324395820753.5527\n",
      "Epoch 412/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1240603083877.5596 - val_loss: 1323794538461.4346\n",
      "Epoch 413/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1240032121491.7778 - val_loss: 1323243388444.0845\n",
      "Epoch 414/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 1239571182909.6440 - val_loss: 1322781715713.8003\n",
      "Epoch 415/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1239089497735.0527 - val_loss: 1322225531591.4712\n",
      "Epoch 416/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1238430597715.4326 - val_loss: 1321592530704.9226\n",
      "Epoch 417/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1238058495908.7644 - val_loss: 1321062036049.3728\n",
      "Epoch 418/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1237652237146.0952 - val_loss: 1320549605825.3501\n",
      "Epoch 419/800\n",
      "4265/4265 [==============================] - 1s 136us/step - loss: 1237008769025.9207 - val_loss: 1320028576559.1675\n",
      "Epoch 420/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1236625733632.4802 - val_loss: 1319527313356.1519\n",
      "Epoch 421/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1236103275262.3792 - val_loss: 1318986548628.7031\n",
      "Epoch 422/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1235587705225.2734 - val_loss: 1318502970006.5034\n",
      "Epoch 423/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1235031517773.1902 - val_loss: 1318007801651.4880\n",
      "Epoch 424/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1234487792149.0083 - val_loss: 1317547994070.2334\n",
      "Epoch 425/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1234072114065.3169 - val_loss: 1316926811877.7158\n",
      "Epoch 426/800\n",
      "4265/4265 [==============================] - 1s 135us/step - loss: 1233589251078.9629 - val_loss: 1316368349757.2095\n",
      "Epoch 427/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1233125732583.4504 - val_loss: 1315844943774.0647\n",
      "Epoch 428/800\n",
      "4265/4265 [==============================] - 1s 139us/step - loss: 1232613940078.2629 - val_loss: 1315402957652.6133\n",
      "Epoch 429/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1232129651211.4045 - val_loss: 1314834294959.7075\n",
      "Epoch 430/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1231716532212.4753 - val_loss: 1314455111222.0085\n",
      "Epoch 431/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1231209552431.4185 - val_loss: 1313875992264.9114\n",
      "Epoch 432/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1230699004941.2051 - val_loss: 1313473567489.0801\n",
      "Epoch 433/800\n",
      "4265/4265 [==============================] - 1s 142us/step - loss: 1230203888804.9443 - val_loss: 1312821167116.9619\n",
      "Epoch 434/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1229717996326.7151 - val_loss: 1312385620114.9028\n",
      "Epoch 435/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1229210337107.1323 - val_loss: 1311857601632.4951\n",
      "Epoch 436/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1228768099595.2244 - val_loss: 1311282222651.7693\n",
      "Epoch 437/800\n",
      "4265/4265 [==============================] - 1s 144us/step - loss: 1228398627326.4395 - val_loss: 1310741193199.4375\n",
      "Epoch 438/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1227748673408.7502 - val_loss: 1310223764173.2322\n",
      "Epoch 439/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1227366345466.0579 - val_loss: 1309720296250.6892\n",
      "Epoch 440/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 1226520744768.6453 - val_loss: 1309467023852.5569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1226433777006.1431 - val_loss: 1308879936141.8621\n",
      "Epoch 442/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1225911944431.8538 - val_loss: 1308250911935.5500\n",
      "Epoch 443/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1225374428516.5393 - val_loss: 1307718251642.4192\n",
      "Epoch 444/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1225024747013.4021 - val_loss: 1307273005312.3601\n",
      "Epoch 445/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1224423121109.2034 - val_loss: 1306903261708.2419\n",
      "Epoch 446/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1224047506173.4189 - val_loss: 1306320966329.0688\n",
      "Epoch 447/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1223562800672.7727 - val_loss: 1305788600256.6301\n",
      "Epoch 448/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1223109494706.6897 - val_loss: 1305215845481.1365\n",
      "Epoch 449/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1222579552478.8069 - val_loss: 1304766308524.8269\n",
      "Epoch 450/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1222150936344.0696 - val_loss: 1304224858296.3489\n",
      "Epoch 451/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1221650018727.5254 - val_loss: 1303789311294.2898\n",
      "Epoch 452/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 1221153029859.7290 - val_loss: 1303295153539.4207\n",
      "Epoch 453/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1220710608939.2168 - val_loss: 1302742011541.0632\n",
      "Epoch 454/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1220283077997.6628 - val_loss: 1302266754792.5964\n",
      "Epoch 455/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1219801681166.1055 - val_loss: 1301747709119.5500\n",
      "Epoch 456/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1219321135764.7383 - val_loss: 1301280647873.7102\n",
      "Epoch 457/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1218940665971.2449 - val_loss: 1300758343651.1956\n",
      "Epoch 458/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1218525996634.8755 - val_loss: 1300285921081.2490\n",
      "Epoch 459/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1218048265570.6187 - val_loss: 1299792214059.2068\n",
      "Epoch 460/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1217483495276.5823 - val_loss: 1299366855186.0029\n",
      "Epoch 461/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1217045645526.6438 - val_loss: 1298872697062.4360\n",
      "Epoch 462/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1216516700275.7253 - val_loss: 1298356926714.5991\n",
      "Epoch 463/800\n",
      "4265/4265 [==============================] - 0s 71us/step - loss: 1216131979738.9055 - val_loss: 1297900789425.8677\n",
      "Epoch 464/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1215732952531.9426 - val_loss: 1297391553321.4065\n",
      "Epoch 465/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1215166821678.2781 - val_loss: 1296880695039.6399\n",
      "Epoch 466/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1214791462063.9888 - val_loss: 1296409342245.8059\n",
      "Epoch 467/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1214317110974.7546 - val_loss: 1296036082649.1140\n",
      "Epoch 468/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1213957174877.7566 - val_loss: 1295419065910.0085\n",
      "Epoch 469/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1213374148735.7300 - val_loss: 1294928037107.3979\n",
      "Epoch 470/800\n",
      "4265/4265 [==============================] - 1s 135us/step - loss: 1212915512257.3354 - val_loss: 1294512061772.6919\n",
      "Epoch 471/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1212511123713.8608 - val_loss: 1294041398351.2124\n",
      "Epoch 472/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1211990889895.2854 - val_loss: 1293566886556.2644\n",
      "Epoch 473/800\n",
      "4265/4265 [==============================] - 1s 129us/step - loss: 1211598516013.6777 - val_loss: 1293086267838.4697\n",
      "Epoch 474/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1211132683912.9736 - val_loss: 1292544629758.5598\n",
      "Epoch 475/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1210730260850.2246 - val_loss: 1292050095471.2573\n",
      "Epoch 476/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1210248740668.0835 - val_loss: 1291605840569.0688\n",
      "Epoch 477/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1209795076266.9468 - val_loss: 1291172155582.1096\n",
      "Epoch 478/800\n",
      "4265/4265 [==============================] - 1s 141us/step - loss: 1209423791762.5771 - val_loss: 1290655221255.9211\n",
      "Epoch 479/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1208921168043.4270 - val_loss: 1290169140375.2236\n",
      "Epoch 480/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1208407987942.1299 - val_loss: 1289778731730.9929\n",
      "Epoch 481/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1208023106707.6577 - val_loss: 1289374658323.8030\n",
      "Epoch 482/800\n",
      "4265/4265 [==============================] - 1s 129us/step - loss: 1207473345809.4668 - val_loss: 1288731874967.9438\n",
      "Epoch 483/800\n",
      "4265/4265 [==============================] - 1s 138us/step - loss: 1207117712638.7395 - val_loss: 1288307382441.9465\n",
      "Epoch 484/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1206717953994.6992 - val_loss: 1287806929728.4500\n",
      "Epoch 485/800\n",
      "4265/4265 [==============================] - 1s 165us/step - loss: 1206349032720.0261 - val_loss: 1287379867564.4670\n",
      "Epoch 486/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1205845486364.6313 - val_loss: 1286885027626.8467\n",
      "Epoch 487/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1205341919695.3809 - val_loss: 1286467478782.9199\n",
      "Epoch 488/800\n",
      "4265/4265 [==============================] - 1s 138us/step - loss: 1204909009723.3630 - val_loss: 1286059766883.3755\n",
      "Epoch 489/800\n",
      "4265/4265 [==============================] - 1s 158us/step - loss: 1204473150446.9534 - val_loss: 1285496284023.1787\n",
      "Epoch 490/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1204076548563.7026 - val_loss: 1285055395237.9858\n",
      "Epoch 491/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1203614309695.0847 - val_loss: 1284582786383.5725\n",
      "Epoch 492/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1203123203747.3838 - val_loss: 1284160626422.9985\n",
      "Epoch 493/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1202754329154.6262 - val_loss: 1283744826311.8313\n",
      "Epoch 494/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1202327649843.0198 - val_loss: 1283223568453.1309\n",
      "Epoch 495/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1201923881609.6938 - val_loss: 1282736138600.0562\n",
      "Epoch 496/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1201449013900.0947 - val_loss: 1282282008408.9338\n",
      "Epoch 497/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1201045817527.9119 - val_loss: 1281893754157.0071\n",
      "Epoch 498/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1200568447848.9810 - val_loss: 1281372565060.4106\n",
      "Epoch 499/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1200073986012.4661 - val_loss: 1280892059164.0845\n",
      "Epoch 500/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1199800442221.1826 - val_loss: 1280469562213.8960\n",
      "Epoch 501/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1199325146168.9021 - val_loss: 1280052434606.9873\n",
      "Epoch 502/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1198907620907.3369 - val_loss: 1279614590250.1265\n",
      "Epoch 503/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 101us/step - loss: 1198377732982.9065 - val_loss: 1279398082810.5991\n",
      "Epoch 504/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1198163804240.6714 - val_loss: 1278677572376.1238\n",
      "Epoch 505/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1197568908307.4475 - val_loss: 1278218164002.2053\n",
      "Epoch 506/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1197144349796.1191 - val_loss: 1277770618473.8564\n",
      "Epoch 507/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1196735123067.0479 - val_loss: 1277342928602.1941\n",
      "Epoch 508/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1196399218383.5610 - val_loss: 1276869538016.6750\n",
      "Epoch 509/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1195897430651.7683 - val_loss: 1276454809156.4106\n",
      "Epoch 510/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1195332764070.5649 - val_loss: 1276219131653.4009\n",
      "Epoch 511/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1195108223935.8950 - val_loss: 1275565615510.1436\n",
      "Epoch 512/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1194648477924.5693 - val_loss: 1275067352676.0957\n",
      "Epoch 513/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1194110114435.9314 - val_loss: 1274627787884.0168\n",
      "Epoch 514/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1193788995319.4167 - val_loss: 1274168497308.9846\n",
      "Epoch 515/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1193410424994.3035 - val_loss: 1273738598754.2954\n",
      "Epoch 516/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1192996605582.2556 - val_loss: 1273293964830.9648\n",
      "Epoch 517/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1192550082353.9995 - val_loss: 1272893701083.9944\n",
      "Epoch 518/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1192112411556.5244 - val_loss: 1272407943919.7974\n",
      "Epoch 519/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1191735537530.5081 - val_loss: 1271981018819.1504\n",
      "Epoch 520/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1191268963835.7983 - val_loss: 1271515324058.8242\n",
      "Epoch 521/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1190899001821.7866 - val_loss: 1271073953754.5542\n",
      "Epoch 522/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1190457977332.8357 - val_loss: 1270870700256.6750\n",
      "Epoch 523/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1190103734216.7783 - val_loss: 1270211622125.6372\n",
      "Epoch 524/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1189616339035.7158 - val_loss: 1269898057068.3770\n",
      "Epoch 525/800\n",
      "4265/4265 [==============================] - 1s 138us/step - loss: 1189218627803.9258 - val_loss: 1269353207816.6414\n",
      "Epoch 526/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1188879640285.4863 - val_loss: 1268962292397.5471\n",
      "Epoch 527/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1188461726325.5259 - val_loss: 1268463113442.1152\n",
      "Epoch 528/800\n",
      "4265/4265 [==============================] - 1s 126us/step - loss: 1187902883735.7993 - val_loss: 1268046073032.1912\n",
      "Epoch 529/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1187639524592.3340 - val_loss: 1267593647053.5920\n",
      "Epoch 530/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1187161576829.0288 - val_loss: 1267210701033.3164\n",
      "Epoch 531/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1186723507990.6289 - val_loss: 1266758023049.9016\n",
      "Epoch 532/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1186503560846.2556 - val_loss: 1266315277227.0266\n",
      "Epoch 533/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1186004895747.3613 - val_loss: 1265890361251.8257\n",
      "Epoch 534/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1185655295868.4287 - val_loss: 1265457706037.2883\n",
      "Epoch 535/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1185132736820.2805 - val_loss: 1265131502118.1660\n",
      "Epoch 536/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1184804040943.1333 - val_loss: 1264632700340.3882\n",
      "Epoch 537/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 1184380104923.9258 - val_loss: 1264307686945.8452\n",
      "Epoch 538/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1184022579445.3757 - val_loss: 1263791431391.9551\n",
      "Epoch 539/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1183641856988.7061 - val_loss: 1263354983108.5908\n",
      "Epoch 540/800\n",
      "4265/4265 [==============================] - 1s 135us/step - loss: 1183166589763.0461 - val_loss: 1262895957534.9648\n",
      "Epoch 541/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1182740419788.3198 - val_loss: 1262475746970.8242\n",
      "Epoch 542/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1182460807832.0994 - val_loss: 1262049266427.3193\n",
      "Epoch 543/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1181947787452.7136 - val_loss: 1261717412493.8621\n",
      "Epoch 544/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1181491840070.8276 - val_loss: 1261221811933.0745\n",
      "Epoch 545/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1181156580728.7070 - val_loss: 1260819117362.7678\n",
      "Epoch 546/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1180808689272.6472 - val_loss: 1260392966434.9255\n",
      "Epoch 547/800\n",
      "4265/4265 [==============================] - 1s 160us/step - loss: 1180390290991.6587 - val_loss: 1259981034883.4207\n",
      "Epoch 548/800\n",
      "4265/4265 [==============================] - 1s 126us/step - loss: 1180034963435.3518 - val_loss: 1259568004152.1687\n",
      "Epoch 549/800\n",
      "4265/4265 [==============================] - 1s 155us/step - loss: 1179560403153.3618 - val_loss: 1259118402397.2546\n",
      "Epoch 550/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1179275983582.2068 - val_loss: 1258714262808.8440\n",
      "Epoch 551/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1178900829949.1790 - val_loss: 1258302334759.9663\n",
      "Epoch 552/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1178484977553.5569 - val_loss: 1257909535941.3108\n",
      "Epoch 553/800\n",
      "4265/4265 [==============================] - 0s 74us/step - loss: 1178058709128.8535 - val_loss: 1257502198822.8860\n",
      "Epoch 554/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1177600244918.7112 - val_loss: 1257068312342.6836\n",
      "Epoch 555/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1177183297484.1396 - val_loss: 1256726507352.9338\n",
      "Epoch 556/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1176867264095.6772 - val_loss: 1256245529892.3657\n",
      "Epoch 557/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1176564020649.2061 - val_loss: 1255866741951.5500\n",
      "Epoch 558/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1176091661239.7319 - val_loss: 1255427010784.6750\n",
      "Epoch 559/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1175746909718.6887 - val_loss: 1255004225236.4331\n",
      "Epoch 560/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1175330945366.3735 - val_loss: 1254654946469.6260\n",
      "Epoch 561/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1175013337796.2766 - val_loss: 1254242017775.4375\n",
      "Epoch 562/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1174545131602.1121 - val_loss: 1253807832004.9507\n",
      "Epoch 563/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1174191286158.1956 - val_loss: 1253382846714.5991\n",
      "Epoch 564/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1173806510273.0354 - val_loss: 1252977279252.5232\n",
      "Epoch 565/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1173483449032.1182 - val_loss: 1252578649204.6582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/800\n",
      "4265/4265 [==============================] - 1s 134us/step - loss: 1173016664193.1707 - val_loss: 1252176915275.9719\n",
      "Epoch 567/800\n",
      "4265/4265 [==============================] - 1s 139us/step - loss: 1172702485070.1504 - val_loss: 1251772122078.8748\n",
      "Epoch 568/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1172339684750.3157 - val_loss: 1251393542175.6851\n",
      "Epoch 569/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1172011682801.8345 - val_loss: 1250976489230.0422\n",
      "Epoch 570/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1171626207287.2217 - val_loss: 1250601077361.0576\n",
      "Epoch 571/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1171161336683.3818 - val_loss: 1250181173737.6765\n",
      "Epoch 572/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1170894090187.1794 - val_loss: 1249790950824.8665\n",
      "Epoch 573/800\n",
      "4265/4265 [==============================] - 1s 138us/step - loss: 1170389736440.5571 - val_loss: 1249403191365.1309\n",
      "Epoch 574/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1170051301621.8562 - val_loss: 1249005740965.2659\n",
      "Epoch 575/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1169668015912.3960 - val_loss: 1248594840660.9734\n",
      "Epoch 576/800\n",
      "4265/4265 [==============================] - 1s 134us/step - loss: 1169297337493.0984 - val_loss: 1248206381597.5247\n",
      "Epoch 577/800\n",
      "4265/4265 [==============================] - 1s 156us/step - loss: 1168938514908.8262 - val_loss: 1247938557534.3347\n",
      "Epoch 578/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1168608801279.8799 - val_loss: 1247465343094.0984\n",
      "Epoch 579/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1168191358134.2312 - val_loss: 1247168451187.9382\n",
      "Epoch 580/800\n",
      "4265/4265 [==============================] - 1s 169us/step - loss: 1167938596861.1189 - val_loss: 1246737249748.0732\n",
      "Epoch 581/800\n",
      "4265/4265 [==============================] - 1s 169us/step - loss: 1167435149339.8511 - val_loss: 1246250722098.0479\n",
      "Epoch 582/800\n",
      "4265/4265 [==============================] - 1s 132us/step - loss: 1167039566622.0718 - val_loss: 1245830701505.3501\n",
      "Epoch 583/800\n",
      "4265/4265 [==============================] - 1s 153us/step - loss: 1166734446172.3162 - val_loss: 1245434585237.7834\n",
      "Epoch 584/800\n",
      "4265/4265 [==============================] - 1s 147us/step - loss: 1166382194543.9436 - val_loss: 1245059566123.9268\n",
      "Epoch 585/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1165980571135.6399 - val_loss: 1244676932667.0493\n",
      "Epoch 586/800\n",
      "4265/4265 [==============================] - 1s 136us/step - loss: 1165597799530.3616 - val_loss: 1244276071169.0801\n",
      "Epoch 587/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1165175985033.6338 - val_loss: 1243869153352.0112\n",
      "Epoch 588/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1164768347471.8911 - val_loss: 1243490605802.0366\n",
      "Epoch 589/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1164480850843.4009 - val_loss: 1243113460040.3713\n",
      "Epoch 590/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1164161229005.5203 - val_loss: 1242728587667.2629\n",
      "Epoch 591/800\n",
      "4265/4265 [==============================] - 0s 91us/step - loss: 1163758664590.9158 - val_loss: 1242341038180.8157\n",
      "Epoch 592/800\n",
      "4265/4265 [==============================] - 1s 129us/step - loss: 1163344382047.5574 - val_loss: 1242059383532.9170\n",
      "Epoch 593/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1163059626360.4670 - val_loss: 1241625627940.3657\n",
      "Epoch 594/800\n",
      "4265/4265 [==============================] - 1s 136us/step - loss: 1162646942972.3386 - val_loss: 1241187704313.5190\n",
      "Epoch 595/800\n",
      "4265/4265 [==============================] - 1s 140us/step - loss: 1162342079841.6582 - val_loss: 1240817197874.0479\n",
      "Epoch 596/800\n",
      "4265/4265 [==============================] - 1s 148us/step - loss: 1161999919919.8389 - val_loss: 1240474696322.3403\n",
      "Epoch 597/800\n",
      "4265/4265 [==============================] - 1s 138us/step - loss: 1161550780414.5593 - val_loss: 1240205749206.2334\n",
      "Epoch 598/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1161334360290.1685 - val_loss: 1239776246356.2532\n",
      "Epoch 599/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1160973784429.6628 - val_loss: 1239397368729.7441\n",
      "Epoch 600/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1160543754899.2976 - val_loss: 1238982387219.4431\n",
      "Epoch 601/800\n",
      "4265/4265 [==============================] - 1s 148us/step - loss: 1160181310947.7891 - val_loss: 1238539503964.5344\n",
      "Epoch 602/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1159646735824.1013 - val_loss: 1238200628277.2883\n",
      "Epoch 603/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1159508230274.6111 - val_loss: 1237772727049.7214\n",
      "Epoch 604/800\n",
      "4265/4265 [==============================] - 1s 132us/step - loss: 1159153961143.9119 - val_loss: 1237402130187.1619\n",
      "Epoch 605/800\n",
      "4265/4265 [==============================] - 1s 149us/step - loss: 1158857144143.7712 - val_loss: 1237032085817.9690\n",
      "Epoch 606/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1158461564960.1726 - val_loss: 1236672220360.1912\n",
      "Epoch 607/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1158021452243.7026 - val_loss: 1236288275077.2207\n",
      "Epoch 608/800\n",
      "4265/4265 [==============================] - 1s 135us/step - loss: 1157697426063.9360 - val_loss: 1235911277439.8201\n",
      "Epoch 609/800\n",
      "4265/4265 [==============================] - 1s 143us/step - loss: 1157392046405.0872 - val_loss: 1235540793122.2053\n",
      "Epoch 610/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1157010664132.2766 - val_loss: 1235210279865.4290\n",
      "Epoch 611/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1156635520248.9773 - val_loss: 1234808670556.5344\n",
      "Epoch 612/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1156310866974.9722 - val_loss: 1234417813483.8369\n",
      "Epoch 613/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1155944248418.6785 - val_loss: 1234045262984.8213\n",
      "Epoch 614/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1155557587194.1777 - val_loss: 1233697667599.1223\n",
      "Epoch 615/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1155292600445.3289 - val_loss: 1233320452430.1323\n",
      "Epoch 616/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1154845233270.1262 - val_loss: 1232937098445.9521\n",
      "Epoch 617/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1154607631449.5549 - val_loss: 1232578777093.7610\n",
      "Epoch 618/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1154225817591.8369 - val_loss: 1232194578423.3586\n",
      "Epoch 619/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1153900599502.0005 - val_loss: 1231840159366.6611\n",
      "Epoch 620/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1153557217767.8706 - val_loss: 1231529632285.5247\n",
      "Epoch 621/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1153189334159.3359 - val_loss: 1231119515279.3025\n",
      "Epoch 622/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1152786860373.4133 - val_loss: 1230742380025.5190\n",
      "Epoch 623/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1152574887554.2507 - val_loss: 1230381989542.3459\n",
      "Epoch 624/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1152098304691.4700 - val_loss: 1230032865997.2322\n",
      "Epoch 625/800\n",
      "4265/4265 [==============================] - 1s 142us/step - loss: 1151808097054.5520 - val_loss: 1229645273465.3389\n",
      "Epoch 626/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1151414350896.7390 - val_loss: 1229320372984.4387\n",
      "Epoch 627/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1150988468635.0405 - val_loss: 1229133320243.8481\n",
      "Epoch 628/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 1s 129us/step - loss: 1150825873457.9395 - val_loss: 1228572996661.2883\n",
      "Epoch 629/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1150477961127.1653 - val_loss: 1228221460174.6724\n",
      "Epoch 630/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1150008462083.6614 - val_loss: 1227844441429.3333\n",
      "Epoch 631/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1149833596208.6790 - val_loss: 1227476464992.8552\n",
      "Epoch 632/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1149419102198.3962 - val_loss: 1227208763060.7483\n",
      "Epoch 633/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1149040670029.0100 - val_loss: 1226760731957.6484\n",
      "Epoch 634/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1148701402818.5959 - val_loss: 1226402579929.8340\n",
      "Epoch 635/800\n",
      "4265/4265 [==============================] - 1s 127us/step - loss: 1148369279308.7700 - val_loss: 1226046773187.5105\n",
      "Epoch 636/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1148076497612.9199 - val_loss: 1225697895471.5273\n",
      "Epoch 637/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1147717993450.6318 - val_loss: 1225353837186.3403\n",
      "Epoch 638/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1147424889660.3235 - val_loss: 1225024538972.5344\n",
      "Epoch 639/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1147117632445.0139 - val_loss: 1224628948836.4558\n",
      "Epoch 640/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1146756843673.1799 - val_loss: 1224284884375.5837\n",
      "Epoch 641/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1146362473033.3486 - val_loss: 1223926948773.2659\n",
      "Epoch 642/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1146066804050.2922 - val_loss: 1223573796377.2039\n",
      "Epoch 643/800\n",
      "4265/4265 [==============================] - 1s 150us/step - loss: 1145697271024.3340 - val_loss: 1223250103473.1477\n",
      "Epoch 644/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1145453971367.1653 - val_loss: 1222868831893.0632\n",
      "Epoch 645/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1145109902987.8547 - val_loss: 1222521949384.1912\n",
      "Epoch 646/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1144846814024.8083 - val_loss: 1222174008713.1814\n",
      "Epoch 647/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1144451216601.7651 - val_loss: 1221833892266.3066\n",
      "Epoch 648/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1144099571361.9431 - val_loss: 1221480681339.4993\n",
      "Epoch 649/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1143757383143.6306 - val_loss: 1221142596324.2756\n",
      "Epoch 650/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1143427995304.6658 - val_loss: 1220784463007.8650\n",
      "Epoch 651/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1143155545511.2854 - val_loss: 1220457283607.0437\n",
      "Epoch 652/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1142791240521.7688 - val_loss: 1220094075536.7427\n",
      "Epoch 653/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1142458053254.3325 - val_loss: 1219782863138.9255\n",
      "Epoch 654/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1142102072919.5142 - val_loss: 1219421548330.8467\n",
      "Epoch 655/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1141841065465.8777 - val_loss: 1219089553370.5542\n",
      "Epoch 656/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1141480299220.6030 - val_loss: 1218719623481.9690\n",
      "Epoch 657/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1141180632426.3015 - val_loss: 1218405758304.8552\n",
      "Epoch 658/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1140890886015.3098 - val_loss: 1218086014797.4121\n",
      "Epoch 659/800\n",
      "4265/4265 [==============================] - 1s 126us/step - loss: 1140411934683.0254 - val_loss: 1217701659707.0493\n",
      "Epoch 660/800\n",
      "4265/4265 [==============================] - 1s 139us/step - loss: 1140221347897.1423 - val_loss: 1217360356053.8735\n",
      "Epoch 661/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1139955961025.2754 - val_loss: 1217018418148.6357\n",
      "Epoch 662/800\n",
      "4265/4265 [==============================] - 0s 103us/step - loss: 1139559310170.5754 - val_loss: 1216685634912.8552\n",
      "Epoch 663/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1139368633117.1113 - val_loss: 1216346156497.1926\n",
      "Epoch 664/800\n",
      "4265/4265 [==============================] - 0s 95us/step - loss: 1138951422343.8330 - val_loss: 1216009434741.3784\n",
      "Epoch 665/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1138699941166.2781 - val_loss: 1215676941578.4417\n",
      "Epoch 666/800\n",
      "4265/4265 [==============================] - 0s 101us/step - loss: 1138364421892.8621 - val_loss: 1215347642811.5894\n",
      "Epoch 667/800\n",
      "4265/4265 [==============================] - 1s 153us/step - loss: 1137999073275.1982 - val_loss: 1215003225322.7566\n",
      "Epoch 668/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1137711699958.8765 - val_loss: 1214686497853.9297\n",
      "Epoch 669/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1137401206064.1990 - val_loss: 1214332719778.0254\n",
      "Epoch 670/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1137067322670.7583 - val_loss: 1214006375200.7651\n",
      "Epoch 671/800\n",
      "4265/4265 [==============================] - 1s 129us/step - loss: 1136760862428.5261 - val_loss: 1213694990160.2925\n",
      "Epoch 672/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1136424264845.8955 - val_loss: 1213349408969.6316\n",
      "Epoch 673/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1136127316458.7517 - val_loss: 1213001010191.8425\n",
      "Epoch 674/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1135856921630.0117 - val_loss: 1212668333417.4966\n",
      "Epoch 675/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1135476765656.3845 - val_loss: 1212351956395.7468\n",
      "Epoch 676/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1135239130038.2913 - val_loss: 1212030521948.8945\n",
      "Epoch 677/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1134892235559.1953 - val_loss: 1211685156538.5090\n",
      "Epoch 678/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1134570320414.6121 - val_loss: 1211345455337.3164\n",
      "Epoch 679/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1134220564140.5076 - val_loss: 1211063140905.0464\n",
      "Epoch 680/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1134040182039.7092 - val_loss: 1210722085477.5359\n",
      "Epoch 681/800\n",
      "4265/4265 [==============================] - 1s 134us/step - loss: 1133697004581.2146 - val_loss: 1210375324085.8284\n",
      "Epoch 682/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1133373851866.4854 - val_loss: 1210104410244.5007\n",
      "Epoch 683/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1133016401773.5427 - val_loss: 1209737217751.3137\n",
      "Epoch 684/800\n",
      "4265/4265 [==============================] - 1s 131us/step - loss: 1132778202292.3105 - val_loss: 1209408012264.9563\n",
      "Epoch 685/800\n",
      "4265/4265 [==============================] - 1s 147us/step - loss: 1132357847335.3154 - val_loss: 1209077873697.1252\n",
      "Epoch 686/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1132179681580.5974 - val_loss: 1208748637608.8665\n",
      "Epoch 687/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1131781606051.6240 - val_loss: 1208428460144.3376\n",
      "Epoch 688/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1131530730140.9014 - val_loss: 1208110960807.0662\n",
      "Epoch 689/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1131242815941.2971 - val_loss: 1207794082448.7427\n",
      "Epoch 690/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1130757880135.0078 - val_loss: 1207487081960.2363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 691/800\n",
      "4265/4265 [==============================] - 0s 78us/step - loss: 1130575609352.7634 - val_loss: 1207146287939.3306\n",
      "Epoch 692/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1130304114879.1147 - val_loss: 1206864120986.1040\n",
      "Epoch 693/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1129900945551.0959 - val_loss: 1206515254423.2236\n",
      "Epoch 694/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1129623470629.3345 - val_loss: 1206196723009.1702\n",
      "Epoch 695/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1129371547387.2581 - val_loss: 1205915587764.0281\n",
      "Epoch 696/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1129039247106.9409 - val_loss: 1205587202814.1997\n",
      "Epoch 697/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1128772098338.5137 - val_loss: 1205263979907.4207\n",
      "Epoch 698/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1128489493907.1174 - val_loss: 1204956081548.0618\n",
      "Epoch 699/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1128214323977.9038 - val_loss: 1204627069535.7749\n",
      "Epoch 700/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1127860075330.0859 - val_loss: 1204327252382.7849\n",
      "Epoch 701/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1127651020253.5466 - val_loss: 1204006049754.5542\n",
      "Epoch 702/800\n",
      "4265/4265 [==============================] - 1s 120us/step - loss: 1127232921578.8718 - val_loss: 1203685310394.8691\n",
      "Epoch 703/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1127007955079.8931 - val_loss: 1203404115512.8889\n",
      "Epoch 704/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1126646155182.8484 - val_loss: 1203109302323.8481\n",
      "Epoch 705/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1126422348436.7383 - val_loss: 1202745465808.4727\n",
      "Epoch 706/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1126200094501.9949 - val_loss: 1202438013941.9185\n",
      "Epoch 707/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1125830698648.3396 - val_loss: 1202149737761.4854\n",
      "Epoch 708/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1125580073626.0200 - val_loss: 1201857319263.4148\n",
      "Epoch 709/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1125282695805.2090 - val_loss: 1201518705720.1687\n",
      "Epoch 710/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1124941906503.9080 - val_loss: 1201213581902.4922\n",
      "Epoch 711/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1124554772955.3855 - val_loss: 1200884002925.4570\n",
      "Epoch 712/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1124358382084.6819 - val_loss: 1200585919019.9268\n",
      "Epoch 713/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1124092176051.2300 - val_loss: 1200306950702.8074\n",
      "Epoch 714/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1123804225003.9521 - val_loss: 1199986949107.0381\n",
      "Epoch 715/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1123465109325.3701 - val_loss: 1199738657905.7778\n",
      "Epoch 716/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1123241057007.4934 - val_loss: 1199351475457.8003\n",
      "Epoch 717/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1122933017662.6646 - val_loss: 1199039894638.8972\n",
      "Epoch 718/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1122636446918.0774 - val_loss: 1198731670719.5500\n",
      "Epoch 719/800\n",
      "4265/4265 [==============================] - 1s 119us/step - loss: 1122284728228.7644 - val_loss: 1198427122899.7131\n",
      "Epoch 720/800\n",
      "4265/4265 [==============================] - 0s 109us/step - loss: 1122058435132.8638 - val_loss: 1198121214770.0479\n",
      "Epoch 721/800\n",
      "4265/4265 [==============================] - 1s 126us/step - loss: 1121760165461.5933 - val_loss: 1197807799496.1912\n",
      "Epoch 722/800\n",
      "4265/4265 [==============================] - 0s 104us/step - loss: 1121454313666.9563 - val_loss: 1197518908235.9719\n",
      "Epoch 723/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1121216973729.1628 - val_loss: 1197206366775.4487\n",
      "Epoch 724/800\n",
      "4265/4265 [==============================] - 0s 113us/step - loss: 1120952539549.4414 - val_loss: 1196913645300.1182\n",
      "Epoch 725/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1120669642672.2891 - val_loss: 1196608634856.9563\n",
      "Epoch 726/800\n",
      "4265/4265 [==============================] - 1s 148us/step - loss: 1120342480721.6919 - val_loss: 1196334877527.4937\n",
      "Epoch 727/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1120143387301.5447 - val_loss: 1196074865429.2434\n",
      "Epoch 728/800\n",
      "4265/4265 [==============================] - 0s 81us/step - loss: 1119795644496.1914 - val_loss: 1195724326361.8340\n",
      "Epoch 729/800\n",
      "4265/4265 [==============================] - 1s 122us/step - loss: 1119546201449.8213 - val_loss: 1195406449386.0366\n",
      "Epoch 730/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1119226166220.3796 - val_loss: 1195106680993.3052\n",
      "Epoch 731/800\n",
      "4265/4265 [==============================] - 1s 128us/step - loss: 1118943479196.7212 - val_loss: 1194810486109.9746\n",
      "Epoch 732/800\n",
      "4265/4265 [==============================] - 1s 140us/step - loss: 1118692588457.8062 - val_loss: 1194519755325.2095\n",
      "Epoch 733/800\n",
      "4265/4265 [==============================] - 1s 134us/step - loss: 1118390610040.5271 - val_loss: 1194223769124.7258\n",
      "Epoch 734/800\n",
      "4265/4265 [==============================] - 1s 148us/step - loss: 1118079509457.6619 - val_loss: 1193919016862.0647\n",
      "Epoch 735/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1117842092029.1189 - val_loss: 1193619337970.6780\n",
      "Epoch 736/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1117499699973.1021 - val_loss: 1193354424129.8904\n",
      "Epoch 737/800\n",
      "4265/4265 [==============================] - 1s 125us/step - loss: 1117337980635.0857 - val_loss: 1193034348886.7734\n",
      "Epoch 738/800\n",
      "4265/4265 [==============================] - 0s 110us/step - loss: 1116980843578.3428 - val_loss: 1192744211336.4614\n",
      "Epoch 739/800\n",
      "4265/4265 [==============================] - 0s 99us/step - loss: 1116751249043.0576 - val_loss: 1192473201818.1040\n",
      "Epoch 740/800\n",
      "4265/4265 [==============================] - 0s 116us/step - loss: 1116437409262.8335 - val_loss: 1192170475640.9790\n",
      "Epoch 741/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1116186419751.2554 - val_loss: 1191865762736.7876\n",
      "Epoch 742/800\n",
      "4265/4265 [==============================] - 0s 108us/step - loss: 1115966282083.8188 - val_loss: 1191574456415.0549\n",
      "Epoch 743/800\n",
      "4265/4265 [==============================] - 0s 105us/step - loss: 1115543385405.6440 - val_loss: 1191287251854.2222\n",
      "Epoch 744/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1115383562140.1208 - val_loss: 1190995934010.6892\n",
      "Epoch 745/800\n",
      "4265/4265 [==============================] - 0s 85us/step - loss: 1115078272495.3135 - val_loss: 1190716771482.1040\n",
      "Epoch 746/800\n",
      "4265/4265 [==============================] - 0s 89us/step - loss: 1114785489823.7224 - val_loss: 1190437674397.3445\n",
      "Epoch 747/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1114536565087.9775 - val_loss: 1190124892177.2827\n",
      "Epoch 748/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1114321805367.7017 - val_loss: 1189839910494.3347\n",
      "Epoch 749/800\n",
      "4265/4265 [==============================] - 0s 112us/step - loss: 1113974475877.0796 - val_loss: 1189590306913.9353\n",
      "Epoch 750/800\n",
      "4265/4265 [==============================] - 1s 123us/step - loss: 1113679127528.7107 - val_loss: 1189266121153.3501\n",
      "Epoch 751/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1113459661708.9951 - val_loss: 1188978108591.7075\n",
      "Epoch 752/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1113187038172.2261 - val_loss: 1188707836652.9170\n",
      "Epoch 753/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4265/4265 [==============================] - 0s 86us/step - loss: 1112834847800.4221 - val_loss: 1188418204587.0266\n",
      "Epoch 754/800\n",
      "4265/4265 [==============================] - 0s 73us/step - loss: 1112632713386.9468 - val_loss: 1188111075791.7524\n",
      "Epoch 755/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1112301591402.4216 - val_loss: 1187882161308.9846\n",
      "Epoch 756/800\n",
      "4265/4265 [==============================] - 0s 80us/step - loss: 1112103861699.6165 - val_loss: 1187544452180.9734\n",
      "Epoch 757/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1111806050304.0000 - val_loss: 1187272329472.3601\n",
      "Epoch 758/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1111526724224.5703 - val_loss: 1186994746444.3320\n",
      "Epoch 759/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1111303943443.3875 - val_loss: 1186690418829.1421\n",
      "Epoch 760/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1110991077675.6372 - val_loss: 1186410784091.0942\n",
      "Epoch 761/800\n",
      "4265/4265 [==============================] - 0s 98us/step - loss: 1110675630181.3196 - val_loss: 1186165349835.4319\n",
      "Epoch 762/800\n",
      "4265/4265 [==============================] - 1s 118us/step - loss: 1110452073639.8254 - val_loss: 1185954706715.7244\n",
      "Epoch 763/800\n",
      "4265/4265 [==============================] - 0s 100us/step - loss: 1110185911122.8923 - val_loss: 1185645301397.0632\n",
      "Epoch 764/800\n",
      "4265/4265 [==============================] - 0s 76us/step - loss: 1110054099019.3896 - val_loss: 1185293186331.7244\n",
      "Epoch 765/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1109650098575.0359 - val_loss: 1184997542466.9705\n",
      "Epoch 766/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1109413991067.4607 - val_loss: 1184712917867.6567\n",
      "Epoch 767/800\n",
      "4265/4265 [==============================] - 0s 79us/step - loss: 1109126356098.8511 - val_loss: 1184432393462.2786\n",
      "Epoch 768/800\n",
      "4265/4265 [==============================] - 0s 87us/step - loss: 1108892016695.7017 - val_loss: 1184153831357.7498\n",
      "Epoch 769/800\n",
      "4265/4265 [==============================] - 0s 83us/step - loss: 1108656104059.7683 - val_loss: 1183873692057.0239\n",
      "Epoch 770/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1108357237373.2090 - val_loss: 1183597911499.4319\n",
      "Epoch 771/800\n",
      "4265/4265 [==============================] - 0s 93us/step - loss: 1108107055590.9102 - val_loss: 1183313172142.9873\n",
      "Epoch 772/800\n",
      "4265/4265 [==============================] - 0s 106us/step - loss: 1107844691519.0247 - val_loss: 1183041276644.2756\n",
      "Epoch 773/800\n",
      "4265/4265 [==============================] - 0s 114us/step - loss: 1107528884060.7363 - val_loss: 1182765409350.5710\n",
      "Epoch 774/800\n",
      "4265/4265 [==============================] - 0s 102us/step - loss: 1107161233554.4573 - val_loss: 1182533472008.2812\n",
      "Epoch 775/800\n",
      "4265/4265 [==============================] - 0s 117us/step - loss: 1107070005998.0530 - val_loss: 1182235834226.8579\n",
      "Epoch 776/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1106758877702.6025 - val_loss: 1181958286501.6260\n",
      "Epoch 777/800\n",
      "4265/4265 [==============================] - 0s 77us/step - loss: 1106580619383.0867 - val_loss: 1181698625219.1504\n",
      "Epoch 778/800\n",
      "4265/4265 [==============================] - 0s 72us/step - loss: 1106204642716.2410 - val_loss: 1181420847057.9128\n",
      "Epoch 779/800\n",
      "4265/4265 [==============================] - 0s 75us/step - loss: 1106060620003.6089 - val_loss: 1181174263295.2798\n",
      "Epoch 780/800\n",
      "4265/4265 [==============================] - 0s 84us/step - loss: 1105791744250.1777 - val_loss: 1180873953562.2842\n",
      "Epoch 781/800\n",
      "4265/4265 [==============================] - 0s 111us/step - loss: 1105501459691.0518 - val_loss: 1180602011515.4993\n",
      "Epoch 782/800\n",
      "4265/4265 [==============================] - 1s 124us/step - loss: 1105228159881.1536 - val_loss: 1180334426644.8833\n",
      "Epoch 783/800\n",
      "4265/4265 [==============================] - 1s 146us/step - loss: 1104964659647.0544 - val_loss: 1180058395833.7891\n",
      "Epoch 784/800\n",
      "4265/4265 [==============================] - 1s 121us/step - loss: 1104794079811.1062 - val_loss: 1179778322898.6328\n",
      "Epoch 785/800\n",
      "4265/4265 [==============================] - 1s 130us/step - loss: 1104507853659.0557 - val_loss: 1179525133641.8115\n",
      "Epoch 786/800\n",
      "4265/4265 [==============================] - 1s 133us/step - loss: 1104250225803.9746 - val_loss: 1179242796258.1152\n",
      "Epoch 787/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1104010886488.0542 - val_loss: 1178970419148.1519\n",
      "Epoch 788/800\n",
      "4265/4265 [==============================] - 0s 94us/step - loss: 1103727326829.1226 - val_loss: 1178699520331.2517\n",
      "Epoch 789/800\n",
      "4265/4265 [==============================] - 0s 97us/step - loss: 1103403651808.6077 - val_loss: 1178433825128.0562\n",
      "Epoch 790/800\n",
      "4265/4265 [==============================] - 0s 92us/step - loss: 1103162545064.3657 - val_loss: 1178166534109.4346\n",
      "Epoch 791/800\n",
      "4265/4265 [==============================] - 0s 90us/step - loss: 1102899645118.5144 - val_loss: 1177902070171.9043\n",
      "Epoch 792/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1102710678631.4802 - val_loss: 1177773585046.5034\n",
      "Epoch 793/800\n",
      "4265/4265 [==============================] - 0s 107us/step - loss: 1102499612606.2144 - val_loss: 1177388741984.8552\n",
      "Epoch 794/800\n",
      "4265/4265 [==============================] - 0s 86us/step - loss: 1102184833486.9009 - val_loss: 1177138158959.2573\n",
      "Epoch 795/800\n",
      "4265/4265 [==============================] - 0s 96us/step - loss: 1101947876346.9580 - val_loss: 1176851942415.8425\n",
      "Epoch 796/800\n",
      "4265/4265 [==============================] - 0s 88us/step - loss: 1101720444192.3525 - val_loss: 1176603329249.3953\n",
      "Epoch 797/800\n",
      "4265/4265 [==============================] - 0s 82us/step - loss: 1101415177581.1826 - val_loss: 1176329492742.1208\n",
      "Epoch 798/800\n",
      "4265/4265 [==============================] - 1s 117us/step - loss: 1101234992856.6846 - val_loss: 1176097398426.8242\n",
      "Epoch 799/800\n",
      "4265/4265 [==============================] - 0s 115us/step - loss: 1101016287331.3989 - val_loss: 1175812092289.9802\n",
      "Epoch 800/800\n",
      "4265/4265 [==============================] - 1s 126us/step - loss: 1100678523392.3601 - val_loss: 1175540909852.4443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f7c2aef28>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_train44 = model44.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train44'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f7c266dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''# Create the plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(model_train11.history['val_loss'], 'r',\n",
    "         model_train22.history['val_loss'], 'y',\n",
    "         model_train33.history['val_loss'], 'g',\n",
    "         model_train44.history['val_loss'], 'b')\n",
    "plt.title(\"red:nadam , yellow:adam , green:adadelta , blue:adagrad\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.savefig(\"NeuralNetwith5HiddenLayer\")\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the less hidden layes didn't work out as per the graph above so using more hidden layers\n",
    "#as per above diagram using only adam and nadam\n",
    "#importing pandas to visualise the dataset in tabular format\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "\n",
    "import numpy as np\n",
    "#introducing stable analysis through random seed\n",
    "np.random.seed(42)\n",
    "#importing time function to create animated behaviour\n",
    "import time\n",
    "#importing regExp for data cleaning\n",
    "import re\n",
    "from numpy import NaN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=80)\n",
    "\n",
    "df = pd.read_csv(\"../data/scaled_train.csv\")\n",
    "test = pd.read_csv(\"../data/scaled_test.csv\")\n",
    "\n",
    "X_train = df.drop(['PRT_ID','SALES_PRICE'],axis=1)\n",
    "y_train = df['SALES_PRICE']\n",
    "\n",
    "X_test = test.drop(['PRT_ID'],axis=1)\n",
    "\n",
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "predictors = X_train.values\n",
    "target = y_train.values.reshape((-1,1)).astype('int32')\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "#model 1\n",
    "model11 = Sequential()\n",
    "model11.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model11.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model11.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "#model 2\n",
    "model22 = Sequential()\n",
    "model22.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model22.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compile model\n",
    "model11.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "print(\"model11 loss:\",model11.loss)\n",
    "\n",
    "# Compile model\n",
    "model22.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print\n",
    "\n",
    "model_train11 = model11.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train11\n",
    "\n",
    "model_train22 = model22.fit(predictors,target, epochs=800, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
