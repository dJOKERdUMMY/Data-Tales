{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#the less hidden layes didn't work out as per the graph above so using more hidden layers\n",
    "#as per above diagram using only adam and nadam\n",
    "#importing pandas to visualise the dataset in tabular format\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#introducing stable analysis through random seed\n",
    "np.random.seed(42)\n",
    "#importing time function to create animated behaviour\n",
    "import time\n",
    "#importing regExp for data cleaning\n",
    "import re\n",
    "from numpy import NaN\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/scaled_train.csv\")\n",
    "test = pd.read_csv(\"../data/scaled_test.csv\")\n",
    "\n",
    "X_train = df.drop(['PRT_ID','SALES_PRICE'],axis=1)\n",
    "y_train = df['SALES_PRICE']\n",
    "\n",
    "X_test = test.drop(['PRT_ID'],axis=1)\n",
    "\n",
    "# Import necessary modules\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Specify the model\n",
    "predictors = X_train.values\n",
    "target = y_train.values.reshape((-1,1)).astype('int32')\n",
    "\n",
    "n_cols = predictors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model 2\n",
    "model22 = Sequential()\n",
    "model22.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model22.add(Dense(70, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "model22.add(Dense(1, kernel_initializer='normal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model22 loss: mean_squared_error\n",
      "Train on 3554 samples, validate on 3555 samples\n",
      "Epoch 1/7000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 133448932687189.7188 - val_loss: 132144054774645.4531\n",
      "Epoch 2/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 129228891323770.5938 - val_loss: 116068129889850.0469\n",
      "Epoch 3/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 76083967819776.0000 - val_loss: 23342549174841.7539\n",
      "Epoch 4/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 10973416942899.1426 - val_loss: 7526727041895.0479\n",
      "Epoch 5/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 7306349954840.3467 - val_loss: 6700048515754.0908\n",
      "Epoch 6/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 6483929849565.5693 - val_loss: 6084097654447.5635\n",
      "Epoch 7/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 5897403332240.3506 - val_loss: 5625833950186.9727\n",
      "Epoch 8/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 5440414876865.0449 - val_loss: 5273792111511.4395\n",
      "Epoch 9/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 5086192126826.7500 - val_loss: 5009547460939.8281\n",
      "Epoch 10/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 4815655023875.8896 - val_loss: 4776615145125.1934\n",
      "Epoch 11/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 4577947583146.2822 - val_loss: 4597683524188.8945\n",
      "Epoch 12/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 4382385710489.7153 - val_loss: 4443553533837.9346\n",
      "Epoch 13/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 4229058068221.8394 - val_loss: 4310983786371.5645\n",
      "Epoch 14/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 4081988859222.8701 - val_loss: 4211000032805.3018\n",
      "Epoch 15/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3959454476946.0801 - val_loss: 4095359287102.4336\n",
      "Epoch 16/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3857500252670.5596 - val_loss: 4021510916919.2324\n",
      "Epoch 17/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 3759940228554.1201 - val_loss: 3921751683605.4595\n",
      "Epoch 18/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3675229381313.3325 - val_loss: 3848967023058.0566\n",
      "Epoch 19/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3593047613085.0288 - val_loss: 3775911065518.7715\n",
      "Epoch 20/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3521529467237.2764 - val_loss: 3708595589867.7646\n",
      "Epoch 21/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3452595535466.3184 - val_loss: 3647408055555.5283\n",
      "Epoch 22/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3388279941215.6582 - val_loss: 3580387783396.8516\n",
      "Epoch 23/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 3332884090204.6328 - val_loss: 3557828058552.9971\n",
      "Epoch 24/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3270366474786.2866 - val_loss: 3467673427596.9980\n",
      "Epoch 25/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3218344711248.0991 - val_loss: 3417439293699.2407\n",
      "Epoch 26/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3157991032332.9658 - val_loss: 3355979419497.0645\n",
      "Epoch 27/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3106419154607.4683 - val_loss: 3313047932288.5400\n",
      "Epoch 28/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 3042599109566.8838 - val_loss: 3257382251989.8013\n",
      "Epoch 29/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3000548137860.1060 - val_loss: 3197433828490.5498\n",
      "Epoch 30/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2952535213212.1641 - val_loss: 3148546862821.7158\n",
      "Epoch 31/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2902749916384.1621 - val_loss: 3099976052071.1919\n",
      "Epoch 32/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2856996986423.6084 - val_loss: 3039604270878.7490\n",
      "Epoch 33/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2804873915662.2622 - val_loss: 2994502394683.2651\n",
      "Epoch 34/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2758756068050.6201 - val_loss: 2956705181510.4990\n",
      "Epoch 35/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2716801747636.6553 - val_loss: 2902870535057.1025\n",
      "Epoch 36/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2675006574737.2153 - val_loss: 2883285300553.8115\n",
      "Epoch 37/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2630658728549.1323 - val_loss: 2822620111131.4365\n",
      "Epoch 38/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2592599561342.1992 - val_loss: 2774046578138.9863\n",
      "Epoch 39/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2554429114422.1675 - val_loss: 2722576727621.5630\n",
      "Epoch 40/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2530178106413.5239 - val_loss: 2698340426758.0488\n",
      "Epoch 41/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2489095034341.7808 - val_loss: 2709620944687.4556\n",
      "Epoch 42/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2464994911411.2144 - val_loss: 2634937730849.0532\n",
      "Epoch 43/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2414987218517.5732 - val_loss: 2579209460370.4707\n",
      "Epoch 44/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2386802088375.1040 - val_loss: 2587375212382.1187\n",
      "Epoch 45/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2370300125787.9121 - val_loss: 2516527715028.7212\n",
      "Epoch 46/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2329513654936.9951 - val_loss: 2475783984049.9399\n",
      "Epoch 47/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2293300283856.4590 - val_loss: 2446483788473.0688\n",
      "Epoch 48/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2270229976134.8789 - val_loss: 2417164226685.5874\n",
      "Epoch 49/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2246563281791.4956 - val_loss: 2405791366449.0396\n",
      "Epoch 50/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2218413888794.9399 - val_loss: 2362175343276.9712\n",
      "Epoch 51/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2199782578439.9233 - val_loss: 2359849252921.0327\n",
      "Epoch 52/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2178853006790.6628 - val_loss: 2316005109385.5415\n",
      "Epoch 53/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2156578604778.2463 - val_loss: 2297867941415.0303\n",
      "Epoch 54/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2138452020093.1907 - val_loss: 2338724666007.0796\n",
      "Epoch 55/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2143653844545.9810 - val_loss: 2245739171970.1963\n",
      "Epoch 56/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2099136300409.4451 - val_loss: 2218928155749.1040\n",
      "Epoch 57/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2068428069081.8235 - val_loss: 2214999766235.2021\n",
      "Epoch 58/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2062080048248.4368 - val_loss: 2192559073460.3162\n",
      "Epoch 59/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2039422907264.6482 - val_loss: 2158209747518.3618\n",
      "Epoch 60/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2018944453528.8511 - val_loss: 2138315713392.8416\n",
      "Epoch 61/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2010541943838.5413 - val_loss: 2113869562496.6121\n",
      "Epoch 62/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1983021995779.6016 - val_loss: 2092935445431.1245\n",
      "Epoch 63/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1974144156855.8245 - val_loss: 2072718284794.8152\n",
      "Epoch 64/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1949659305150.7393 - val_loss: 2085960959228.6155\n",
      "Epoch 65/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1933680466478.9646 - val_loss: 2037287624770.8264\n",
      "Epoch 66/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1917751030075.7864 - val_loss: 2020961163056.0315\n",
      "Epoch 67/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1901464506448.6753 - val_loss: 2011038438704.4636\n",
      "Epoch 68/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1881504037538.2148 - val_loss: 1985376659825.2737\n",
      "Epoch 69/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1864953587752.3376 - val_loss: 1980149455335.9482\n",
      "Epoch 70/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1864257861264.3513 - val_loss: 1949704555996.7146\n",
      "Epoch 71/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1860365656833.8728 - val_loss: 1927485155932.3184\n",
      "Epoch 72/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1812525161237.4653 - val_loss: 1906817265750.4136\n",
      "Epoch 73/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1795832036702.9375 - val_loss: 1891661840697.3931\n",
      "Epoch 74/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1780982591388.8848 - val_loss: 1862905060046.3843\n",
      "Epoch 75/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1756633025829.3125 - val_loss: 1841678070800.4185\n",
      "Epoch 76/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1739339739129.0850 - val_loss: 1839190669415.1201\n",
      "Epoch 77/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1721622825548.9297 - val_loss: 1802844461995.6028\n",
      "Epoch 78/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1698131500452.0876 - val_loss: 1791338359973.6260\n",
      "Epoch 79/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1701344567005.5688 - val_loss: 1796707810323.2991\n",
      "Epoch 80/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1661450709917.4609 - val_loss: 1743055953673.4336\n",
      "Epoch 81/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1643113024574.8115 - val_loss: 1743161362218.2705\n",
      "Epoch 82/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1628009880516.6460 - val_loss: 1707240638476.0979\n",
      "Epoch 83/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1611321407230.9915 - val_loss: 1697882195270.3550\n",
      "Epoch 84/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1586933824779.9573 - val_loss: 1665561857989.2388\n",
      "Epoch 85/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1569625949496.9050 - val_loss: 1644898497515.5488\n",
      "Epoch 86/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1547343084987.1377 - val_loss: 1635158236559.5183\n",
      "Epoch 87/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1537640033181.4609 - val_loss: 1609824824305.8857\n",
      "Epoch 88/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1514052111247.6309 - val_loss: 1592716679711.2529\n",
      "Epoch 89/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1492212987619.3315 - val_loss: 1566391497066.9368\n",
      "Epoch 90/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1475870157061.6184 - val_loss: 1548560790364.6785\n",
      "Epoch 91/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1465941139875.5115 - val_loss: 1533217212838.2739\n",
      "Epoch 92/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1439159951060.3489 - val_loss: 1516547994303.9819\n",
      "Epoch 93/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1424511806137.2651 - val_loss: 1495448292303.3203\n",
      "Epoch 94/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1410937040114.6021 - val_loss: 1476343820531.1101\n",
      "Epoch 95/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1392147761132.4075 - val_loss: 1476272655219.1460\n",
      "Epoch 96/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1379529895984.4053 - val_loss: 1445936638367.3608\n",
      "Epoch 97/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1358526377820.3445 - val_loss: 1427326715077.3108\n",
      "Epoch 98/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1344158235823.7571 - val_loss: 1408505550363.5083\n",
      "Epoch 99/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1340195166674.7644 - val_loss: 1394171657802.1716\n",
      "Epoch 100/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1305414977378.1069 - val_loss: 1385481346244.7346\n",
      "Epoch 101/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1295132962034.6021 - val_loss: 1360553052029.2275\n",
      "Epoch 102/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1279343346982.4648 - val_loss: 1347926095081.0283\n",
      "Epoch 103/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1262360501621.9875 - val_loss: 1333154231902.3347\n",
      "Epoch 104/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1258372112176.5493 - val_loss: 1334124587709.1016\n",
      "Epoch 105/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1240639725793.8909 - val_loss: 1305272017907.6140\n",
      "Epoch 106/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 1229557678808.3828 - val_loss: 1310389243255.3228\n",
      "Epoch 107/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 1211850435494.6809 - val_loss: 1282748110192.9856\n",
      "Epoch 108/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1199141530737.5217 - val_loss: 1272620626084.7617\n",
      "Epoch 109/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1195072418753.1885 - val_loss: 1258583004063.5049\n",
      "Epoch 110/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1178623166665.6882 - val_loss: 1246201287694.1143\n",
      "Epoch 111/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1176641336335.5588 - val_loss: 1252294463023.6714\n",
      "Epoch 112/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1158309497430.1497 - val_loss: 1225361931702.6926\n",
      "Epoch 113/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1152315850954.8406 - val_loss: 1217165885135.8245\n",
      "Epoch 114/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1143131350646.9961 - val_loss: 1207156832008.8574\n",
      "Epoch 115/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1132340899215.3428 - val_loss: 1199776691976.8574\n",
      "Epoch 116/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1122622089062.7170 - val_loss: 1189837114162.6240\n",
      "Epoch 117/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1115497318334.3071 - val_loss: 1188849318405.9050\n",
      "Epoch 118/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1110476151379.8447 - val_loss: 1173129285282.8894\n",
      "Epoch 119/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1099184580344.0765 - val_loss: 1166479947455.1179\n",
      "Epoch 120/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1092580810341.7085 - val_loss: 1161318704774.6611\n",
      "Epoch 121/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1088800706852.7361 - val_loss: 1164277901317.1848\n",
      "Epoch 122/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1081711623989.1593 - val_loss: 1147233727121.8948\n",
      "Epoch 123/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1075924921245.4609 - val_loss: 1147207165150.6587\n",
      "Epoch 124/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1066354105395.2864 - val_loss: 1165161131777.3682\n",
      "Epoch 125/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1068067367201.2786 - val_loss: 1127029877612.2329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1060200966774.4198 - val_loss: 1160627519644.6965\n",
      "Epoch 127/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1052352432892.1102 - val_loss: 1115929076871.0930\n",
      "Epoch 128/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1044014988113.3956 - val_loss: 1110455672414.9109\n",
      "Epoch 129/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1041345030838.9601 - val_loss: 1108904780875.1797\n",
      "Epoch 130/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1034432175302.8070 - val_loss: 1103130678668.6379\n",
      "Epoch 131/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1026343716355.7456 - val_loss: 1094832677147.4363\n",
      "Epoch 132/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1023933873290.8768 - val_loss: 1105598227457.4402\n",
      "Epoch 133/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1021592607214.4243 - val_loss: 1088513389418.5046\n",
      "Epoch 134/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1014117448391.0951 - val_loss: 1084689375358.7397\n",
      "Epoch 135/7000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 1014158115446.153 - 0s 72us/step - loss: 1015152907565.3799 - val_loss: 1082389166026.4236\n",
      "Epoch 136/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1010354010067.0522 - val_loss: 1086682192498.2098\n",
      "Epoch 137/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1009663320667.9122 - val_loss: 1069320507572.8922\n",
      "Epoch 138/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1003560513390.7844 - val_loss: 1064378536697.5909\n",
      "Epoch 139/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 994414789511.5632 - val_loss: 1062612195124.6403\n",
      "Epoch 140/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 994365221946.2015 - val_loss: 1057662774087.3632\n",
      "Epoch 141/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 990618732589.2358 - val_loss: 1053341601498.1941\n",
      "Epoch 142/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 982136653901.2178 - val_loss: 1055176677080.4658\n",
      "Epoch 143/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 981978756042.4086 - val_loss: 1049937215607.8268\n",
      "Epoch 144/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 982234531977.1481 - val_loss: 1060138973038.2493\n",
      "Epoch 145/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 974222329541.3663 - val_loss: 1040281978975.0548\n",
      "Epoch 146/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 977539888655.2706 - val_loss: 1044145979802.1761\n",
      "Epoch 147/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 971198708128.0540 - val_loss: 1060243211336.8754\n",
      "Epoch 148/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 973788673956.9521 - val_loss: 1030968372864.3241\n",
      "Epoch 149/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 966302309178.3455 - val_loss: 1030996504749.9791\n",
      "Epoch 150/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 961798493393.1796 - val_loss: 1031902034742.0804\n",
      "Epoch 151/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 958501905531.8943 - val_loss: 1022566522167.9528\n",
      "Epoch 152/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 952555548621.2898 - val_loss: 1033983616715.7919\n",
      "Epoch 153/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 954508038519.7164 - val_loss: 1016950886820.5457\n",
      "Epoch 154/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 947754221650.9803 - val_loss: 1013286519046.6970\n",
      "Epoch 155/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 957201196441.7152 - val_loss: 1011069057698.6014\n",
      "Epoch 156/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 947468854110.0731 - val_loss: 1009426929354.6396\n",
      "Epoch 157/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 939500521129.7063 - val_loss: 1007364162462.3528\n",
      "Epoch 158/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 942922442027.0748 - val_loss: 1009229186208.4410\n",
      "Epoch 159/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 934916655287.2482 - val_loss: 1029102071064.8439\n",
      "Epoch 160/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 938587493769.5802 - val_loss: 1020122794821.0588\n",
      "Epoch 161/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 930471020598.1677 - val_loss: 1012675135601.4897\n",
      "Epoch 162/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 930893342179.4756 - val_loss: 993379683224.0157\n",
      "Epoch 163/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 933289277690.0934 - val_loss: 990751674212.4557\n",
      "Epoch 164/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 925852551821.4700 - val_loss: 998111472238.7533\n",
      "Epoch 165/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 925875902885.8165 - val_loss: 1016008841680.9047\n",
      "Epoch 166/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 923534353199.9730 - val_loss: 989254836138.7386\n",
      "Epoch 167/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 920266988023.6444 - val_loss: 982077966951.8402\n",
      "Epoch 168/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 933769454586.8137 - val_loss: 983079523535.6804\n",
      "Epoch 169/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 915512523527.6354 - val_loss: 997327064056.7988\n",
      "Epoch 170/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 917451663492.5380 - val_loss: 978695976897.2062\n",
      "Epoch 171/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 910237200589.1458 - val_loss: 975771673453.9612\n",
      "Epoch 172/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 916374320157.3889 - val_loss: 988014436838.2200\n",
      "Epoch 173/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 911574998721.9089 - val_loss: 968634983863.2687\n",
      "Epoch 174/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 906736479622.1226 - val_loss: 975408775015.9122\n",
      "Epoch 175/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 904707218474.0664 - val_loss: 982441616685.0071\n",
      "Epoch 176/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 903131906103.3202 - val_loss: 962367780054.5935\n",
      "Epoch 177/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 903088334952.8779 - val_loss: 960037819774.8118\n",
      "Epoch 178/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 900285516669.1908 - val_loss: 965046524991.0819\n",
      "Epoch 179/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 894545158923.0928 - val_loss: 957895890021.6799\n",
      "Epoch 180/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 893516740129.7108 - val_loss: 954135911325.7766\n",
      "Epoch 181/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 893034886348.5695 - val_loss: 954618597976.2858\n",
      "Epoch 182/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 891043488098.9713 - val_loss: 951403987252.7843\n",
      "Epoch 183/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 900276097242.9758 - val_loss: 949910205737.5505\n",
      "Epoch 184/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 886432472161.3867 - val_loss: 962816850755.9066\n",
      "Epoch 185/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 884476184796.1284 - val_loss: 955248989571.4205\n",
      "Epoch 186/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 886757664374.9961 - val_loss: 946268359046.0129\n",
      "Epoch 187/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 883528564182.7980 - val_loss: 941204044478.2538\n",
      "Epoch 188/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 885872529820.5964 - val_loss: 940109502136.2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 881136086816.4142 - val_loss: 939313416065.8363\n",
      "Epoch 190/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 876513840089.3911 - val_loss: 939774399571.2450\n",
      "Epoch 191/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 877200483627.0748 - val_loss: 943330657703.1381\n",
      "Epoch 192/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 879636464565.6636 - val_loss: 940239788054.4675\n",
      "Epoch 193/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 873338375120.1711 - val_loss: 935598695572.9193\n",
      "Epoch 194/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 874962596791.9685 - val_loss: 931317820563.1910\n",
      "Epoch 195/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 869229131463.6714 - val_loss: 936798979264.7021\n",
      "Epoch 196/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 873648276737.5846 - val_loss: 928645532406.7106\n",
      "Epoch 197/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 868449067335.3112 - val_loss: 943564280048.8057\n",
      "Epoch 198/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 866890008289.0265 - val_loss: 924167608305.5978\n",
      "Epoch 199/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 866275211606.2937 - val_loss: 923801832551.4082\n",
      "Epoch 200/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 871613991150.5684 - val_loss: 942484804608.2881\n",
      "Epoch 201/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 864116392166.5009 - val_loss: 920924298012.7325\n",
      "Epoch 202/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 864421470433.8909 - val_loss: 923627041992.7673\n",
      "Epoch 203/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 859193837188.8260 - val_loss: 919039500774.2200\n",
      "Epoch 204/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 858063220102.1226 - val_loss: 920062826979.3395\n",
      "Epoch 205/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 856462864711.3112 - val_loss: 925729827120.7516\n",
      "Epoch 206/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 859164451290.8317 - val_loss: 912886678670.0062\n",
      "Epoch 207/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 856676327004.4885 - val_loss: 919933560325.6168\n",
      "Epoch 208/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 857711093164.1554 - val_loss: 912109655722.3787\n",
      "Epoch 209/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 853030809851.2460 - val_loss: 908855059245.4391\n",
      "Epoch 210/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 854792793966.7844 - val_loss: 912574125171.5061\n",
      "Epoch 211/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 861236643612.9567 - val_loss: 911970967720.2183\n",
      "Epoch 212/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 849913170555.0298 - val_loss: 906090639086.9333\n",
      "Epoch 213/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 846984539443.1425 - val_loss: 920701708113.4447\n",
      "Epoch 214/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 850757646779.7141 - val_loss: 903458380872.2993\n",
      "Epoch 215/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 846669597307.0298 - val_loss: 903516895136.9452\n",
      "Epoch 216/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 844336401664.4323 - val_loss: 910176522522.2841\n",
      "Epoch 217/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 848406384822.0956 - val_loss: 911920054949.4818\n",
      "Epoch 218/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 847266729290.1925 - val_loss: 900497471637.2073\n",
      "Epoch 219/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 845059624708.7540 - val_loss: 897256309524.0912\n",
      "Epoch 220/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 843136929273.3730 - val_loss: 902565724989.5696\n",
      "Epoch 221/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 837474268208.9814 - val_loss: 895600584626.2279\n",
      "Epoch 222/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 842892408906.9128 - val_loss: 893953946280.9384\n",
      "Epoch 223/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 838953690918.1768 - val_loss: 894369548954.5361\n",
      "Epoch 224/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 845021262029.1458 - val_loss: 891619577815.6737\n",
      "Epoch 225/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 842152927124.8170 - val_loss: 901855879963.0043\n",
      "Epoch 226/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 832991054809.3911 - val_loss: 894009022292.9012\n",
      "Epoch 227/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 835045881359.2706 - val_loss: 925697290386.3269\n",
      "Epoch 228/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 838288019674.9758 - val_loss: 893107512008.0472\n",
      "Epoch 229/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 832176076787.3224 - val_loss: 888721292367.2124\n",
      "Epoch 230/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 831827789813.6276 - val_loss: 889837683987.6591\n",
      "Epoch 231/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 829902474459.5520 - val_loss: 886604468764.3724\n",
      "Epoch 232/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 835373524700.4165 - val_loss: 893034015494.8411\n",
      "Epoch 233/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 833491596391.1492 - val_loss: 896560207295.3339\n",
      "Epoch 234/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 832513168547.0793 - val_loss: 884565004298.3696\n",
      "Epoch 235/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 832496127343.0725 - val_loss: 885445960484.2217\n",
      "Epoch 236/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 827930465129.5981 - val_loss: 906298741566.7218\n",
      "Epoch 237/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 830612050323.3766 - val_loss: 892957037720.3759\n",
      "Epoch 238/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 827404609945.1390 - val_loss: 879408389731.5195\n",
      "Epoch 239/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 825945465694.0731 - val_loss: 898146828914.7859\n",
      "Epoch 240/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 823341668081.1615 - val_loss: 878351779073.2242\n",
      "Epoch 241/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 826356073517.5239 - val_loss: 881926323260.7775\n",
      "Epoch 242/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 829470830506.7147 - val_loss: 877075797927.8583\n",
      "Epoch 243/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 821470415944.0315 - val_loss: 874692299031.4037\n",
      "Epoch 244/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 819725786587.4081 - val_loss: 875663389818.4191\n",
      "Epoch 245/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 819524535452.1644 - val_loss: 872924739816.1643\n",
      "Epoch 246/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 819955384679.5813 - val_loss: 874120676673.7462\n",
      "Epoch 247/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 820214134010.6697 - val_loss: 873277711495.0931\n",
      "Epoch 248/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 818315810563.6016 - val_loss: 872208988585.7305\n",
      "Epoch 249/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 819242722160.5132 - val_loss: 887609465260.0349\n",
      "Epoch 250/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 821706934381.4879 - val_loss: 871962229986.6914\n",
      "Epoch 251/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 816238296940.4795 - val_loss: 874003530714.2661\n",
      "Epoch 252/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 817271214972.0382 - val_loss: 878229990510.3213\n",
      "Epoch 253/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 817446623339.1830 - val_loss: 880936797574.8771\n",
      "Epoch 254/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 817978349951.7839 - val_loss: 867719578742.9626\n",
      "Epoch 255/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 812001595107.6195 - val_loss: 879628101282.3134\n",
      "Epoch 256/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 813240825977.0129 - val_loss: 881817647262.4248\n",
      "Epoch 257/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 818579416252.4344 - val_loss: 865990502617.1859\n",
      "Epoch 258/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 815688271769.4271 - val_loss: 866208015283.0920\n",
      "Epoch 259/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 813188522975.1537 - val_loss: 866088642823.2732\n",
      "Epoch 260/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 815395347613.3168 - val_loss: 870406315679.4329\n",
      "Epoch 261/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 811342611031.3021 - val_loss: 864017124248.0157\n",
      "Epoch 262/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 812900531348.0967 - val_loss: 865023953622.4495\n",
      "Epoch 263/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 810758550769.4497 - val_loss: 865395387751.1921\n",
      "Epoch 264/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 811215878864.3151 - val_loss: 861929148311.1516\n",
      "Epoch 265/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 811930226648.8148 - val_loss: 863546904037.9319\n",
      "Epoch 266/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 806401598343.5632 - val_loss: 860758714635.8818\n",
      "Epoch 267/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 808313287506.5481 - val_loss: 862323517883.0132\n",
      "Epoch 268/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 808854578004.8530 - val_loss: 859310746278.0580\n",
      "Epoch 269/7000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 806492100687.5228 - val_loss: 859077817867.3778\n",
      "Epoch 270/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 813826323236.4480 - val_loss: 869548655200.3511\n",
      "Epoch 271/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 807032319928.5447 - val_loss: 858467495616.8462\n",
      "Epoch 272/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 807754447927.8965 - val_loss: 857827390793.8115\n",
      "Epoch 273/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 802203748673.5487 - val_loss: 856112704511.7119\n",
      "Epoch 274/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 804371072285.2448 - val_loss: 871853935406.0151\n",
      "Epoch 275/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 806290655028.0067 - val_loss: 857855195478.7736\n",
      "Epoch 276/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 803553720392.6077 - val_loss: 857963240718.4742\n",
      "Epoch 277/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 803980256149.3934 - val_loss: 862809964594.6959\n",
      "Epoch 278/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 803070726740.9972 - val_loss: 853344452488.1732\n",
      "Epoch 279/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 800846942722.0168 - val_loss: 854016404120.2318\n",
      "Epoch 280/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 801171220105.4362 - val_loss: 864680304001.6923\n",
      "Epoch 281/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 802140342040.3466 - val_loss: 853340533472.2430\n",
      "Epoch 282/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 801553235387.1379 - val_loss: 851415294788.4827\n",
      "Epoch 283/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 800143790916.7181 - val_loss: 852161464189.2275\n",
      "Epoch 284/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 801996119635.2684 - val_loss: 849606675425.7552\n",
      "Epoch 285/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 800663766006.2037 - val_loss: 848828235130.7792\n",
      "Epoch 286/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 797917993961.5261 - val_loss: 851098379606.4855\n",
      "Epoch 287/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 797493709803.8312 - val_loss: 849362839953.5348\n",
      "Epoch 288/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 797151200559.6849 - val_loss: 847885912573.8397\n",
      "Epoch 289/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 801621327825.8999 - val_loss: 848283368780.6920\n",
      "Epoch 290/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 800846907649.0084 - val_loss: 850677285089.8273\n",
      "Epoch 291/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 794341065413.9426 - val_loss: 846850922324.9012\n",
      "Epoch 292/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 802111657132.2993 - val_loss: 846631721595.4272\n",
      "Epoch 293/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 795025833827.2594 - val_loss: 845569189883.9674\n",
      "Epoch 294/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 794911665791.0636 - val_loss: 845962288112.7336\n",
      "Epoch 295/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 796036753872.4592 - val_loss: 867453900840.9023\n",
      "Epoch 296/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 796754188671.7839 - val_loss: 854109798773.3063\n",
      "Epoch 297/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 801937235539.2684 - val_loss: 845628317255.8672\n",
      "Epoch 298/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 793722236884.2048 - val_loss: 843845738037.4324\n",
      "Epoch 299/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 803660980615.8513 - val_loss: 845295558501.6079\n",
      "Epoch 300/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 794816894391.6804 - val_loss: 844046543963.8864\n",
      "Epoch 301/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 792242104553.9584 - val_loss: 843750749587.5510\n",
      "Epoch 302/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 798368072638.8835 - val_loss: 842846766991.6625\n",
      "Epoch 303/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 792216463206.7169 - val_loss: 840955428532.1721\n",
      "Epoch 304/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 790174714099.1785 - val_loss: 840479834600.5243\n",
      "Epoch 305/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 790515100188.5244 - val_loss: 851300242690.3763\n",
      "Epoch 306/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 788920284936.2117 - val_loss: 843686631156.1182\n",
      "Epoch 307/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 791141983106.9534 - val_loss: 839920992592.4365\n",
      "Epoch 308/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 791755145003.9392 - val_loss: 856017974085.6349\n",
      "Epoch 309/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 794468007650.1790 - val_loss: 843125891756.3949\n",
      "Epoch 310/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 797887710207.4238 - val_loss: 839234058693.3828\n",
      "Epoch 311/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 798714667796.3129 - val_loss: 839351145430.8096\n",
      "Epoch 312/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 792071029593.4631 - val_loss: 839939210687.9100\n",
      "Epoch 313/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 788243115020.1012 - val_loss: 853314341940.7123\n",
      "Epoch 314/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 798605938814.7754 - val_loss: 839240661993.8206\n",
      "Epoch 315/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 72us/step - loss: 786611298290.1700 - val_loss: 866156579754.4507\n",
      "Epoch 316/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 793804316925.5509 - val_loss: 836435186868.0281\n",
      "Epoch 317/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 789068122866.8904 - val_loss: 837589091423.9189\n",
      "Epoch 318/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 794519982271.3157 - val_loss: 839916632383.7300\n",
      "Epoch 319/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 787523610249.4362 - val_loss: 843160485141.3873\n",
      "Epoch 320/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 788099224739.0793 - val_loss: 838713898558.0737\n",
      "Epoch 321/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 786279070102.8340 - val_loss: 834449244703.8290\n",
      "Epoch 322/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 788307075943.2932 - val_loss: 834526830718.7397\n",
      "Epoch 323/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 785789887639.5543 - val_loss: 834249070070.6385\n",
      "Epoch 324/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 789004082467.0073 - val_loss: 846709732515.8976\n",
      "Epoch 325/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 789744452764.1644 - val_loss: 833842732915.4341\n",
      "Epoch 326/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 790584634767.3427 - val_loss: 835177061443.6906\n",
      "Epoch 327/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 788476701071.9191 - val_loss: 835201794260.0011\n",
      "Epoch 328/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 782803617394.3861 - val_loss: 848865150683.9224\n",
      "Epoch 329/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 786122922266.3635 - val_loss: 832514122158.9153\n",
      "Epoch 330/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 788194461823.9280 - val_loss: 832490344721.0667\n",
      "Epoch 331/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 787073940978.4581 - val_loss: 839552831512.1958\n",
      "Epoch 332/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 790773763466.7327 - val_loss: 858177921888.1350\n",
      "Epoch 333/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 782711957350.7169 - val_loss: 830886797650.7410\n",
      "Epoch 334/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 781341078105.6072 - val_loss: 836642187144.7494\n",
      "Epoch 335/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 780890526477.3979 - val_loss: 830748020469.8464\n",
      "Epoch 336/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 782332264632.4008 - val_loss: 831400413348.1857\n",
      "Epoch 337/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 784984985404.0742 - val_loss: 830684662698.4507\n",
      "Epoch 338/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 784842278901.6276 - val_loss: 852472443763.7220\n",
      "Epoch 339/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 788093159837.7490 - val_loss: 830168018647.8898\n",
      "Epoch 340/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 787549981758.8114 - val_loss: 859755365545.6586\n",
      "Epoch 341/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 782622207403.0028 - val_loss: 827708890875.6073\n",
      "Epoch 342/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 784444370909.4249 - val_loss: 829819400596.4152\n",
      "Epoch 343/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 779548977948.3805 - val_loss: 835036276621.3580\n",
      "Epoch 344/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 780735668634.2915 - val_loss: 861363312929.7733\n",
      "Epoch 345/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 781525769691.4081 - val_loss: 830955658943.6940\n",
      "Epoch 346/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 782201954612.8711 - val_loss: 828028054873.3660\n",
      "Epoch 347/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 779298954853.7085 - val_loss: 835836631518.7308\n",
      "Epoch 348/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 778698451150.2982 - val_loss: 839345915565.8352\n",
      "Epoch 349/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 777669246422.7980 - val_loss: 826086441729.6562\n",
      "Epoch 350/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 780469904553.9944 - val_loss: 827304884986.1671\n",
      "Epoch 351/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 779752252346.2734 - val_loss: 830493578304.8102\n",
      "Epoch 352/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 779129397132.7496 - val_loss: 827502679187.7671\n",
      "Epoch 353/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 775608604484.1418 - val_loss: 827006300416.9362\n",
      "Epoch 354/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 780043156747.9573 - val_loss: 826317515904.7561\n",
      "Epoch 355/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 776253114366.8474 - val_loss: 836261230413.9882\n",
      "Epoch 356/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 777253466713.6072 - val_loss: 824242252301.9702\n",
      "Epoch 357/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 780350296061.6951 - val_loss: 830247035458.3944\n",
      "Epoch 358/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 780198055597.1638 - val_loss: 824611703178.6217\n",
      "Epoch 359/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 777888465915.6781 - val_loss: 824026484856.9789\n",
      "Epoch 360/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 777218925074.1520 - val_loss: 827782582503.5881\n",
      "Epoch 361/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 777073277030.5729 - val_loss: 823528978323.1190\n",
      "Epoch 362/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 774521664513.7288 - val_loss: 826512805737.0645\n",
      "Epoch 363/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 775696250291.0703 - val_loss: 831078024997.6619\n",
      "Epoch 364/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 783112928604.6326 - val_loss: 822706853113.1589\n",
      "Epoch 365/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 775484177786.5975 - val_loss: 823167482051.2946\n",
      "Epoch 366/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 776436721012.8351 - val_loss: 822641731298.8354\n",
      "Epoch 367/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 781825520162.2870 - val_loss: 825114841310.3707\n",
      "Epoch 368/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 774436023842.8632 - val_loss: 832369842415.0774\n",
      "Epoch 369/7000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 777663639462.1046 - val_loss: 820664990047.9910\n",
      "Epoch 370/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 772246566839.3923 - val_loss: 824033526686.3528\n",
      "Epoch 371/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 777049889400.7249 - val_loss: 821930955135.6759\n",
      "Epoch 372/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 772565349313.1886 - val_loss: 830496297265.3278\n",
      "Epoch 373/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 776073978376.9320 - val_loss: 822740075532.9620\n",
      "Epoch 374/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 772021972680.8239 - val_loss: 818961585110.8096\n",
      "Epoch 375/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 773337006845.2628 - val_loss: 825314724488.1013\n",
      "Epoch 376/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 775726836130.3590 - val_loss: 842391445732.1316\n",
      "Epoch 377/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 779017686392.2926 - val_loss: 823907346722.9254\n",
      "Epoch 378/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 776465073059.2234 - val_loss: 828997798715.8413\n",
      "Epoch 379/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 772238854649.9493 - val_loss: 834808704594.2368\n",
      "Epoch 380/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 772844341606.4288 - val_loss: 830534216874.2346\n",
      "Epoch 381/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 773064480190.5953 - val_loss: 830293549943.1786\n",
      "Epoch 382/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 774199025972.8711 - val_loss: 821103989635.8527\n",
      "Epoch 383/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 774426566780.4705 - val_loss: 819017654661.4369\n",
      "Epoch 384/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 780119197759.3878 - val_loss: 834279507155.1370\n",
      "Epoch 385/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 771745119149.5959 - val_loss: 834452403465.2894\n",
      "Epoch 386/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 772143216235.4711 - val_loss: 824668833684.2711\n",
      "Epoch 387/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 772364245597.0647 - val_loss: 821143489344.4501\n",
      "Epoch 388/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 770371797074.9803 - val_loss: 817108543622.2290\n",
      "Epoch 389/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 770884034490.8497 - val_loss: 818749554287.0414\n",
      "Epoch 390/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 778698526376.5537 - val_loss: 837164843639.3947\n",
      "Epoch 391/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 770924540717.6680 - val_loss: 816841840095.5950\n",
      "Epoch 392/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 766277437171.4666 - val_loss: 829421103805.1016\n",
      "Epoch 393/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 771290637426.6742 - val_loss: 814338418218.4867\n",
      "Epoch 394/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 772306669206.1136 - val_loss: 828519981371.6974\n",
      "Epoch 395/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 772821301037.0917 - val_loss: 816676715414.8635\n",
      "Epoch 396/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 775245907478.1858 - val_loss: 819491761976.6729\n",
      "Epoch 397/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 769972331772.9747 - val_loss: 834405502843.4993\n",
      "Epoch 398/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 769876348501.5734 - val_loss: 817305775947.1078\n",
      "Epoch 399/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 769991242209.7468 - val_loss: 814526705091.3666\n",
      "Epoch 400/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 772477135459.9797 - val_loss: 820268598427.8324\n",
      "Epoch 401/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 769382439802.8859 - val_loss: 814996473683.7491\n",
      "Epoch 402/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 771740363935.6218 - val_loss: 814015958967.4126\n",
      "Epoch 403/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 769774775400.3016 - val_loss: 861314604251.7783\n",
      "Epoch 404/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 773315937924.8260 - val_loss: 814790632868.8337\n",
      "Epoch 405/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 770390362276.8081 - val_loss: 814755135077.8239\n",
      "Epoch 406/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 772584240212.7091 - val_loss: 812018018286.1412\n",
      "Epoch 407/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 768549701818.7057 - val_loss: 812109410081.6293\n",
      "Epoch 408/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 767578162507.3450 - val_loss: 811432031663.2034\n",
      "Epoch 409/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 766604004331.8312 - val_loss: 814590910240.4771\n",
      "Epoch 410/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 764697359859.6105 - val_loss: 814631524512.7291\n",
      "Epoch 411/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 769892267138.8092 - val_loss: 821836358085.9590\n",
      "Epoch 412/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 767458103740.8666 - val_loss: 837373840174.0151\n",
      "Epoch 413/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 767720179978.2285 - val_loss: 814410248043.3688\n",
      "Epoch 414/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 764949319594.7147 - val_loss: 811840292627.2271\n",
      "Epoch 415/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 766089189556.9432 - val_loss: 810109358578.8939\n",
      "Epoch 416/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 764930398015.5317 - val_loss: 812768452375.2596\n",
      "Epoch 417/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 765749486107.3719 - val_loss: 811445927064.9519\n",
      "Epoch 418/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 770026131407.5947 - val_loss: 811802280659.5691\n",
      "Epoch 419/7000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 768622084363.3811 - val_loss: 818058090218.9008\n",
      "Epoch 420/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 764469231957.1414 - val_loss: 811395726249.2985\n",
      "Epoch 421/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 772518714354.1700 - val_loss: 816523407879.6332\n",
      "Epoch 422/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 764701895991.7524 - val_loss: 811901019888.6616\n",
      "Epoch 423/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 764943278960.5132 - val_loss: 808672263712.6931\n",
      "Epoch 424/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 768764062944.1621 - val_loss: 815332638042.5182\n",
      "Epoch 425/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 769015874273.6027 - val_loss: 810219137379.4475\n",
      "Epoch 426/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 764726377297.3956 - val_loss: 809751542941.5607\n",
      "Epoch 427/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 765138598470.0146 - val_loss: 813350712866.4214\n",
      "Epoch 428/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 764150185080.4368 - val_loss: 808844474825.9916\n",
      "Epoch 429/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 764048028692.1688 - val_loss: 815103589860.7798\n",
      "Epoch 430/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 763091081756.5244 - val_loss: 808311494945.7733\n",
      "Epoch 431/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 763396205275.8401 - val_loss: 816128407927.0345\n",
      "Epoch 432/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 764681610152.9860 - val_loss: 812670040420.5997\n",
      "Epoch 433/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 767162194248.4637 - val_loss: 836007371657.6135\n",
      "Epoch 434/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 771370891366.5729 - val_loss: 812638597427.6321\n",
      "Epoch 435/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 763000909273.1029 - val_loss: 815824682975.7390\n",
      "Epoch 436/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 768031781433.3370 - val_loss: 812612763560.4343\n",
      "Epoch 437/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 762694735195.4800 - val_loss: 812433788947.5870\n",
      "Epoch 438/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 761755929072.7294 - val_loss: 806605419937.3772\n",
      "Epoch 439/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 763830388356.8260 - val_loss: 812363434917.8419\n",
      "Epoch 440/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 765314684159.2797 - val_loss: 807221645045.8464\n",
      "Epoch 441/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 92us/step - loss: 765469408533.1772 - val_loss: 808961578514.2909\n",
      "Epoch 442/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 771121510991.2346 - val_loss: 814764456905.5594\n",
      "Epoch 443/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 765948528399.1267 - val_loss: 824015069287.6962\n",
      "Epoch 444/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 762179050547.2864 - val_loss: 810552608042.7026\n",
      "Epoch 445/7000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 763260570207.3696 - val_loss: 806454782749.8846\n",
      "Epoch 446/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 759425202084.9521 - val_loss: 816918391360.6661\n",
      "Epoch 447/7000\n",
      "3554/3554 [==============================] - 0s 140us/step - loss: 760382663756.6416 - val_loss: 809519594344.7764\n",
      "Epoch 448/7000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 765763006825.8864 - val_loss: 805725254672.4186\n",
      "Epoch 449/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 765024470753.2426 - val_loss: 805044484899.6456\n",
      "Epoch 450/7000\n",
      "3554/3554 [==============================] - 1s 155us/step - loss: 765318556831.6218 - val_loss: 811805596845.6912\n",
      "Epoch 451/7000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 762357973914.0034 - val_loss: 820947097060.2036\n",
      "Epoch 452/7000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 762734483470.9825 - val_loss: 808393027002.1490\n",
      "Epoch 453/7000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 759966280393.4001 - val_loss: 823735270239.5590\n",
      "Epoch 454/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 760212521586.9623 - val_loss: 807687690704.9047\n",
      "Epoch 455/7000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 758627488727.0861 - val_loss: 826761276991.8020\n",
      "Epoch 456/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 761851550245.7445 - val_loss: 805881664567.5927\n",
      "Epoch 457/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 758520665029.7985 - val_loss: 872379940624.3466\n",
      "Epoch 458/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 766821618242.5571 - val_loss: 834823416218.7522\n",
      "Epoch 459/7000\n",
      "3554/3554 [==============================] - 1s 155us/step - loss: 758609964177.7917 - val_loss: 807074814568.9924\n",
      "Epoch 460/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 759750774389.8435 - val_loss: 812022339836.9036\n",
      "Epoch 461/7000\n",
      "3554/3554 [==============================] - 1s 143us/step - loss: 762122301866.4266 - val_loss: 803277602779.1302\n",
      "Epoch 462/7000\n",
      "3554/3554 [==============================] - 1s 184us/step - loss: 765979080603.7322 - val_loss: 816659209237.6034\n",
      "Epoch 463/7000\n",
      "3554/3554 [==============================] - 1s 155us/step - loss: 758804077840.5673 - val_loss: 802926116840.9564\n",
      "Epoch 464/7000\n",
      "3554/3554 [==============================] - 1s 148us/step - loss: 760144575193.5352 - val_loss: 803741867223.1697\n",
      "Epoch 465/7000\n",
      "3554/3554 [==============================] - 0s 132us/step - loss: 760696343061.6094 - val_loss: 803766591928.1328\n",
      "Epoch 466/7000\n",
      "3554/3554 [==============================] - 1s 156us/step - loss: 758451727165.8030 - val_loss: 807915639762.2008\n",
      "Epoch 467/7000\n",
      "3554/3554 [==============================] - 0s 133us/step - loss: 760730530711.6984 - val_loss: 808957653654.5035\n",
      "Epoch 468/7000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 760060570802.0619 - val_loss: 823404196811.2877\n",
      "Epoch 469/7000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 760335629155.8356 - val_loss: 806286149782.3595\n",
      "Epoch 470/7000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 760797973465.3911 - val_loss: 804703556239.5905\n",
      "Epoch 471/7000\n",
      "3554/3554 [==============================] - 0s 131us/step - loss: 758497693592.2747 - val_loss: 801567820331.9269\n",
      "Epoch 472/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 759187233935.4868 - val_loss: 844536262037.2793\n",
      "Epoch 473/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 758623300527.9010 - val_loss: 802105300599.6827\n",
      "Epoch 474/7000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 769572669641.6882 - val_loss: 806566328653.5562\n",
      "Epoch 475/7000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 760536906088.7339 - val_loss: 801713750569.0464\n",
      "Epoch 476/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 758301332071.4373 - val_loss: 800466657849.1769\n",
      "Epoch 477/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 758104557529.9674 - val_loss: 820113058096.7516\n",
      "Epoch 478/7000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 763991128249.5531 - val_loss: 806943727240.3893\n",
      "Epoch 479/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 760738920209.4316 - val_loss: 802164942194.1378\n",
      "Epoch 480/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 754451147105.8187 - val_loss: 807444487327.8650\n",
      "Epoch 481/7000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 763390310968.1846 - val_loss: 807944215698.0388\n",
      "Epoch 482/7000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 754398017159.1312 - val_loss: 800893837277.4346\n",
      "Epoch 483/7000\n",
      "3554/3554 [==============================] - 0s 133us/step - loss: 755837741529.6793 - val_loss: 817479214417.8768\n",
      "Epoch 484/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 757454037399.9865 - val_loss: 801068340657.5077\n",
      "Epoch 485/7000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 757066460467.1425 - val_loss: 798990724929.8903\n",
      "Epoch 486/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 755456131708.1824 - val_loss: 801140212836.5277\n",
      "Epoch 487/7000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 755355056645.4744 - val_loss: 829722680198.7330\n",
      "Epoch 488/7000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 768456271465.1660 - val_loss: 802470712191.2439\n",
      "Epoch 489/7000\n",
      "3554/3554 [==============================] - 1s 142us/step - loss: 760350030664.1757 - val_loss: 799016473019.5894\n",
      "Epoch 490/7000\n",
      "3554/3554 [==============================] - 0s 132us/step - loss: 756504606914.7732 - val_loss: 800223389544.4883\n",
      "Epoch 491/7000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 755487109251.3855 - val_loss: 799796937971.3981\n",
      "Epoch 492/7000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 753331961330.4581 - val_loss: 820674628716.0168\n",
      "Epoch 493/7000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 754596703705.1029 - val_loss: 799558498379.4678\n",
      "Epoch 494/7000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 757755805364.0787 - val_loss: 812886266626.8085\n",
      "Epoch 495/7000\n",
      "3554/3554 [==============================] - 1s 151us/step - loss: 758183457821.3889 - val_loss: 804495514806.9086\n",
      "Epoch 496/7000\n",
      "3554/3554 [==============================] - 1s 163us/step - loss: 754815421382.9510 - val_loss: 798091156796.5615\n",
      "Epoch 497/7000\n",
      "3554/3554 [==============================] - 1s 187us/step - loss: 757154779539.3766 - val_loss: 800187767954.0388\n",
      "Epoch 498/7000\n",
      "3554/3554 [==============================] - 1s 172us/step - loss: 755414392320.2881 - val_loss: 810933405837.1421\n",
      "Epoch 499/7000\n",
      "3554/3554 [==============================] - 0s 131us/step - loss: 755461570089.7783 - val_loss: 804592365824.9362\n",
      "Epoch 500/7000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 757485628704.7024 - val_loss: 798108488284.8945\n",
      "Epoch 501/7000\n",
      "3554/3554 [==============================] - 1s 159us/step - loss: 755617616137.0759 - val_loss: 799270926171.8143\n",
      "Epoch 502/7000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 759034298063.7389 - val_loss: 807758915082.2256\n",
      "Epoch 503/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 756353693270.7260 - val_loss: 798674325749.4143\n",
      "Epoch 504/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 756552668516.7001 - val_loss: 808113890411.4408\n",
      "Epoch 505/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 756332596904.5537 - val_loss: 796538718939.0582\n",
      "Epoch 506/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 757154110386.2059 - val_loss: 799137121836.7910\n",
      "Epoch 507/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 757464721780.2589 - val_loss: 797764831207.2281\n",
      "Epoch 508/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 754218917917.9651 - val_loss: 797459055458.4393\n",
      "Epoch 509/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 754183927846.0326 - val_loss: 796705728448.0540\n",
      "Epoch 510/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 751951693278.8655 - val_loss: 800588379378.2458\n",
      "Epoch 511/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 757330223873.8728 - val_loss: 806625612814.1143\n",
      "Epoch 512/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 752035742479.7029 - val_loss: 796476103300.6448\n",
      "Epoch 513/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 756661851345.1796 - val_loss: 796038217317.8239\n",
      "Epoch 514/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 754738724310.7980 - val_loss: 795262011570.0118\n",
      "Epoch 515/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 759115600985.3191 - val_loss: 797025492021.8644\n",
      "Epoch 516/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 751464417398.7080 - val_loss: 795251316647.8583\n",
      "Epoch 517/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 753257035495.9415 - val_loss: 803537610314.7477\n",
      "Epoch 518/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 755037866665.1300 - val_loss: 795535694874.2120\n",
      "Epoch 519/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 759773281755.4081 - val_loss: 799004944822.4045\n",
      "Epoch 520/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 753743815656.3735 - val_loss: 795490335307.0358\n",
      "Epoch 521/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 753903920915.7367 - val_loss: 795367481576.7404\n",
      "Epoch 522/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 757971185020.9027 - val_loss: 805353981802.7927\n",
      "Epoch 523/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 756214225823.7659 - val_loss: 793941815193.4559\n",
      "Epoch 524/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 754557248401.9359 - val_loss: 810348973938.2819\n",
      "Epoch 525/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 752000358239.8019 - val_loss: 795986880120.8348\n",
      "Epoch 526/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 753959528414.5774 - val_loss: 801059355851.6478\n",
      "Epoch 527/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 753646737441.4226 - val_loss: 795433961276.7054\n",
      "Epoch 528/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 751065937580.0112 - val_loss: 803178324735.3519\n",
      "Epoch 529/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 760469438325.6995 - val_loss: 803263413253.4729\n",
      "Epoch 530/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 750896079368.9320 - val_loss: 795486769143.9347\n",
      "Epoch 531/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 750826758987.6332 - val_loss: 793780967027.0740\n",
      "Epoch 532/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 756140169332.4030 - val_loss: 828440285116.0214\n",
      "Epoch 533/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 757947741167.2887 - val_loss: 795075549409.2512\n",
      "Epoch 534/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 749878532101.7626 - val_loss: 808072342228.1451\n",
      "Epoch 535/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 755146100669.1548 - val_loss: 796161908701.4346\n",
      "Epoch 536/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 755151020619.7771 - val_loss: 801468025401.7531\n",
      "Epoch 537/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 751369965169.2335 - val_loss: 792603005062.8051\n",
      "Epoch 538/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 751713349800.2656 - val_loss: 824603947476.9373\n",
      "Epoch 539/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 757661572977.0895 - val_loss: 793421694057.1365\n",
      "Epoch 540/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 752455482848.0179 - val_loss: 799389265033.9735\n",
      "Epoch 541/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 752186056173.8480 - val_loss: 792142072419.2314\n",
      "Epoch 542/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 762777614825.8142 - val_loss: 792707424004.8247\n",
      "Epoch 543/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 751634674367.0276 - val_loss: 792813416087.3677\n",
      "Epoch 544/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 749763367831.6984 - val_loss: 793832695919.1854\n",
      "Epoch 545/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 748518111067.1919 - val_loss: 801779670438.5620\n",
      "Epoch 546/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 752510214784.7924 - val_loss: 792193095919.0774\n",
      "Epoch 547/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 749477310493.9651 - val_loss: 801398725948.5615\n",
      "Epoch 548/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 751134623254.1858 - val_loss: 803362193698.3494\n",
      "Epoch 549/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 751956859125.4834 - val_loss: 809041858786.9795\n",
      "Epoch 550/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 757823558132.1869 - val_loss: 805159617742.5283\n",
      "Epoch 551/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 748927015083.7231 - val_loss: 794006553940.1812\n",
      "Epoch 552/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 749868641531.8221 - val_loss: 799710251467.1438\n",
      "Epoch 553/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 751587075868.9567 - val_loss: 795634622014.6498\n",
      "Epoch 554/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 752551894500.6281 - val_loss: 793835727645.8846\n",
      "Epoch 555/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 750946639415.6083 - val_loss: 794990183635.1370\n",
      "Epoch 556/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 751551216871.0770 - val_loss: 811151689412.8788\n",
      "Epoch 557/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 752063558657.7288 - val_loss: 803433748189.0746\n",
      "Epoch 558/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 751100408295.5093 - val_loss: 798808320037.4458\n",
      "Epoch 559/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 751266004346.5975 - val_loss: 793370133733.5719\n",
      "Epoch 560/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 753488321550.4064 - val_loss: 791480435913.9196\n",
      "Epoch 561/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 747933756345.1210 - val_loss: 789510777777.3638\n",
      "Epoch 562/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 747996331583.6759 - val_loss: 792646607159.0886\n",
      "Epoch 563/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 748256418498.4851 - val_loss: 789262311278.8253\n",
      "Epoch 564/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 750938054955.6511 - val_loss: 797902182901.7744\n",
      "Epoch 565/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 748519426139.0479 - val_loss: 795539578264.7358\n",
      "Epoch 566/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 753018650153.7783 - val_loss: 791561718168.4479\n",
      "Epoch 567/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 80us/step - loss: 748064639196.1284 - val_loss: 790616174019.3666\n",
      "Epoch 568/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 747259847010.9713 - val_loss: 791704385284.2487\n",
      "Epoch 569/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 748606209780.6189 - val_loss: 790227727182.8523\n",
      "Epoch 570/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 749081567947.7052 - val_loss: 792688681953.1792\n",
      "Epoch 571/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 748754930289.8098 - val_loss: 789259112310.8906\n",
      "Epoch 572/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 748602442696.1035 - val_loss: 790760620437.5674\n",
      "Epoch 573/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 751162273139.6826 - val_loss: 789196373218.6914\n",
      "Epoch 574/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 747377711994.8859 - val_loss: 790074092050.5789\n",
      "Epoch 575/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 748187273390.0281 - val_loss: 792857490936.9429\n",
      "Epoch 576/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 747757375965.1368 - val_loss: 789353743165.5696\n",
      "Epoch 577/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 749530114143.6580 - val_loss: 791298252018.8219\n",
      "Epoch 578/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 749752377938.6923 - val_loss: 793716171556.2217\n",
      "Epoch 579/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 746790673938.7281 - val_loss: 812325142961.2197\n",
      "Epoch 580/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 746680842019.8716 - val_loss: 801943260535.3226\n",
      "Epoch 581/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 753900753797.2583 - val_loss: 799386436726.9626\n",
      "Epoch 582/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 750589691561.7063 - val_loss: 791177844947.4250\n",
      "Epoch 583/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 749097696109.0557 - val_loss: 791676328760.3848\n",
      "Epoch 584/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 753948205152.8103 - val_loss: 788535265209.1409\n",
      "Epoch 585/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 749610055008.0900 - val_loss: 790319116664.7628\n",
      "Epoch 586/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 745070308705.2426 - val_loss: 792312553557.2614\n",
      "Epoch 587/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 746212816067.3494 - val_loss: 788609557307.2653\n",
      "Epoch 588/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 745751742757.8887 - val_loss: 788101974221.6642\n",
      "Epoch 589/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 746535029109.6995 - val_loss: 789439790656.9541\n",
      "Epoch 590/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 746507587258.9938 - val_loss: 787627717814.3325\n",
      "Epoch 591/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 749490336502.3478 - val_loss: 814353673847.1066\n",
      "Epoch 592/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 745010737292.6055 - val_loss: 788917119156.3162\n",
      "Epoch 593/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 745080452752.3511 - val_loss: 787490701053.9116\n",
      "Epoch 594/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 747008656927.4058 - val_loss: 819234629508.1406\n",
      "Epoch 595/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 746294054190.5323 - val_loss: 786806591406.1952\n",
      "Epoch 596/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 747036193581.6680 - val_loss: 787357035598.0602\n",
      "Epoch 597/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 747923484886.3657 - val_loss: 785949759040.3781\n",
      "Epoch 598/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 744589222021.6906 - val_loss: 795914449180.5885\n",
      "Epoch 599/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 745440315464.6077 - val_loss: 788023087754.9817\n",
      "Epoch 600/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 746175841224.1035 - val_loss: 798901414874.8422\n",
      "Epoch 601/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 745965492386.5031 - val_loss: 808381878342.2830\n",
      "Epoch 602/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 755445849415.8876 - val_loss: 789127314769.3007\n",
      "Epoch 603/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 748505099939.9437 - val_loss: 791302188584.7584\n",
      "Epoch 604/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 744019834355.6105 - val_loss: 790249012317.0386\n",
      "Epoch 605/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 747973359534.1722 - val_loss: 788460195407.9325\n",
      "Epoch 606/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 743070294666.0123 - val_loss: 799073721619.0830\n",
      "Epoch 607/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 743502700106.0483 - val_loss: 786764855435.9899\n",
      "Epoch 608/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 750079107113.4901 - val_loss: 795620098301.4796\n",
      "Epoch 609/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 746105200736.8103 - val_loss: 786603460315.9224\n",
      "Epoch 610/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 745782356367.9191 - val_loss: 790623475047.7682\n",
      "Epoch 611/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 745311155681.1705 - val_loss: 794671956027.9133\n",
      "Epoch 612/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 746083968013.2538 - val_loss: 787665343534.3752\n",
      "Epoch 613/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 748284331154.9443 - val_loss: 785161609941.8734\n",
      "Epoch 614/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 743372441774.0281 - val_loss: 807279625150.6138\n",
      "Epoch 615/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 750655925850.1835 - val_loss: 785507884357.7789\n",
      "Epoch 616/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 744624168374.5278 - val_loss: 791681470976.7201\n",
      "Epoch 617/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 747096239060.7811 - val_loss: 789840852413.0295\n",
      "Epoch 618/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 744916319149.5959 - val_loss: 787723706439.4352\n",
      "Epoch 619/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 746161851187.4305 - val_loss: 789100040077.9342\n",
      "Epoch 620/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 745895080869.5284 - val_loss: 808877803043.5736\n",
      "Epoch 621/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 750628606668.8575 - val_loss: 787286413119.5859\n",
      "Epoch 622/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 742187153565.8932 - val_loss: 787051092552.7313\n",
      "Epoch 623/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 748270836695.0861 - val_loss: 786569776738.3673\n",
      "Epoch 624/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 743073949634.9174 - val_loss: 792504991863.8268\n",
      "Epoch 625/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 745896025111.6265 - val_loss: 789194903313.4988\n",
      "Epoch 626/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 744487399811.2415 - val_loss: 792668251524.5728\n",
      "Epoch 627/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 740925925268.8170 - val_loss: 787238273397.3063\n",
      "Epoch 628/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 745607511335.6173 - val_loss: 784037004370.0928\n",
      "Epoch 629/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 748962428661.7715 - val_loss: 784338728459.3778\n",
      "Epoch 630/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 747796519802.8859 - val_loss: 783674791815.5972\n",
      "Epoch 631/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 748894171756.0472 - val_loss: 801430940994.3224\n",
      "Epoch 632/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 744011827088.2072 - val_loss: 802868351603.9381\n",
      "Epoch 633/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 744488731343.1627 - val_loss: 793837241006.6993\n",
      "Epoch 634/7000\n",
      "3554/3554 [==============================] - 0s 136us/step - loss: 747260642146.6832 - val_loss: 784552547190.0265\n",
      "Epoch 635/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 743290059608.3105 - val_loss: 785833985576.1823\n",
      "Epoch 636/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 741305014074.9219 - val_loss: 783316828183.0436\n",
      "Epoch 637/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 740976066025.2380 - val_loss: 784244091645.6237\n",
      "Epoch 638/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 750699346087.1130 - val_loss: 798042100648.7224\n",
      "Epoch 639/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 760314534864.1711 - val_loss: 783264802945.6202\n",
      "Epoch 640/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 743147519068.7766 - val_loss: 782250766800.3285\n",
      "Epoch 641/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 752063281909.1953 - val_loss: 781894650029.4031\n",
      "Epoch 642/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 745971539588.8260 - val_loss: 797540073144.7809\n",
      "Epoch 643/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 746379262497.1345 - val_loss: 785027682176.1080\n",
      "Epoch 644/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 741209880420.4120 - val_loss: 785206545516.0168\n",
      "Epoch 645/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 741515910656.2881 - val_loss: 783363029302.5126\n",
      "Epoch 646/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 741464947915.4170 - val_loss: 786924280973.4301\n",
      "Epoch 647/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 745480395843.4215 - val_loss: 785090444998.6071\n",
      "Epoch 648/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 740224306626.0529 - val_loss: 781077319224.8889\n",
      "Epoch 649/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 746079025190.0326 - val_loss: 780038053374.1277\n",
      "Epoch 650/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 739005585376.8824 - val_loss: 800264703844.4557\n",
      "Epoch 651/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 744505364062.7935 - val_loss: 788932069392.7067\n",
      "Epoch 652/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 739483242060.3534 - val_loss: 785482269598.3528\n",
      "Epoch 653/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 747758868651.7231 - val_loss: 781292632826.1671\n",
      "Epoch 654/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 745363344594.3320 - val_loss: 800882562716.5525\n",
      "Epoch 655/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 737458680400.3872 - val_loss: 779201875817.0645\n",
      "Epoch 656/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 743047354258.5121 - val_loss: 779382361785.9331\n",
      "Epoch 657/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 741742512569.4092 - val_loss: 780640188399.2933\n",
      "Epoch 658/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 738472646034.8002 - val_loss: 824696181916.9845\n",
      "Epoch 659/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 745650857000.9139 - val_loss: 797940027923.1550\n",
      "Epoch 660/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 743577073619.0522 - val_loss: 792381147922.3629\n",
      "Epoch 661/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 738199937627.9122 - val_loss: 780348036248.3759\n",
      "Epoch 662/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 740780437432.5447 - val_loss: 780677076161.8542\n",
      "Epoch 663/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 738260425899.1470 - val_loss: 779084061865.0824\n",
      "Epoch 664/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 745054538349.1998 - val_loss: 780632373411.0334\n",
      "Epoch 665/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 740335902835.2504 - val_loss: 776676980567.2056\n",
      "Epoch 666/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 740604796250.3275 - val_loss: 778117443707.2832\n",
      "Epoch 667/7000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 737365900104.1757 - val_loss: 779128481428.4872\n",
      "Epoch 668/7000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 738031924327.1492 - val_loss: 777817647855.7975\n",
      "Epoch 669/7000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 735252339542.0056 - val_loss: 776544044109.4841\n",
      "Epoch 670/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 737257370773.2493 - val_loss: 776365592475.7604\n",
      "Epoch 671/7000\n",
      "3554/3554 [==============================] - 0s 138us/step - loss: 734095002732.9117 - val_loss: 775857349467.5262\n",
      "Epoch 672/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 734081871782.1046 - val_loss: 781176592908.8180\n",
      "Epoch 673/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 742243255982.3163 - val_loss: 777619689772.7190\n",
      "Epoch 674/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 734866115622.6089 - val_loss: 782438891266.5204\n",
      "Epoch 675/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 737472882940.3984 - val_loss: 774820744866.8894\n",
      "Epoch 676/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 743392011581.5149 - val_loss: 774098640635.6073\n",
      "Epoch 677/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 737283162250.3005 - val_loss: 774227654067.8121\n",
      "Epoch 678/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 733009816007.2391 - val_loss: 780449534681.3300\n",
      "Epoch 679/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 739482858101.2673 - val_loss: 774127364704.9271\n",
      "Epoch 680/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 734248909953.6567 - val_loss: 777495296928.0811\n",
      "Epoch 681/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 741660476101.9426 - val_loss: 776216221971.9471\n",
      "Epoch 682/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 734014916523.8671 - val_loss: 776668499632.7156\n",
      "Epoch 683/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 740151194524.3083 - val_loss: 783055915780.5367\n",
      "Epoch 684/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 736034517381.5464 - val_loss: 773377696824.7449\n",
      "Epoch 685/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 732779803677.3889 - val_loss: 773865127405.9972\n",
      "Epoch 686/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 734475099786.5886 - val_loss: 772426232751.6355\n",
      "Epoch 687/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 736479093321.4722 - val_loss: 771788292778.0906\n",
      "Epoch 688/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 732823857259.1830 - val_loss: 771044720755.2180\n",
      "Epoch 689/7000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 736184948391.4012 - val_loss: 774376942015.6219\n",
      "Epoch 690/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 735752781067.9573 - val_loss: 771848511338.2166\n",
      "Epoch 691/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 731683152429.2358 - val_loss: 773775228310.4315\n",
      "Epoch 692/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 731428279274.6787 - val_loss: 774410858393.4559\n",
      "Epoch 693/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 87us/step - loss: 735210273654.2758 - val_loss: 788387577620.9553\n",
      "Epoch 694/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 735559293717.4655 - val_loss: 786514730171.5173\n",
      "Epoch 695/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 731430245483.7592 - val_loss: 780158433576.6864\n",
      "Epoch 696/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 730549599804.2184 - val_loss: 771790247391.5950\n",
      "Epoch 697/7000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 730611115500.6956 - val_loss: 775628222237.0205\n",
      "Epoch 698/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 730159573043.8627 - val_loss: 787591142541.4301\n",
      "Epoch 699/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 733282497500.8485 - val_loss: 773430994884.6627\n",
      "Epoch 700/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 727003688602.7238 - val_loss: 776252988142.6453\n",
      "Epoch 701/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 730346208975.7389 - val_loss: 771424371250.2638\n",
      "Epoch 702/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 732815878239.6580 - val_loss: 775312334845.6957\n",
      "Epoch 703/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 728898328784.6031 - val_loss: 771440917223.7322\n",
      "Epoch 704/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 733532013871.6849 - val_loss: 775564284462.8074\n",
      "Epoch 705/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 728212057028.6461 - val_loss: 768990945369.8700\n",
      "Epoch 706/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 730651259481.6072 - val_loss: 774702722006.2335\n",
      "Epoch 707/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 730666909760.5402 - val_loss: 773609995062.0804\n",
      "Epoch 708/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 728567808239.7209 - val_loss: 771446837949.6776\n",
      "Epoch 709/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 725662434533.9247 - val_loss: 768213408863.3429\n",
      "Epoch 710/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 733833306844.9927 - val_loss: 764519089911.5747\n",
      "Epoch 711/7000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 730393040828.0022 - val_loss: 764455019060.5682\n",
      "Epoch 712/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 731652243940.0518 - val_loss: 767883810511.2484\n",
      "Epoch 713/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 728283102116.3760 - val_loss: 786859164090.4371\n",
      "Epoch 714/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 736506025298.2600 - val_loss: 768044415005.3806\n",
      "Epoch 715/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 730627268704.8103 - val_loss: 766177673225.2174\n",
      "Epoch 716/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 730813670353.3235 - val_loss: 763827658841.8700\n",
      "Epoch 717/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 726356841155.6377 - val_loss: 765679768654.6362\n",
      "Epoch 718/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 729897950388.3668 - val_loss: 765970977750.8096\n",
      "Epoch 719/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 727250303880.7158 - val_loss: 767791219932.3544\n",
      "Epoch 720/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 729766962366.7395 - val_loss: 762216409544.8394\n",
      "Epoch 721/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 720873220542.0192 - val_loss: 768925344059.6974\n",
      "Epoch 722/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 725062860938.3005 - val_loss: 766264350175.5950\n",
      "Epoch 723/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 729276211951.4327 - val_loss: 762560309451.3597\n",
      "Epoch 724/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 724027482310.2307 - val_loss: 765139504747.5848\n",
      "Epoch 725/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 726048647964.3805 - val_loss: 762193987144.4434\n",
      "Epoch 726/7000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 722863059351.9865 - val_loss: 774953100593.6157\n",
      "Epoch 727/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 726977350598.0867 - val_loss: 765482015613.5156\n",
      "Epoch 728/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 723625124093.5509 - val_loss: 760149203961.9510\n",
      "Epoch 729/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 724353097317.7085 - val_loss: 761084834638.8523\n",
      "Epoch 730/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 721089015063.4823 - val_loss: 759047349627.3552\n",
      "Epoch 731/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 721942694251.0388 - val_loss: 760666203658.2256\n",
      "Epoch 732/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 721159605541.3123 - val_loss: 761433705262.0151\n",
      "Epoch 733/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 719706504708.8982 - val_loss: 758689418910.2808\n",
      "Epoch 734/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 724361407813.5824 - val_loss: 760859587913.8115\n",
      "Epoch 735/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 725345495246.2982 - val_loss: 765942716438.4675\n",
      "Epoch 736/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 719968258738.3500 - val_loss: 761326467207.3811\n",
      "Epoch 737/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 727493985611.3450 - val_loss: 766890806020.5367\n",
      "Epoch 738/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 718348025892.8802 - val_loss: 759443765752.0787\n",
      "Epoch 739/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 722091034539.8671 - val_loss: 765779269313.4222\n",
      "Epoch 740/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 723350561925.1141 - val_loss: 814045187530.2797\n",
      "Epoch 741/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 725656758432.1981 - val_loss: 764597992845.2141\n",
      "Epoch 742/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 717145841511.8694 - val_loss: 770162867339.1257\n",
      "Epoch 743/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 722286166901.1232 - val_loss: 776129592703.3879\n",
      "Epoch 744/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 719536883489.5667 - val_loss: 763525499601.8408\n",
      "Epoch 745/7000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 722111143654.7889 - val_loss: 773836848474.5182\n",
      "Epoch 746/7000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 727849740740.9342 - val_loss: 758276732040.2452\n",
      "Epoch 747/7000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 715011662649.1930 - val_loss: 759281828379.2202\n",
      "Epoch 748/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 719199877084.2723 - val_loss: 756487798373.8239\n",
      "Epoch 749/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 717441415215.2527 - val_loss: 757617013836.3319\n",
      "Epoch 750/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 717029382933.4655 - val_loss: 764138895771.9044\n",
      "Epoch 751/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 725276894543.9550 - val_loss: 805234777524.3882\n",
      "Epoch 752/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 718472118441.4182 - val_loss: 754437720066.8805\n",
      "Epoch 753/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 714843754161.7738 - val_loss: 757320287871.7479\n",
      "Epoch 754/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 715793945763.0793 - val_loss: 756057351897.6180\n",
      "Epoch 755/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 714733960672.0179 - val_loss: 758799914629.5089\n",
      "Epoch 756/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 720162639666.8542 - val_loss: 754259985826.5294\n",
      "Epoch 757/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 716941594013.1729 - val_loss: 752745626781.5607\n",
      "Epoch 758/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 713397414398.5593 - val_loss: 755135650897.5167\n",
      "Epoch 759/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 722174530723.0793 - val_loss: 751783028556.8361\n",
      "Epoch 760/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 716441087822.5143 - val_loss: 751921952350.3347\n",
      "Epoch 761/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 713553709859.2954 - val_loss: 755546986343.3362\n",
      "Epoch 762/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 712044221612.2993 - val_loss: 754381336302.9333\n",
      "Epoch 763/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 715069290570.9128 - val_loss: 754418166447.2754\n",
      "Epoch 764/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 716868281080.6528 - val_loss: 752789773481.0824\n",
      "Epoch 765/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 717364068757.6815 - val_loss: 776800852953.6901\n",
      "Epoch 766/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 712853433097.9404 - val_loss: 750618001177.5640\n",
      "Epoch 767/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 716561767493.7266 - val_loss: 751398108640.1710\n",
      "Epoch 768/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 711235203590.6270 - val_loss: 756152376963.7806\n",
      "Epoch 769/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 715100684638.3612 - val_loss: 761323616953.6450\n",
      "Epoch 770/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 715687830974.5953 - val_loss: 753535537849.0690\n",
      "Epoch 771/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 709124921838.4243 - val_loss: 753226588895.0908\n",
      "Epoch 772/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 711726273321.0580 - val_loss: 755382196891.6884\n",
      "Epoch 773/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 720896270220.7496 - val_loss: 750543824527.8785\n",
      "Epoch 774/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 715128546810.5256 - val_loss: 755542481877.0813\n",
      "Epoch 775/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 710985005250.7732 - val_loss: 749400431052.2959\n",
      "Epoch 776/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 713652788090.8859 - val_loss: 748674524779.0087\n",
      "Epoch 777/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 709703975087.1807 - val_loss: 747508398284.5120\n",
      "Epoch 778/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 708842790261.9877 - val_loss: 751733383290.1311\n",
      "Epoch 779/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 711969196722.3500 - val_loss: 750148699849.4874\n",
      "Epoch 780/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 709839444397.8842 - val_loss: 746936248578.6644\n",
      "Epoch 781/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 708815055072.7383 - val_loss: 758858759731.1279\n",
      "Epoch 782/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 715442369853.5149 - val_loss: 746722070036.0192\n",
      "Epoch 783/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 709469757204.8892 - val_loss: 779046715120.0856\n",
      "Epoch 784/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 711226257908.1869 - val_loss: 744553108295.9392\n",
      "Epoch 785/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 711353690392.0585 - val_loss: 746096312117.2163\n",
      "Epoch 786/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 708920169616.6393 - val_loss: 769011986748.8495\n",
      "Epoch 787/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 716244153613.1096 - val_loss: 745604198463.3699\n",
      "Epoch 788/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 705782494062.2083 - val_loss: 745831764065.9353\n",
      "Epoch 789/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 707915183539.0703 - val_loss: 752812094701.6371\n",
      "Epoch 790/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 707257181298.6742 - val_loss: 744812404521.6945\n",
      "Epoch 791/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 707588920566.0596 - val_loss: 753217097945.7621\n",
      "Epoch 792/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 707647876264.8418 - val_loss: 742725970648.7539\n",
      "Epoch 793/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 707856387605.0332 - val_loss: 743433893299.8121\n",
      "Epoch 794/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 706122825949.2808 - val_loss: 747502410026.9907\n",
      "Epoch 795/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 705609481722.8137 - val_loss: 753663187224.5558\n",
      "Epoch 796/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 707741348395.5071 - val_loss: 752801432054.3505\n",
      "Epoch 797/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 712184433763.6917 - val_loss: 741276808641.6383\n",
      "Epoch 798/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 711505945714.6742 - val_loss: 749111599032.5648\n",
      "Epoch 799/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 708225098636.1733 - val_loss: 741833409152.6121\n",
      "Epoch 800/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 711368515099.9482 - val_loss: 740361853706.2976\n",
      "Epoch 801/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 702567298695.1312 - val_loss: 752690373131.0897\n",
      "Epoch 802/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 709057247546.6337 - val_loss: 754233210554.5092\n",
      "Epoch 803/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 705029368807.2212 - val_loss: 742315913827.8076\n",
      "Epoch 804/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 704331152651.9573 - val_loss: 740334659819.6208\n",
      "Epoch 805/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 705161048186.1655 - val_loss: 750983841532.1833\n",
      "Epoch 806/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 709475482486.2758 - val_loss: 749847139853.6821\n",
      "Epoch 807/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 703791856797.3168 - val_loss: 743653263395.1415\n",
      "Epoch 808/7000\n",
      "3554/3554 [==============================] - 0s 131us/step - loss: 702029581356.3713 - val_loss: 742437532315.1122\n",
      "Epoch 809/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 709280345367.4823 - val_loss: 786035396848.8057\n",
      "Epoch 810/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 705288144562.9263 - val_loss: 782772753454.3752\n",
      "Epoch 811/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 707657925622.2037 - val_loss: 745535186303.9640\n",
      "Epoch 812/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 703748613953.8368 - val_loss: 741354925659.4543\n",
      "Epoch 813/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 706679027242.9308 - val_loss: 745816737370.8782\n",
      "Epoch 814/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 705443046551.5543 - val_loss: 741459235729.6787\n",
      "Epoch 815/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 701657462646.8519 - val_loss: 737345202158.1412\n",
      "Epoch 816/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 703706827285.6094 - val_loss: 758109361719.1606\n",
      "Epoch 817/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 708174541281.7468 - val_loss: 739455862597.9230\n",
      "Epoch 818/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 704395656081.9359 - val_loss: 737913195959.8447\n",
      "Epoch 819/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 85us/step - loss: 702362786821.1863 - val_loss: 749650723101.7406\n",
      "Epoch 820/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 702851589179.3539 - val_loss: 736964937869.1421\n",
      "Epoch 821/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 699262094039.2302 - val_loss: 745403596634.0861\n",
      "Epoch 822/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 704717648648.7878 - val_loss: 742001216965.6709\n",
      "Epoch 823/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 700593088727.5183 - val_loss: 743009568867.6636\n",
      "Epoch 824/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 706527912917.9336 - val_loss: 752023135293.3536\n",
      "Epoch 825/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 703462316775.3651 - val_loss: 768266157960.1732\n",
      "Epoch 826/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 704150679416.0045 - val_loss: 741502765104.9677\n",
      "Epoch 827/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 705857831953.8638 - val_loss: 739317710214.3010\n",
      "Epoch 828/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 702980805693.6591 - val_loss: 735075427062.1344\n",
      "Epoch 829/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 697336449271.7883 - val_loss: 740084944360.2363\n",
      "Epoch 830/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 701321202889.1119 - val_loss: 735842685118.1097\n",
      "Epoch 831/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 703766629844.4929 - val_loss: 733991342956.8090\n",
      "Epoch 832/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 699785361768.7339 - val_loss: 735590623935.4059\n",
      "Epoch 833/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 697083930360.0765 - val_loss: 736756174489.6720\n",
      "Epoch 834/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 700415620429.6500 - val_loss: 739212391233.0261\n",
      "Epoch 835/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 704840689040.4952 - val_loss: 737823431102.4697\n",
      "Epoch 836/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 701454523572.3668 - val_loss: 736652577614.2762\n",
      "Epoch 837/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 701592746206.4333 - val_loss: 734130820543.3339\n",
      "Epoch 838/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 697862732966.5369 - val_loss: 737666093370.5452\n",
      "Epoch 839/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 701867369992.3556 - val_loss: 735632292122.8602\n",
      "Epoch 840/7000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 697707343982.70 - 0s 84us/step - loss: 697769319973.1683 - val_loss: 750440036998.9491\n",
      "Epoch 841/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 704901231322.6877 - val_loss: 733610386221.4391\n",
      "Epoch 842/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 699350883259.4260 - val_loss: 737196836470.5305\n",
      "Epoch 843/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 695799232540.8126 - val_loss: 739714265059.1956\n",
      "Epoch 844/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 697728670393.2650 - val_loss: 737157800634.2211\n",
      "Epoch 845/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 700984649883.5880 - val_loss: 744817939103.7209\n",
      "Epoch 846/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 697470195763.8627 - val_loss: 743154711645.0386\n",
      "Epoch 847/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 697909963622.1407 - val_loss: 733743482606.3573\n",
      "Epoch 848/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 700801791549.3707 - val_loss: 731781662448.6616\n",
      "Epoch 849/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 696653223785.0220 - val_loss: 731681505008.0856\n",
      "Epoch 850/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 695954184944.5852 - val_loss: 733083943753.0914\n",
      "Epoch 851/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 697269682618.5616 - val_loss: 770182292186.4822\n",
      "Epoch 852/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 696566753320.3376 - val_loss: 729229453561.4470\n",
      "Epoch 853/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 695194444075.6511 - val_loss: 732472892774.0399\n",
      "Epoch 854/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 693239384210.3679 - val_loss: 755092522181.3108\n",
      "Epoch 855/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 697008623078.9331 - val_loss: 736913564101.9590\n",
      "Epoch 856/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 698924867058.4581 - val_loss: 729858544211.6771\n",
      "Epoch 857/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 695134582287.2706 - val_loss: 729922920514.8264\n",
      "Epoch 858/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 691209052044.7496 - val_loss: 755071655929.0869\n",
      "Epoch 859/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 693985282987.2909 - val_loss: 726860922866.1738\n",
      "Epoch 860/7000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 688929641911.6804 - val_loss: 732655067361.8273\n",
      "Epoch 861/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 689637349437.0826 - val_loss: 724996331869.6866\n",
      "Epoch 862/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 689214625530.3816 - val_loss: 728467330032.1575\n",
      "Epoch 863/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 687607394555.2460 - val_loss: 721517656238.5553\n",
      "Epoch 864/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 688302378424.8328 - val_loss: 720475882078.0468\n",
      "Epoch 865/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 693536679635.1964 - val_loss: 720332350638.2672\n",
      "Epoch 866/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 686970600015.2346 - val_loss: 721991134714.6711\n",
      "Epoch 867/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 683908525602.8632 - val_loss: 720484213918.1367\n",
      "Epoch 868/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 684815941041.9178 - val_loss: 720278352827.1572\n",
      "Epoch 869/7000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 684513847115.0568 - val_loss: 723448103250.7410\n",
      "Epoch 870/7000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 678556481361.9719 - val_loss: 737156134727.0751\n",
      "Epoch 871/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 683536565173.6636 - val_loss: 716602462593.9803\n",
      "Epoch 872/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 681485293979.4441 - val_loss: 723600135083.0267\n",
      "Epoch 873/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 677693144553.2380 - val_loss: 714844169510.0940\n",
      "Epoch 874/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 680430035630.3163 - val_loss: 712350146546.1738\n",
      "Epoch 875/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 679050029041.0175 - val_loss: 715635639348.1361\n",
      "Epoch 876/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 679042244495.9191 - val_loss: 732792252444.2284\n",
      "Epoch 877/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 674174603298.5751 - val_loss: 710477190353.9849\n",
      "Epoch 878/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 679178153912.5447 - val_loss: 712175393677.6461\n",
      "Epoch 879/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 673154550736.1711 - val_loss: 717246931980.9620\n",
      "Epoch 880/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 672161678445.4879 - val_loss: 716618693222.6880\n",
      "Epoch 881/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 672078461602.2150 - val_loss: 711631514114.4484\n",
      "Epoch 882/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 669273890749.1548 - val_loss: 704995994520.0157\n",
      "Epoch 883/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 667304379114.8228 - val_loss: 705918867420.8585\n",
      "Epoch 884/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 671981205025.7108 - val_loss: 707120061537.6472\n",
      "Epoch 885/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 668137009716.7271 - val_loss: 700520824557.7811\n",
      "Epoch 886/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 664696757290.0664 - val_loss: 697268725066.9637\n",
      "Epoch 887/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 660861045297.2695 - val_loss: 699836442815.8380\n",
      "Epoch 888/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 666174345720.7969 - val_loss: 718299692627.3890\n",
      "Epoch 889/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 660167501394.1158 - val_loss: 697723646190.5012\n",
      "Epoch 890/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 658997829632.0000 - val_loss: 706418107490.2234\n",
      "Epoch 891/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 656838583106.4131 - val_loss: 702781115410.1469\n",
      "Epoch 892/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 663648667407.1267 - val_loss: 699068802835.2271\n",
      "Epoch 893/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 648897037873.8457 - val_loss: 689754763020.8900\n",
      "Epoch 894/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 652614286189.0557 - val_loss: 709289619526.2830\n",
      "Epoch 895/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 649525515924.9612 - val_loss: 683818628773.7699\n",
      "Epoch 896/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 647248720538.1475 - val_loss: 678017384229.0858\n",
      "Epoch 897/7000\n",
      "3554/3554 [==============================] - 0s 121us/step - loss: 644004264831.4957 - val_loss: 675905960605.9927\n",
      "Epoch 898/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 641342927348.1869 - val_loss: 675343921892.8518\n",
      "Epoch 899/7000\n",
      "3554/3554 [==============================] - 1s 143us/step - loss: 643361333695.1716 - val_loss: 673980985011.0200\n",
      "Epoch 900/7000\n",
      "3554/3554 [==============================] - 0s 140us/step - loss: 635968005953.8368 - val_loss: 674612105873.8948\n",
      "Epoch 901/7000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 634735477150.9016 - val_loss: 666965812527.3114\n",
      "Epoch 902/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 633691168993.8909 - val_loss: 665611215193.6541\n",
      "Epoch 903/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 629000044470.2397 - val_loss: 667862976254.4878\n",
      "Epoch 904/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 626048327334.2488 - val_loss: 700405205953.4943\n",
      "Epoch 905/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 629840232567.8605 - val_loss: 656278374642.5339\n",
      "Epoch 906/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 619598785760.1621 - val_loss: 652501506860.8630\n",
      "Epoch 907/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 618174339428.1238 - val_loss: 652666241860.4827\n",
      "Epoch 908/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 611609926504.4457 - val_loss: 645204661451.0718\n",
      "Epoch 909/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 610507193372.2363 - val_loss: 660269535189.3694\n",
      "Epoch 910/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 610561454690.8273 - val_loss: 658794923603.9651\n",
      "Epoch 911/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 611394858088.3016 - val_loss: 642635461256.6774\n",
      "Epoch 912/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 603748252912.2971 - val_loss: 638818946923.3688\n",
      "Epoch 913/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 599605317642.3726 - val_loss: 634560514562.4484\n",
      "Epoch 914/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 594104453118.8475 - val_loss: 627357512909.3761\n",
      "Epoch 915/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 593605041673.5082 - val_loss: 644652721978.4011\n",
      "Epoch 916/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 585195035173.7445 - val_loss: 652144873226.5857\n",
      "Epoch 917/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 589415779944.0135 - val_loss: 617012633475.5646\n",
      "Epoch 918/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 577903466215.9415 - val_loss: 611040004732.5795\n",
      "Epoch 919/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 576195816496.9814 - val_loss: 606805325731.2495\n",
      "Epoch 920/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 570857675762.7462 - val_loss: 606955847815.9573\n",
      "Epoch 921/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 571027946155.4349 - val_loss: 615669588947.3530\n",
      "Epoch 922/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 563219448683.9032 - val_loss: 602835042343.1741\n",
      "Epoch 923/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 561563758557.4248 - val_loss: 593558460696.5558\n",
      "Epoch 924/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 558058960814.1721 - val_loss: 589378355117.0431\n",
      "Epoch 925/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 557742938357.4834 - val_loss: 587106311323.2562\n",
      "Epoch 926/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 550433781632.0720 - val_loss: 581416726082.1063\n",
      "Epoch 927/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 548505358794.6967 - val_loss: 579094836927.1179\n",
      "Epoch 928/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 545282011555.9438 - val_loss: 575475454437.0677\n",
      "Epoch 929/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 546412220939.2369 - val_loss: 582921978024.2183\n",
      "Epoch 930/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 538966692095.8560 - val_loss: 567796328001.5303\n",
      "Epoch 931/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 533278675688.5177 - val_loss: 567020713140.0281\n",
      "Epoch 932/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 536052169018.6337 - val_loss: 565848874617.4110\n",
      "Epoch 933/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 527403086729.8683 - val_loss: 563660702043.9584\n",
      "Epoch 934/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 527799159465.1300 - val_loss: 557769437710.5463\n",
      "Epoch 935/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 522203890586.0034 - val_loss: 553210359398.9761\n",
      "Epoch 936/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 518799491293.2808 - val_loss: 550266064470.8456\n",
      "Epoch 937/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 517965490211.7277 - val_loss: 549650932811.1797\n",
      "Epoch 938/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 514945203700.7631 - val_loss: 546633654258.1738\n",
      "Epoch 939/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 514346145087.8199 - val_loss: 545947649322.7027\n",
      "Epoch 940/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 509226795533.5419 - val_loss: 539900579516.2374\n",
      "Epoch 941/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 504654949867.5430 - val_loss: 534643225365.2433\n",
      "Epoch 942/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 502075852646.7170 - val_loss: 531865714754.8264\n",
      "Epoch 943/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 497189307963.0659 - val_loss: 528105332759.0436\n",
      "Epoch 944/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 492369354968.6708 - val_loss: 524146476780.0529\n",
      "Epoch 945/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 85us/step - loss: 501596380709.1682 - val_loss: 533438897053.7767\n",
      "Epoch 946/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 489445442540.9837 - val_loss: 521924232859.4003\n",
      "Epoch 947/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 487913338235.7501 - val_loss: 518903001985.5482\n",
      "Epoch 948/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 484076100742.2667 - val_loss: 514830706036.4422\n",
      "Epoch 949/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 482419135300.7181 - val_loss: 514363167140.5457\n",
      "Epoch 950/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 479802669508.9341 - val_loss: 508815401494.6115\n",
      "Epoch 951/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 477783709351.4012 - val_loss: 528715697815.3677\n",
      "Epoch 952/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 484172051273.3281 - val_loss: 513802414483.8391\n",
      "Epoch 953/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 477284224625.2335 - val_loss: 503382814867.4791\n",
      "Epoch 954/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 473499872701.4429 - val_loss: 499209606483.0290\n",
      "Epoch 955/7000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 469835988798.3793 - val_loss: 498106588842.9547\n",
      "Epoch 956/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 466941146030.7485 - val_loss: 515134523623.0121\n",
      "Epoch 957/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 466864065080.1846 - val_loss: 496927856964.3387\n",
      "Epoch 958/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 462912169590.9960 - val_loss: 493613604177.3007\n",
      "Epoch 959/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 460946990130.1339 - val_loss: 492915571394.8624\n",
      "Epoch 960/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 465556254141.4429 - val_loss: 492040893244.4174\n",
      "Epoch 961/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 462194129745.3956 - val_loss: 492590777172.6132\n",
      "Epoch 962/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 456720056264.6797 - val_loss: 486949094903.7907\n",
      "Epoch 963/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 455864594775.4463 - val_loss: 485423033169.7328\n",
      "Epoch 964/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 454476398462.9195 - val_loss: 487154674365.1016\n",
      "Epoch 965/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 451765742963.6826 - val_loss: 481896422536.8214\n",
      "Epoch 966/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 452733879055.7029 - val_loss: 491906569926.0309\n",
      "Epoch 967/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 449290945922.6652 - val_loss: 476665745308.9125\n",
      "Epoch 968/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 446668238758.1047 - val_loss: 475940892737.6743\n",
      "Epoch 969/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 443167793248.8103 - val_loss: 486252532609.5482\n",
      "Epoch 970/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 445787265638.2847 - val_loss: 473674378748.9755\n",
      "Epoch 971/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 441187580707.2954 - val_loss: 479796048960.8101\n",
      "Epoch 972/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 446547599890.1519 - val_loss: 471472404269.7272\n",
      "Epoch 973/7000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 440188778192.8914 - val_loss: 474321201383.0121\n",
      "Epoch 974/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 449006893204.6730 - val_loss: 475164342536.4253\n",
      "Epoch 975/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 441462023845.0963 - val_loss: 471300819064.4028\n",
      "Epoch 976/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 434586529702.1047 - val_loss: 469314936269.7361\n",
      "Epoch 977/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 431623621925.3123 - val_loss: 481435292810.5496\n",
      "Epoch 978/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 434870750344.5718 - val_loss: 463875433673.9196\n",
      "Epoch 979/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 434827518454.4918 - val_loss: 467837166010.4371\n",
      "Epoch 980/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 444466360866.2870 - val_loss: 463279110127.2934\n",
      "Epoch 981/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 427903068338.6381 - val_loss: 458767814321.8678\n",
      "Epoch 982/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 427974731087.9550 - val_loss: 456847978241.3682\n",
      "Epoch 983/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 425334615835.8041 - val_loss: 467774386230.1525\n",
      "Epoch 984/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 427800805643.3810 - val_loss: 462073777304.9519\n",
      "Epoch 985/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 430713827809.7468 - val_loss: 457895215734.2425\n",
      "Epoch 986/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 423036360737.4226 - val_loss: 469029483817.5505\n",
      "Epoch 987/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 421414539289.3551 - val_loss: 450648752128.0000\n",
      "Epoch 988/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 420984579527.8154 - val_loss: 462599575016.8124\n",
      "Epoch 989/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 425377531810.6472 - val_loss: 453379921324.8990\n",
      "Epoch 990/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 418611266301.2628 - val_loss: 446898121157.3828\n",
      "Epoch 991/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 418230213729.3866 - val_loss: 448125602762.7117\n",
      "Epoch 992/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 418589433491.8087 - val_loss: 463871425979.3013\n",
      "Epoch 993/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 414594516528.6934 - val_loss: 447586950673.7148\n",
      "Epoch 994/7000\n",
      "3554/3554 [==============================] - 0s 124us/step - loss: 414134896841.6883 - val_loss: 445035922766.4202\n",
      "Epoch 995/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 416802471899.1199 - val_loss: 443302276133.7339\n",
      "Epoch 996/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 415379616225.7468 - val_loss: 464934469387.7379\n",
      "Epoch 997/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 410979659976.5357 - val_loss: 443088640626.2098\n",
      "Epoch 998/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 409294399288.6168 - val_loss: 437810447008.8731\n",
      "Epoch 999/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 407897500103.2392 - val_loss: 436091601334.9806\n",
      "Epoch 1000/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 407090139631.5768 - val_loss: 435955493666.4934\n",
      "Epoch 1001/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 405617125757.4789 - val_loss: 452713325162.7207\n",
      "Epoch 1002/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 409495925033.3461 - val_loss: 433590429439.6400\n",
      "Epoch 1003/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 406689046587.9302 - val_loss: 434941405001.3795\n",
      "Epoch 1004/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 405664350290.9803 - val_loss: 443041798317.4031\n",
      "Epoch 1005/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 406897169672.4997 - val_loss: 441288986148.1497\n",
      "Epoch 1006/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 406244192087.7344 - val_loss: 431443049868.0619\n",
      "Epoch 1007/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 401642991572.2049 - val_loss: 428976784858.9862\n",
      "Epoch 1008/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 401206816742.6449 - val_loss: 428895608462.1502\n",
      "Epoch 1009/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 399243120665.3551 - val_loss: 428241505700.2577\n",
      "Epoch 1010/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 400853506394.9037 - val_loss: 440537736267.1797\n",
      "Epoch 1011/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 404514387001.0490 - val_loss: 429605870174.6228\n",
      "Epoch 1012/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 400080295517.6409 - val_loss: 449018065901.8531\n",
      "Epoch 1013/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 397865356000.4502 - val_loss: 436372081712.1035\n",
      "Epoch 1014/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 396588964398.9645 - val_loss: 425145713500.6785\n",
      "Epoch 1015/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 395078923542.3298 - val_loss: 427988672161.7372\n",
      "Epoch 1016/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 396981125163.7951 - val_loss: 434007953065.8026\n",
      "Epoch 1017/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 393812555546.6517 - val_loss: 429422578475.7108\n",
      "Epoch 1018/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 400314013452.8217 - val_loss: 430456093452.6020\n",
      "Epoch 1019/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 393018165348.2679 - val_loss: 421680154667.2067\n",
      "Epoch 1020/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 390577807908.5920 - val_loss: 420870412283.9673\n",
      "Epoch 1021/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 397093156067.6196 - val_loss: 421529773772.9440\n",
      "Epoch 1022/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 392503028950.9421 - val_loss: 425564628926.9019\n",
      "Epoch 1023/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 389868678100.7811 - val_loss: 426502078801.5887\n",
      "Epoch 1024/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 389697952173.8841 - val_loss: 424998068575.4149\n",
      "Epoch 1025/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 390437184967.8154 - val_loss: 424603529357.7181\n",
      "Epoch 1026/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 389096117715.3405 - val_loss: 416307682446.5823\n",
      "Epoch 1027/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 387440610169.7333 - val_loss: 419532631546.6711\n",
      "Epoch 1028/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 387186752457.2560 - val_loss: 414637828292.4467\n",
      "Epoch 1029/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 386926756310.2217 - val_loss: 417434928275.7671\n",
      "Epoch 1030/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 389378924954.2915 - val_loss: 416538779232.0630\n",
      "Epoch 1031/7000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 389496976943.5408 - val_loss: 423454594974.6408\n",
      "Epoch 1032/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 386812101649.8638 - val_loss: 413728416480.2430\n",
      "Epoch 1033/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 386160177463.1761 - val_loss: 411797490645.0813\n",
      "Epoch 1034/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 382081685786.3635 - val_loss: 410824937557.2614\n",
      "Epoch 1035/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 385556112821.3754 - val_loss: 411865392333.3761\n",
      "Epoch 1036/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 383310410726.0687 - val_loss: 414044810381.4301\n",
      "Epoch 1037/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 385067410463.6939 - val_loss: 410873377127.4802\n",
      "Epoch 1038/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 385666218833.3956 - val_loss: 435210847970.8354\n",
      "Epoch 1039/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 384207880422.5009 - val_loss: 409997681844.0281\n",
      "Epoch 1040/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 379596601566.4333 - val_loss: 408871911846.8501\n",
      "Epoch 1041/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 382864831881.5802 - val_loss: 410088703156.0281\n",
      "Epoch 1042/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 379706389708.5695 - val_loss: 415136201545.6675\n",
      "Epoch 1043/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 379893654310.7529 - val_loss: 428203734199.4847\n",
      "Epoch 1044/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 386307736796.7046 - val_loss: 407972124553.9016\n",
      "Epoch 1045/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 377720692187.4080 - val_loss: 409131233853.2095\n",
      "Epoch 1046/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 381995796477.6950 - val_loss: 404920346153.3345\n",
      "Epoch 1047/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 378369477284.5200 - val_loss: 412496511761.7868\n",
      "Epoch 1048/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 378644452173.9381 - val_loss: 403871186468.1497\n",
      "Epoch 1049/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 374486248796.6325 - val_loss: 406388217388.7910\n",
      "Epoch 1050/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 377182651419.0839 - val_loss: 407790442814.5778\n",
      "Epoch 1051/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 385469270651.0298 - val_loss: 428180038041.6000\n",
      "Epoch 1052/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 380427612466.5662 - val_loss: 401669894311.0661\n",
      "Epoch 1053/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 374565634646.1497 - val_loss: 413728406274.2324\n",
      "Epoch 1054/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 374630540872.8959 - val_loss: 401565706216.3803\n",
      "Epoch 1055/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 375486635202.1970 - val_loss: 411808332532.4062\n",
      "Epoch 1056/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 378861606899.3225 - val_loss: 406049120152.8799\n",
      "Epoch 1057/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 372771299484.7406 - val_loss: 401305842770.0928\n",
      "Epoch 1058/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 373754304195.0613 - val_loss: 415129798908.9035\n",
      "Epoch 1059/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 373036161670.5548 - val_loss: 403947288914.1649\n",
      "Epoch 1060/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 372953500744.0315 - val_loss: 400264122986.7207\n",
      "Epoch 1061/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 374491823155.2864 - val_loss: 402133485625.0329\n",
      "Epoch 1062/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 372734764158.1992 - val_loss: 406090576906.6577\n",
      "Epoch 1063/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 371532099106.8633 - val_loss: 401589517528.8979\n",
      "Epoch 1064/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 370711143132.9927 - val_loss: 398615983705.7260\n",
      "Epoch 1065/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 373123350952.1216 - val_loss: 406281947963.8414\n",
      "Epoch 1066/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 373114943387.7321 - val_loss: 422112001121.6473\n",
      "Epoch 1067/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 368873782834.9983 - val_loss: 399123588967.3361\n",
      "Epoch 1068/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 371149353414.0867 - val_loss: 395684302288.0405\n",
      "Epoch 1069/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 369729582505.8503 - val_loss: 394993167406.3752\n",
      "Epoch 1070/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 369785584105.8143 - val_loss: 398606781726.6048\n",
      "Epoch 1071/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 86us/step - loss: 368171761046.2578 - val_loss: 404150249689.7620\n",
      "Epoch 1072/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 368981585710.2443 - val_loss: 397548593274.1311\n",
      "Epoch 1073/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 368235836556.0292 - val_loss: 394966537209.3749\n",
      "Epoch 1074/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 366975795891.5025 - val_loss: 405566119507.3890\n",
      "Epoch 1075/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 366696952824.5087 - val_loss: 398129238619.7423\n",
      "Epoch 1076/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 370729099531.3810 - val_loss: 420549716989.4076\n",
      "Epoch 1077/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 368031859407.1626 - val_loss: 395588142806.1614\n",
      "Epoch 1078/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 365768690870.6719 - val_loss: 400066320734.8388\n",
      "Epoch 1079/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 366311286188.1553 - val_loss: 407032330538.9907\n",
      "Epoch 1080/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 363345274618.3815 - val_loss: 402018246271.1719\n",
      "Epoch 1081/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 364168130276.4839 - val_loss: 391965617666.7365\n",
      "Epoch 1082/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 367557811223.6263 - val_loss: 400951652525.9792\n",
      "Epoch 1083/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 367212285184.4321 - val_loss: 398166571304.6863\n",
      "Epoch 1084/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 363702571918.4783 - val_loss: 396213064110.9153\n",
      "Epoch 1085/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 362126779248.5132 - val_loss: 396884196932.4107\n",
      "Epoch 1086/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 367321465583.4327 - val_loss: 389964913131.9809\n",
      "Epoch 1087/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 360831782522.4536 - val_loss: 388282410541.9432\n",
      "Epoch 1088/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 365244502317.9561 - val_loss: 392835121894.5800\n",
      "Epoch 1089/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 367470407963.5161 - val_loss: 388381586556.1474\n",
      "Epoch 1090/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 361552008023.1581 - val_loss: 391217594114.5204\n",
      "Epoch 1091/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 363125420799.5679 - val_loss: 402285846880.5671\n",
      "Epoch 1092/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 361327245944.7249 - val_loss: 388566397374.4698\n",
      "Epoch 1093/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 358223956510.8295 - val_loss: 388133779579.2833\n",
      "Epoch 1094/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 359920094353.7917 - val_loss: 390695163908.0327\n",
      "Epoch 1095/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 363670000434.2780 - val_loss: 391099444115.4070\n",
      "Epoch 1096/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 361958030281.2560 - val_loss: 388764355095.7637\n",
      "Epoch 1097/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 357688817453.0917 - val_loss: 386432852025.0329\n",
      "Epoch 1098/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 358291574449.7737 - val_loss: 387279619572.9103\n",
      "Epoch 1099/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 358830371187.1064 - val_loss: 387613594252.7100\n",
      "Epoch 1100/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 363330390604.9296 - val_loss: 387778931091.8391\n",
      "Epoch 1101/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 356323071756.8217 - val_loss: 396502240025.8521\n",
      "Epoch 1102/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 361270222105.7872 - val_loss: 384243668086.3865\n",
      "Epoch 1103/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 357337004173.7580 - val_loss: 384378417751.1336\n",
      "Epoch 1104/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 358566248844.4615 - val_loss: 385227173108.5502\n",
      "Epoch 1105/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 358735255142.8610 - val_loss: 390687555559.8042\n",
      "Epoch 1106/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 359622776810.1024 - val_loss: 389488410282.9547\n",
      "Epoch 1107/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 355796443703.0320 - val_loss: 400527926717.0295\n",
      "Epoch 1108/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 358102925791.4418 - val_loss: 394221460360.4613\n",
      "Epoch 1109/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 358335568633.8052 - val_loss: 391813873206.0084\n",
      "Epoch 1110/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 356280015960.1666 - val_loss: 384578762704.7606\n",
      "Epoch 1111/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 353720916065.9629 - val_loss: 384985735537.2737\n",
      "Epoch 1112/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 359259974068.7991 - val_loss: 407146863550.0377\n",
      "Epoch 1113/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 351787992559.0005 - val_loss: 389259552805.4459\n",
      "Epoch 1114/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 351820920731.7321 - val_loss: 382761045159.9302\n",
      "Epoch 1115/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 354674201493.3934 - val_loss: 408727479759.4644\n",
      "Epoch 1116/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 351713026960.7833 - val_loss: 383132386209.8093\n",
      "Epoch 1117/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 357000721681.7198 - val_loss: 379591690543.0234\n",
      "Epoch 1118/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 353273045817.1931 - val_loss: 389850202795.8188\n",
      "Epoch 1119/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 351480334805.0693 - val_loss: 379626003372.1789\n",
      "Epoch 1120/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 354893152285.9651 - val_loss: 383266286288.6886\n",
      "Epoch 1121/7000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 353845198433.0985 - val_loss: 384815185661.9117\n",
      "Epoch 1122/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 352471048251.9302 - val_loss: 378774475704.2768\n",
      "Epoch 1123/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 350959607526.7889 - val_loss: 380732898724.8337\n",
      "Epoch 1124/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 348726783071.0816 - val_loss: 380457272380.7775\n",
      "Epoch 1125/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 353295635932.5605 - val_loss: 386382442175.1179\n",
      "Epoch 1126/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 348726314356.8351 - val_loss: 383762082074.8602\n",
      "Epoch 1127/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 351301500946.4401 - val_loss: 379895399306.7657\n",
      "Epoch 1128/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 351300783950.5143 - val_loss: 383045767930.7432\n",
      "Epoch 1129/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 348910407133.7130 - val_loss: 388156582722.7545\n",
      "Epoch 1130/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 350004818189.6860 - val_loss: 380160008084.2712\n",
      "Epoch 1131/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 355333512990.1092 - val_loss: 379954038259.7581\n",
      "Epoch 1132/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 347686584532.0607 - val_loss: 379021590093.0521\n",
      "Epoch 1133/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 347974527670.9601 - val_loss: 380463224772.0866\n",
      "Epoch 1134/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 90us/step - loss: 349720548699.4800 - val_loss: 406975326481.6428\n",
      "Epoch 1135/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 351424139731.9167 - val_loss: 376263841415.8132\n",
      "Epoch 1136/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 346645583200.0901 - val_loss: 389602096999.3361\n",
      "Epoch 1137/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 346125161893.2403 - val_loss: 376068575503.3384\n",
      "Epoch 1138/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 348713296863.1536 - val_loss: 378006076672.6481\n",
      "Epoch 1139/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 346416620096.2521 - val_loss: 374265014669.5021\n",
      "Epoch 1140/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 344027451629.4158 - val_loss: 375246781353.8746\n",
      "Epoch 1141/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 344168507459.9977 - val_loss: 375547242010.3561\n",
      "Epoch 1142/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 345571483726.3703 - val_loss: 375010172474.6172\n",
      "Epoch 1143/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 344813123488.3422 - val_loss: 373716059765.3784\n",
      "Epoch 1144/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 345409596884.4930 - val_loss: 374182376125.1016\n",
      "Epoch 1145/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 344099570558.3433 - val_loss: 374482659034.1941\n",
      "Epoch 1146/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 345770946506.9848 - val_loss: 372525704896.8461\n",
      "Epoch 1147/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 346084696345.2111 - val_loss: 371735368121.8610\n",
      "Epoch 1148/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 347672376625.9899 - val_loss: 378643960261.3828\n",
      "Epoch 1149/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 345346690544.1531 - val_loss: 378034602857.9285\n",
      "Epoch 1150/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 342583504806.1047 - val_loss: 381223628041.8655\n",
      "Epoch 1151/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 352257580608.8284 - val_loss: 385098302535.1471\n",
      "Epoch 1152/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 349347289997.3259 - val_loss: 371196229485.6732\n",
      "Epoch 1153/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 342086004710.6449 - val_loss: 370246204195.6456\n",
      "Epoch 1154/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 341236957414.5009 - val_loss: 378929153741.5201\n",
      "Epoch 1155/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 342867348822.8700 - val_loss: 397113906241.9623\n",
      "Epoch 1156/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 340940108949.2493 - val_loss: 383646700134.1119\n",
      "Epoch 1157/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 346093021058.3770 - val_loss: 373703573857.7192\n",
      "Epoch 1158/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 342464586599.2932 - val_loss: 379484386758.2469\n",
      "Epoch 1159/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 342924106213.2043 - val_loss: 371276219068.5255\n",
      "Epoch 1160/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 340598198334.2352 - val_loss: 373899383417.6990\n",
      "Epoch 1161/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 338727939074.8813 - val_loss: 371752904900.4467\n",
      "Epoch 1162/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 344850359917.7760 - val_loss: 369093103064.1058\n",
      "Epoch 1163/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 341591733932.0112 - val_loss: 371713621634.0523\n",
      "Epoch 1164/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 341314844756.1328 - val_loss: 368405777574.2020\n",
      "Epoch 1165/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 338166128875.1108 - val_loss: 378176969105.5347\n",
      "Epoch 1166/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 342934779425.7108 - val_loss: 375693287803.0672\n",
      "Epoch 1167/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 343060653000.1036 - val_loss: 385716602090.7567\n",
      "Epoch 1168/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 342850876970.9307 - val_loss: 391234206111.3609\n",
      "Epoch 1169/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 339759080295.2932 - val_loss: 375659056975.7164\n",
      "Epoch 1170/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 338462644607.7839 - val_loss: 371507187512.6729\n",
      "Epoch 1171/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 340361632284.5245 - val_loss: 371954496768.0720\n",
      "Epoch 1172/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 338168936578.2330 - val_loss: 366572985099.4498\n",
      "Epoch 1173/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 336174593191.6894 - val_loss: 377730096181.2883\n",
      "Epoch 1174/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 335993838025.5441 - val_loss: 371567633207.2326\n",
      "Epoch 1175/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 337757337813.2133 - val_loss: 371829349747.8661\n",
      "Epoch 1176/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 342376688052.7991 - val_loss: 367440423391.8830\n",
      "Epoch 1177/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 340314227388.7226 - val_loss: 365640360569.6990\n",
      "Epoch 1178/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 338840257765.9246 - val_loss: 370659430436.8698\n",
      "Epoch 1179/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 338987980586.7867 - val_loss: 369544925406.9468\n",
      "Epoch 1180/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 333829616143.8469 - val_loss: 368818217678.3843\n",
      "Epoch 1181/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 337918493437.2628 - val_loss: 365769194863.8335\n",
      "Epoch 1182/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 336774703485.4789 - val_loss: 367041179369.7485\n",
      "Epoch 1183/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 336847129985.5126 - val_loss: 369156874495.7840\n",
      "Epoch 1184/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 341279197542.4288 - val_loss: 363895033581.2051\n",
      "Epoch 1185/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 336595960264.3917 - val_loss: 371983920264.8214\n",
      "Epoch 1186/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 334186494425.6792 - val_loss: 364339640152.9339\n",
      "Epoch 1187/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 335056267691.5791 - val_loss: 364403357714.4349\n",
      "Epoch 1188/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 335027586324.0247 - val_loss: 382030269354.4506\n",
      "Epoch 1189/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 334457720571.5340 - val_loss: 385310415895.9077\n",
      "Epoch 1190/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 336691440735.0816 - val_loss: 369239266574.4742\n",
      "Epoch 1191/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 331375172456.4457 - val_loss: 366779905277.7676\n",
      "Epoch 1192/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 332049165222.6809 - val_loss: 362746891366.8321\n",
      "Epoch 1193/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 332735004884.0607 - val_loss: 363370290814.5958\n",
      "Epoch 1194/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 335173194177.4767 - val_loss: 363371685140.2352\n",
      "Epoch 1195/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 331489042584.1306 - val_loss: 367443279787.3148\n",
      "Epoch 1196/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 337291347948.4074 - val_loss: 362527701107.2180\n",
      "Epoch 1197/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 336296676128.9904 - val_loss: 362166962848.8731\n",
      "Epoch 1198/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 330655211729.7557 - val_loss: 367085635390.4338\n",
      "Epoch 1199/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 332159637306.9218 - val_loss: 392164841676.5120\n",
      "Epoch 1200/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 339359360695.5363 - val_loss: 362890730740.5502\n",
      "Epoch 1201/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 333644241316.6640 - val_loss: 360134440100.4737\n",
      "Epoch 1202/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 334383968147.6646 - val_loss: 371883635353.3840\n",
      "Epoch 1203/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 335326983962.6517 - val_loss: 365363480526.4562\n",
      "Epoch 1204/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 328982821095.6533 - val_loss: 362414518606.1322\n",
      "Epoch 1205/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 330600600268.8576 - val_loss: 363307561802.2436\n",
      "Epoch 1206/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 332007833406.3793 - val_loss: 361246358699.9629\n",
      "Epoch 1207/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 331956778510.1182 - val_loss: 359621806334.6318\n",
      "Epoch 1208/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 333916322125.0737 - val_loss: 358412777264.6076\n",
      "Epoch 1209/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 332581815147.9032 - val_loss: 364278436109.0341\n",
      "Epoch 1210/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 333502117594.1115 - val_loss: 371492227760.4276\n",
      "Epoch 1211/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 330405431340.9477 - val_loss: 361661907250.4799\n",
      "Epoch 1212/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 329841217164.8936 - val_loss: 363470267549.5606\n",
      "Epoch 1213/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 328281016088.3467 - val_loss: 362393340446.6768\n",
      "Epoch 1214/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 335404931338.2285 - val_loss: 361473472481.4672\n",
      "Epoch 1215/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 334401830174.3973 - val_loss: 361820099255.9167\n",
      "Epoch 1216/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 331007492935.0231 - val_loss: 380844540997.4188\n",
      "Epoch 1217/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 332566243105.5667 - val_loss: 358773404767.3429\n",
      "Epoch 1218/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 327798400667.8762 - val_loss: 369203364630.9716\n",
      "Epoch 1219/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 330973376007.2032 - val_loss: 365695068556.3499\n",
      "Epoch 1220/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 325916845143.0140 - val_loss: 360056235570.5519\n",
      "Epoch 1221/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 328859561423.8829 - val_loss: 359580703082.3606\n",
      "Epoch 1222/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 329524368076.8576 - val_loss: 359393228089.1049\n",
      "Epoch 1223/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 328907589252.8261 - val_loss: 362462160355.0515\n",
      "Epoch 1224/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 325431855563.8492 - val_loss: 370834333743.2394\n",
      "Epoch 1225/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 325990755865.6432 - val_loss: 359764572876.0799\n",
      "Epoch 1226/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 327911568948.1508 - val_loss: 361688528942.6633\n",
      "Epoch 1227/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 326639016955.9662 - val_loss: 356071816979.8031\n",
      "Epoch 1228/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 326018868854.9960 - val_loss: 358270669363.4160\n",
      "Epoch 1229/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 328210078199.6443 - val_loss: 360237070951.2642\n",
      "Epoch 1230/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 328246577113.9674 - val_loss: 358978940921.6630\n",
      "Epoch 1231/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 328907661625.4811 - val_loss: 363299238904.7989\n",
      "Epoch 1232/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 329414709171.9347 - val_loss: 355713086397.4616\n",
      "Epoch 1233/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 326374496095.8019 - val_loss: 357709524487.9213\n",
      "Epoch 1234/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 327043832864.8464 - val_loss: 355477068569.8521\n",
      "Epoch 1235/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 327032620308.0247 - val_loss: 356010852212.5862\n",
      "Epoch 1236/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 328134231076.3038 - val_loss: 364969450885.1488\n",
      "Epoch 1237/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 325439107009.7648 - val_loss: 357307944730.1401\n",
      "Epoch 1238/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 329086220224.6123 - val_loss: 353946448610.5474\n",
      "Epoch 1239/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 325613119952.4592 - val_loss: 366845325762.5024\n",
      "Epoch 1240/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 327432910249.8503 - val_loss: 356993607766.4135\n",
      "Epoch 1241/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 324566949809.6297 - val_loss: 357046825035.4678\n",
      "Epoch 1242/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 327647735100.3624 - val_loss: 354037913103.9865\n",
      "Epoch 1243/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 327252876975.4688 - val_loss: 352890198490.1221\n",
      "Epoch 1244/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 327660742226.1159 - val_loss: 354550419216.9227\n",
      "Epoch 1245/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 325569889496.0945 - val_loss: 354843913741.6821\n",
      "Epoch 1246/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 323550116560.3152 - val_loss: 352742385321.2264\n",
      "Epoch 1247/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 326964732351.1716 - val_loss: 370085653245.0475\n",
      "Epoch 1248/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 321144167846.9691 - val_loss: 380323024318.7578\n",
      "Epoch 1249/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 322779761044.5290 - val_loss: 354526168198.5170\n",
      "Epoch 1250/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 320697802361.9494 - val_loss: 356480297539.5466\n",
      "Epoch 1251/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 324069669441.4046 - val_loss: 363109303788.2689\n",
      "Epoch 1252/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 323058510371.4395 - val_loss: 350222242382.4922\n",
      "Epoch 1253/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 321935548796.3264 - val_loss: 351905465079.8627\n",
      "Epoch 1254/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 324513261407.8019 - val_loss: 352394181255.5251\n",
      "Epoch 1255/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 321485063038.3433 - val_loss: 355834855218.9119\n",
      "Epoch 1256/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 328068508997.5825 - val_loss: 350625128124.8135\n",
      "Epoch 1257/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 324662185649.7737 - val_loss: 360729178296.9249\n",
      "Epoch 1258/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 321845662947.6196 - val_loss: 353783454753.1252\n",
      "Epoch 1259/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 322244748384.2341 - val_loss: 363831824836.2307\n",
      "Epoch 1260/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 76us/step - loss: 326566379879.5814 - val_loss: 366943608990.7128\n",
      "Epoch 1261/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 321920263573.1052 - val_loss: 350718148921.3929\n",
      "Epoch 1262/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 322648066023.7974 - val_loss: 349973840106.4686\n",
      "Epoch 1263/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 324918281279.9640 - val_loss: 351895566287.6085\n",
      "Epoch 1264/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 321651829375.6398 - val_loss: 352055609533.5336\n",
      "Epoch 1265/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 321099303076.8080 - val_loss: 360759417719.1786\n",
      "Epoch 1266/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 321232836770.5031 - val_loss: 371779088751.5454\n",
      "Epoch 1267/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 329071608960.5042 - val_loss: 349470139155.5151\n",
      "Epoch 1268/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 319241936771.5295 - val_loss: 351011369440.4590\n",
      "Epoch 1269/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 323590050259.9167 - val_loss: 349271464944.7336\n",
      "Epoch 1270/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 319234225319.6894 - val_loss: 353382027952.4276\n",
      "Epoch 1271/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 322621899563.3630 - val_loss: 362070883231.5049\n",
      "Epoch 1272/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 319316160740.1959 - val_loss: 353935561781.8644\n",
      "Epoch 1273/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 318117241076.3309 - val_loss: 354434106158.5913\n",
      "Epoch 1274/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 320708285510.8790 - val_loss: 348665481495.9797\n",
      "Epoch 1275/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 322357369437.0648 - val_loss: 360030854627.6276\n",
      "Epoch 1276/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 320605553091.7816 - val_loss: 348305373910.4495\n",
      "Epoch 1277/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 317928024694.4198 - val_loss: 352239151890.9390\n",
      "Epoch 1278/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 324569746006.1497 - val_loss: 349466440476.1564\n",
      "Epoch 1279/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 317586335138.9352 - val_loss: 348858536483.8616\n",
      "Epoch 1280/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 316507129080.3647 - val_loss: 362314039216.4996\n",
      "Epoch 1281/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 319676154778.0034 - val_loss: 353303892984.2228\n",
      "Epoch 1282/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 318079122497.1165 - val_loss: 351298727609.3569\n",
      "Epoch 1283/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 317947122509.9381 - val_loss: 345706669074.1469\n",
      "Epoch 1284/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 322698524958.9736 - val_loss: 347708812854.0084\n",
      "Epoch 1285/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 316141233948.9567 - val_loss: 350117588236.1699\n",
      "Epoch 1286/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 319363721025.8369 - val_loss: 346618328578.1603\n",
      "Epoch 1287/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 319965451945.1300 - val_loss: 358014107291.9764\n",
      "Epoch 1288/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 317477820707.5836 - val_loss: 347003316597.3063\n",
      "Epoch 1289/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 319152479536.8374 - val_loss: 353081051430.3820\n",
      "Epoch 1290/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 320040501672.1216 - val_loss: 347261937710.0872\n",
      "Epoch 1291/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 317937782173.1727 - val_loss: 350616456065.5482\n",
      "Epoch 1292/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 318526351345.0175 - val_loss: 345981498854.5080\n",
      "Epoch 1293/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 315887427671.5903 - val_loss: 346220351119.8785\n",
      "Epoch 1294/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 319752099052.8397 - val_loss: 350675611736.1418\n",
      "Epoch 1295/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 317761294720.9364 - val_loss: 349156893227.3508\n",
      "Epoch 1296/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 315562212361.7963 - val_loss: 345823593294.2762\n",
      "Epoch 1297/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 315116247949.3258 - val_loss: 347727978632.5333\n",
      "Epoch 1298/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 315776965323.7051 - val_loss: 344919424203.6478\n",
      "Epoch 1299/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 318266388233.3641 - val_loss: 343776429690.5632\n",
      "Epoch 1300/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 317038494267.6421 - val_loss: 351360894186.1806\n",
      "Epoch 1301/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 321728006474.7687 - val_loss: 349625449223.9932\n",
      "Epoch 1302/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 314234103313.5757 - val_loss: 364412487723.2067\n",
      "Epoch 1303/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 314821973348.7001 - val_loss: 360959692399.0413\n",
      "Epoch 1304/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 315099066498.2330 - val_loss: 344481807051.5038\n",
      "Epoch 1305/7000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 312959638219.7051 - val_loss: 345509758717.3356\n",
      "Epoch 1306/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 315367045296.3331 - val_loss: 349284531916.3679\n",
      "Epoch 1307/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 315873046281.3641 - val_loss: 349335044072.9564\n",
      "Epoch 1308/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 317118912692.9432 - val_loss: 350141720156.0304\n",
      "Epoch 1309/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 315557530447.0906 - val_loss: 348791005179.3913\n",
      "Epoch 1310/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 318520823363.1334 - val_loss: 346380618463.6669\n",
      "Epoch 1311/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 316541582832.7293 - val_loss: 342018325824.5941\n",
      "Epoch 1312/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 311874313838.9285 - val_loss: 341954203913.0014\n",
      "Epoch 1313/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 311305338777.4271 - val_loss: 361845940154.8692\n",
      "Epoch 1314/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 318306690439.2751 - val_loss: 349347157260.7460\n",
      "Epoch 1315/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 313657089752.9589 - val_loss: 358565149150.1547\n",
      "Epoch 1316/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 313607519702.7980 - val_loss: 346726513346.8624\n",
      "Epoch 1317/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 313803858201.2110 - val_loss: 343273409219.4385\n",
      "Epoch 1318/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 316914878024.8959 - val_loss: 341374283268.4647\n",
      "Epoch 1319/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 311840662596.5740 - val_loss: 342910736006.6610\n",
      "Epoch 1320/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 313910498125.9381 - val_loss: 348482978229.8284\n",
      "Epoch 1321/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 311395461814.9600 - val_loss: 340419266984.0023\n",
      "Epoch 1322/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 314597686420.0968 - val_loss: 340532832256.0000\n",
      "Epoch 1323/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 315654915112.9139 - val_loss: 344761627963.4093\n",
      "Epoch 1324/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 313592533183.3157 - val_loss: 343611446260.7662\n",
      "Epoch 1325/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 312627990382.7845 - val_loss: 341672739051.6208\n",
      "Epoch 1326/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 312509530121.2200 - val_loss: 343672540032.9722\n",
      "Epoch 1327/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 312047743088.9454 - val_loss: 348147566382.8793\n",
      "Epoch 1328/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 315474929229.5059 - val_loss: 341638648956.7235\n",
      "Epoch 1329/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 309788441572.9161 - val_loss: 343154995142.3910\n",
      "Epoch 1330/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 312051597293.5599 - val_loss: 344454920968.5693\n",
      "Epoch 1331/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 309635682126.5143 - val_loss: 349631949376.6661\n",
      "Epoch 1332/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 308260867485.1728 - val_loss: 343842525016.3578\n",
      "Epoch 1333/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 311228600778.6967 - val_loss: 341570766260.9643\n",
      "Epoch 1334/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 308853658639.5588 - val_loss: 342734794548.0641\n",
      "Epoch 1335/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 312854600460.2454 - val_loss: 341728319123.9111\n",
      "Epoch 1336/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 316401700963.1154 - val_loss: 340453157963.4678\n",
      "Epoch 1337/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 309915850527.2617 - val_loss: 345634087965.3806\n",
      "Epoch 1338/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 307669259463.9595 - val_loss: 344705442416.4816\n",
      "Epoch 1339/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 310406570707.7726 - val_loss: 374365622804.3072\n",
      "Epoch 1340/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 314694604727.3922 - val_loss: 344224148544.8101\n",
      "Epoch 1341/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 306306907779.6736 - val_loss: 349986895219.8661\n",
      "Epoch 1342/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 310519394675.1064 - val_loss: 342481947588.3747\n",
      "Epoch 1343/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 311749908826.9037 - val_loss: 336938485690.8692\n",
      "Epoch 1344/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 308948676499.6646 - val_loss: 338158475412.6312\n",
      "Epoch 1345/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 306357069085.2448 - val_loss: 337247077226.2166\n",
      "Epoch 1346/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 304812630656.7924 - val_loss: 343842942156.8000\n",
      "Epoch 1347/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 310192904446.7034 - val_loss: 337619719767.9977\n",
      "Epoch 1348/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 308014578836.0968 - val_loss: 343427368127.5499\n",
      "Epoch 1349/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 312254916588.4075 - val_loss: 332941823285.6484\n",
      "Epoch 1350/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 303124543263.2617 - val_loss: 348686744375.2326\n",
      "Epoch 1351/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 303890989498.5616 - val_loss: 334789630597.2208\n",
      "Epoch 1352/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 301544410110.2712 - val_loss: 336458395882.4686\n",
      "Epoch 1353/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 303379879176.4997 - val_loss: 331696658710.5396\n",
      "Epoch 1354/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 298739904431.3247 - val_loss: 336833412384.9091\n",
      "Epoch 1355/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 303940548931.8537 - val_loss: 339045386849.5032\n",
      "Epoch 1356/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 301720744085.8256 - val_loss: 327951481862.0490\n",
      "Epoch 1357/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 301173490530.6832 - val_loss: 328254280612.1136\n",
      "Epoch 1358/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 297064524959.6218 - val_loss: 330786639039.2619\n",
      "Epoch 1359/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 298980016496.8013 - val_loss: 327708352726.3055\n",
      "Epoch 1360/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 301150401551.5588 - val_loss: 325970859724.3679\n",
      "Epoch 1361/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 295017594926.1002 - val_loss: 324854119570.0388\n",
      "Epoch 1362/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 294139165927.0771 - val_loss: 325740953037.1600\n",
      "Epoch 1363/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 292012577438.7574 - val_loss: 327060168149.2253\n",
      "Epoch 1364/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 295304655163.7862 - val_loss: 321540800346.0861\n",
      "Epoch 1365/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 296197163135.9280 - val_loss: 358717413526.3595\n",
      "Epoch 1366/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 295966144420.9521 - val_loss: 319733802242.9525\n",
      "Epoch 1367/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 289195707740.6325 - val_loss: 323751115636.2982\n",
      "Epoch 1368/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 289931706335.7299 - val_loss: 332570520144.7966\n",
      "Epoch 1369/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 289623932503.8784 - val_loss: 321384209074.7319\n",
      "Epoch 1370/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 288855797891.3855 - val_loss: 323881448588.8540\n",
      "Epoch 1371/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 290894211176.8779 - val_loss: 335787873303.3317\n",
      "Epoch 1372/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 288980339507.4305 - val_loss: 312911064706.9164\n",
      "Epoch 1373/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 287443839486.5594 - val_loss: 316948972366.8523\n",
      "Epoch 1374/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 286342727752.6078 - val_loss: 319237143752.7674\n",
      "Epoch 1375/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 285264731528.4277 - val_loss: 311660025239.0076\n",
      "Epoch 1376/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 282572383171.4935 - val_loss: 313464420731.0672\n",
      "Epoch 1377/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 280956246708.6550 - val_loss: 308023085721.9601\n",
      "Epoch 1378/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 284577148211.1424 - val_loss: 315445281992.7674\n",
      "Epoch 1379/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 278127720366.1722 - val_loss: 316558777904.8236\n",
      "Epoch 1380/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 277694013444.6100 - val_loss: 307904540116.6492\n",
      "Epoch 1381/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 277129111970.3591 - val_loss: 304337199627.0897\n",
      "Epoch 1382/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 277916095102.4874 - val_loss: 312408036713.4965\n",
      "Epoch 1383/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 274719686812.7406 - val_loss: 304993062784.1080\n",
      "Epoch 1384/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 273895044890.6516 - val_loss: 311244311206.0579\n",
      "Epoch 1385/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 273292426802.9983 - val_loss: 299586662435.1415\n",
      "Epoch 1386/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 84us/step - loss: 272341763394.1249 - val_loss: 301406865379.1955\n",
      "Epoch 1387/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 275382929683.4485 - val_loss: 301832279471.7795\n",
      "Epoch 1388/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 268289624069.7625 - val_loss: 319264119961.5280\n",
      "Epoch 1389/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 269701228003.4755 - val_loss: 296240593812.5592\n",
      "Epoch 1390/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 265054909082.1474 - val_loss: 294775296471.8177\n",
      "Epoch 1391/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 270077155075.6016 - val_loss: 293700401955.0695\n",
      "Epoch 1392/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 266579919991.2842 - val_loss: 298698592078.5643\n",
      "Epoch 1393/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 270848995720.7158 - val_loss: 293026348061.0925\n",
      "Epoch 1394/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 264206216259.4215 - val_loss: 306592091561.4425\n",
      "Epoch 1395/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 268834326646.7079 - val_loss: 301222292283.8414\n",
      "Epoch 1396/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 263773651939.7636 - val_loss: 297131560179.3980\n",
      "Epoch 1397/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 265939619800.8148 - val_loss: 299025343594.8647\n",
      "Epoch 1398/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 264932521169.1795 - val_loss: 287325608483.2855\n",
      "Epoch 1399/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 258568947074.6652 - val_loss: 286884895097.9150\n",
      "Epoch 1400/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 258038469527.6984 - val_loss: 286771763960.4388\n",
      "Epoch 1401/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 258572466167.9325 - val_loss: 288352245471.3789\n",
      "Epoch 1402/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 261381180086.9600 - val_loss: 282613881990.5170\n",
      "Epoch 1403/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 253288344403.7006 - val_loss: 286599970803.3260\n",
      "Epoch 1404/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 254415801766.3928 - val_loss: 281750643050.3606\n",
      "Epoch 1405/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 252857950203.3900 - val_loss: 298833478730.6036\n",
      "Epoch 1406/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 252504896891.7502 - val_loss: 280200090982.0399\n",
      "Epoch 1407/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 254365153271.9325 - val_loss: 280350465482.8557\n",
      "Epoch 1408/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 248620131663.9550 - val_loss: 275684414256.3196\n",
      "Epoch 1409/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 250122553628.6685 - val_loss: 285524720473.2219\n",
      "Epoch 1410/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 249446864851.0523 - val_loss: 274913379270.1030\n",
      "Epoch 1411/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 249691340920.1486 - val_loss: 273827648455.2551\n",
      "Epoch 1412/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 247274495820.2093 - val_loss: 273115999056.0045\n",
      "Epoch 1413/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 249423367341.4519 - val_loss: 271915701464.3218\n",
      "Epoch 1414/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 249224631560.4997 - val_loss: 287906190356.4512\n",
      "Epoch 1415/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 243715441711.8289 - val_loss: 270035622289.5347\n",
      "Epoch 1416/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 243019016499.7186 - val_loss: 279837581314.3044\n",
      "Epoch 1417/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 244516740949.4294 - val_loss: 267545611286.7556\n",
      "Epoch 1418/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 239407015377.0355 - val_loss: 292853700352.7921\n",
      "Epoch 1419/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 240608186453.2853 - val_loss: 271471729404.4714\n",
      "Epoch 1420/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 240192851449.3731 - val_loss: 265208432707.9786\n",
      "Epoch 1421/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 238330374958.8205 - val_loss: 271681949059.9966\n",
      "Epoch 1422/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 235287694020.7901 - val_loss: 277718738482.5519\n",
      "Epoch 1423/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 243292878655.5318 - val_loss: 268041494339.0425\n",
      "Epoch 1424/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 242497913735.5633 - val_loss: 298076121340.9035\n",
      "Epoch 1425/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 242305103670.3118 - val_loss: 264632755150.1682\n",
      "Epoch 1426/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 236631530021.7445 - val_loss: 268621152281.9240\n",
      "Epoch 1427/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 233677480622.8925 - val_loss: 261339460820.2892\n",
      "Epoch 1428/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 233092655757.4699 - val_loss: 268587570286.8973\n",
      "Epoch 1429/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 239225808982.4378 - val_loss: 316012979852.4219\n",
      "Epoch 1430/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 250422611569.2336 - val_loss: 259625797229.8892\n",
      "Epoch 1431/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 227928310446.8925 - val_loss: 261728118340.4107\n",
      "Epoch 1432/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 226834933423.7569 - val_loss: 263251928163.3755\n",
      "Epoch 1433/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 228319618375.8875 - val_loss: 256536673862.1389\n",
      "Epoch 1434/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 226770686971.9662 - val_loss: 253779446302.6768\n",
      "Epoch 1435/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 227196785937.7198 - val_loss: 252568666730.7207\n",
      "Epoch 1436/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 227330453918.3253 - val_loss: 262156303621.8329\n",
      "Epoch 1437/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 225063920247.5723 - val_loss: 260539329406.0917\n",
      "Epoch 1438/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 226451934658.0529 - val_loss: 251021749160.1463\n",
      "Epoch 1439/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 223967025037.9021 - val_loss: 253239343055.0323\n",
      "Epoch 1440/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 225949713665.5847 - val_loss: 259358767422.5778\n",
      "Epoch 1441/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 225974618317.2898 - val_loss: 252351929010.7319\n",
      "Epoch 1442/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 223983059893.6635 - val_loss: 248228993621.9814\n",
      "Epoch 1443/7000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 222052790145.2245 - val_loss: 249575339904.6841\n",
      "Epoch 1444/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 220025526044.9567 - val_loss: 261061371452.9215\n",
      "Epoch 1445/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 220588454793.8683 - val_loss: 252002707292.9665\n",
      "Epoch 1446/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 219728375349.3033 - val_loss: 247990682631.2011\n",
      "Epoch 1447/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 225428685832.6438 - val_loss: 246131806439.0121\n",
      "Epoch 1448/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 218334923926.4018 - val_loss: 242729688090.2121\n",
      "Epoch 1449/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 216164136329.5802 - val_loss: 247510167618.8264\n",
      "Epoch 1450/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 217061944977.5037 - val_loss: 242791515559.1381\n",
      "Epoch 1451/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 216832780982.9600 - val_loss: 244663464995.4295\n",
      "Epoch 1452/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 220008686869.1773 - val_loss: 245603631435.8279\n",
      "Epoch 1453/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 215662080237.4159 - val_loss: 242647319244.6560\n",
      "Epoch 1454/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 214305772849.9899 - val_loss: 240195189593.2220\n",
      "Epoch 1455/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 215568039405.8481 - val_loss: 255817288529.7328\n",
      "Epoch 1456/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 214343176004.1418 - val_loss: 239364863783.9662\n",
      "Epoch 1457/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 211236831314.9803 - val_loss: 237242048897.9803\n",
      "Epoch 1458/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 212094026779.0838 - val_loss: 242378615888.9406\n",
      "Epoch 1459/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 211952450244.7901 - val_loss: 238164644855.6467\n",
      "Epoch 1460/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 209860070813.1728 - val_loss: 252092898103.2326\n",
      "Epoch 1461/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 215451506315.7411 - val_loss: 236702495027.0560\n",
      "Epoch 1462/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 211842141763.1334 - val_loss: 239200370236.3454\n",
      "Epoch 1463/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 211454152365.1638 - val_loss: 240766843753.9286\n",
      "Epoch 1464/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 210674988416.3602 - val_loss: 237189983508.2352\n",
      "Epoch 1465/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 208728134441.0580 - val_loss: 255446607061.4413\n",
      "Epoch 1466/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 214931048870.9691 - val_loss: 240495894313.6945\n",
      "Epoch 1467/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 206434039567.1266 - val_loss: 235088809835.3688\n",
      "Epoch 1468/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 206010646785.0084 - val_loss: 239239551713.9713\n",
      "Epoch 1469/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 209730726472.3196 - val_loss: 234094991463.6962\n",
      "Epoch 1470/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 204062557616.7653 - val_loss: 234525596061.0565\n",
      "Epoch 1471/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 208242237924.0518 - val_loss: 230100276405.1803\n",
      "Epoch 1472/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 208285383781.9966 - val_loss: 234357888029.0926\n",
      "Epoch 1473/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 204772119636.7090 - val_loss: 239840880713.1634\n",
      "Epoch 1474/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 205808464980.7090 - val_loss: 240998060469.8284\n",
      "Epoch 1475/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 205742633432.5267 - val_loss: 230755398409.7215\n",
      "Epoch 1476/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 205355440816.0450 - val_loss: 233996934959.4554\n",
      "Epoch 1477/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 207386354618.2735 - val_loss: 229236089309.0025\n",
      "Epoch 1478/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 199206064171.7952 - val_loss: 242012230476.5480\n",
      "Epoch 1479/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 203581674317.9381 - val_loss: 228449090338.4934\n",
      "Epoch 1480/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 202281199739.3180 - val_loss: 234683278578.2458\n",
      "Epoch 1481/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 205222647297.4406 - val_loss: 271482312634.2931\n",
      "Epoch 1482/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 204844946565.1143 - val_loss: 229331206520.7629\n",
      "Epoch 1483/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 199111840861.9291 - val_loss: 227484283375.4374\n",
      "Epoch 1484/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 201923753413.5104 - val_loss: 247064149570.1063\n",
      "Epoch 1485/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 201852363973.6544 - val_loss: 245802276143.3114\n",
      "Epoch 1486/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 201678323722.9488 - val_loss: 222281023475.3260\n",
      "Epoch 1487/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 200596614732.3534 - val_loss: 223074036931.5826\n",
      "Epoch 1488/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 198528757614.7845 - val_loss: 221445923479.6557\n",
      "Epoch 1489/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 198816143135.2617 - val_loss: 224069150116.8337\n",
      "Epoch 1490/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 197320473240.9949 - val_loss: 228471227388.8315\n",
      "Epoch 1491/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 197807504061.8751 - val_loss: 229007313125.8599\n",
      "Epoch 1492/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 198912121427.8447 - val_loss: 221600960486.6520\n",
      "Epoch 1493/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 194962781858.7912 - val_loss: 224271851253.8464\n",
      "Epoch 1494/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 196242986801.7018 - val_loss: 221578677509.2568\n",
      "Epoch 1495/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 197134034375.8154 - val_loss: 221441838447.2574\n",
      "Epoch 1496/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 199537839655.4733 - val_loss: 223371193206.6025\n",
      "Epoch 1497/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 199924301261.5779 - val_loss: 221205445204.2532\n",
      "Epoch 1498/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 195531941808.4772 - val_loss: 233295992349.8127\n",
      "Epoch 1499/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 194557424717.7940 - val_loss: 223018825433.6180\n",
      "Epoch 1500/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 194695653766.6989 - val_loss: 222169839940.3387\n",
      "Epoch 1501/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 194665345053.9651 - val_loss: 244577571806.8748\n",
      "Epoch 1502/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 198678921050.0394 - val_loss: 225797277519.1404\n",
      "Epoch 1503/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 195915415441.3596 - val_loss: 220403472522.8377\n",
      "Epoch 1504/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 192368810886.6044 - val_loss: 216536063213.0610\n",
      "Epoch 1505/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 192514601026.8453 - val_loss: 221832917874.5699\n",
      "Epoch 1506/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 191910494659.7817 - val_loss: 225686030285.8802\n",
      "Epoch 1507/7000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 191495019740.7046 - val_loss: 216973553723.9134\n",
      "Epoch 1508/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 195934246150.1947 - val_loss: 225797761772.3409\n",
      "Epoch 1509/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 190065132266.8227 - val_loss: 224603944132.1587\n",
      "Epoch 1510/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 190855171365.3123 - val_loss: 213798415508.3432\n",
      "Epoch 1511/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 193688244857.8773 - val_loss: 217805212009.2084\n",
      "Epoch 1512/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 82us/step - loss: 191284079618.8813 - val_loss: 215491273813.5494\n",
      "Epoch 1513/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 190348718478.7664 - val_loss: 214883310981.1488\n",
      "Epoch 1514/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 189910764118.1497 - val_loss: 218134506923.1707\n",
      "Epoch 1515/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 190820446178.0349 - val_loss: 215428331627.7288\n",
      "Epoch 1516/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 187616398546.3320 - val_loss: 214733314950.7331\n",
      "Epoch 1517/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 191698531412.7090 - val_loss: 211643868335.4194\n",
      "Epoch 1518/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 190442540316.0923 - val_loss: 223014520556.0529\n",
      "Epoch 1519/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 191588809931.4170 - val_loss: 212829581547.0447\n",
      "Epoch 1520/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 189979042884.5740 - val_loss: 215142099241.8385\n",
      "Epoch 1521/7000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 190829351084.8756 - val_loss: 212344442762.4776\n",
      "Epoch 1522/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 187970098336.7743 - val_loss: 214158975862.0264\n",
      "Epoch 1523/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 185736899745.9268 - val_loss: 211529057144.3308\n",
      "Epoch 1524/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 188910063364.7541 - val_loss: 212184008116.9643\n",
      "Epoch 1525/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 187877643297.9989 - val_loss: 214538279855.0594\n",
      "Epoch 1526/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 189272410839.2302 - val_loss: 218539123804.4624\n",
      "Epoch 1527/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 194500807779.6916 - val_loss: 209859591605.5404\n",
      "Epoch 1528/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 185324486056.6978 - val_loss: 215653886948.0596\n",
      "Epoch 1529/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 186844923607.8064 - val_loss: 225389281459.7401\n",
      "Epoch 1530/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 187391943462.7530 - val_loss: 248811694949.3198\n",
      "Epoch 1531/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 191939139525.2223 - val_loss: 207516885519.6985\n",
      "Epoch 1532/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 188317639536.5132 - val_loss: 222840186614.4225\n",
      "Epoch 1533/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 186084141400.5988 - val_loss: 207466798722.6284\n",
      "Epoch 1534/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 185139375328.1621 - val_loss: 205290882789.1398\n",
      "Epoch 1535/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 184333735729.7018 - val_loss: 214308835771.5893\n",
      "Epoch 1536/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 185671657583.7929 - val_loss: 215244053477.2118\n",
      "Epoch 1537/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 185246095043.0613 - val_loss: 207042792880.9316\n",
      "Epoch 1538/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 184563538525.0647 - val_loss: 205403488478.3708\n",
      "Epoch 1539/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 187161915200.6843 - val_loss: 210715742405.0228\n",
      "Epoch 1540/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 183420198003.8267 - val_loss: 208262983320.5198\n",
      "Epoch 1541/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 182833418265.3551 - val_loss: 213039755225.9781\n",
      "Epoch 1542/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 182773302852.2859 - val_loss: 210392356874.9457\n",
      "Epoch 1543/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 180652805662.2532 - val_loss: 207398279077.5539\n",
      "Epoch 1544/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 181663736338.1519 - val_loss: 203454243337.9376\n",
      "Epoch 1545/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 186845113245.4609 - val_loss: 206871388872.0473\n",
      "Epoch 1546/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 183932360898.7732 - val_loss: 205315952919.9797\n",
      "Epoch 1547/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 178957039490.3770 - val_loss: 210215499236.2036\n",
      "Epoch 1548/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 182433991604.5110 - val_loss: 208334000202.8917\n",
      "Epoch 1549/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 180072022808.9229 - val_loss: 205280841365.0633\n",
      "Epoch 1550/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 182094683611.4080 - val_loss: 203317579424.5851\n",
      "Epoch 1551/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 179848890283.8672 - val_loss: 202357322316.7640\n",
      "Epoch 1552/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 178861809166.1182 - val_loss: 207742042881.6562\n",
      "Epoch 1553/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 181727640440.5807 - val_loss: 207530865171.1550\n",
      "Epoch 1554/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 180460806637.2718 - val_loss: 202440348381.9387\n",
      "Epoch 1555/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 177570794607.2166 - val_loss: 214999271572.0551\n",
      "Epoch 1556/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 178272028645.4924 - val_loss: 203313761891.5196\n",
      "Epoch 1557/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 177806446903.1761 - val_loss: 204402354510.7083\n",
      "Epoch 1558/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 179060917265.8638 - val_loss: 222596364697.6000\n",
      "Epoch 1559/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 183702049089.5487 - val_loss: 204080441208.6188\n",
      "Epoch 1560/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 177181237722.8317 - val_loss: 198460570258.4709\n",
      "Epoch 1561/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 175921425960.3376 - val_loss: 204420759743.2619\n",
      "Epoch 1562/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 176325258805.3033 - val_loss: 211646911453.7227\n",
      "Epoch 1563/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 175836583379.3405 - val_loss: 199727218012.8225\n",
      "Epoch 1564/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 175821609613.4699 - val_loss: 198948942562.5474\n",
      "Epoch 1565/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 172369973798.8970 - val_loss: 203164717860.2216\n",
      "Epoch 1566/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 180323579942.0327 - val_loss: 210092594323.7671\n",
      "Epoch 1567/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 179498637834.0844 - val_loss: 199544344422.7601\n",
      "Epoch 1568/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 174788766644.5110 - val_loss: 196122522227.9381\n",
      "Epoch 1569/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 173234338770.4761 - val_loss: 205523109761.8363\n",
      "Epoch 1570/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 172209590094.5143 - val_loss: 220221348113.6428\n",
      "Epoch 1571/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 178651575445.2493 - val_loss: 197194582094.9243\n",
      "Epoch 1572/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 170708347284.5290 - val_loss: 190943077303.7007\n",
      "Epoch 1573/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 169510988664.5807 - val_loss: 189601099235.6276\n",
      "Epoch 1574/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 168817296393.7963 - val_loss: 194077837779.2090\n",
      "Epoch 1575/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 77us/step - loss: 169216668175.8470 - val_loss: 186482014103.7277\n",
      "Epoch 1576/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 165481535346.2420 - val_loss: 187101894353.5527\n",
      "Epoch 1577/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 164171155141.3664 - val_loss: 203209186196.5592\n",
      "Epoch 1578/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 166541851141.4744 - val_loss: 185567068974.5913\n",
      "Epoch 1579/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 163589915210.6246 - val_loss: 185531257144.2408\n",
      "Epoch 1580/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 161536575901.7490 - val_loss: 182141895187.1550\n",
      "Epoch 1581/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 159338622886.1047 - val_loss: 187770873563.9224\n",
      "Epoch 1582/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 166999441850.5616 - val_loss: 181230962872.9249\n",
      "Epoch 1583/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 158316377557.6454 - val_loss: 185570073377.3412\n",
      "Epoch 1584/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 155033337286.6629 - val_loss: 179554624145.8948\n",
      "Epoch 1585/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 155164301935.5048 - val_loss: 175831331044.1317\n",
      "Epoch 1586/7000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 156114488184.75 - 0s 71us/step - loss: 155753361836.7316 - val_loss: 173655516369.9848\n",
      "Epoch 1587/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 153130440678.6449 - val_loss: 175056125201.9308\n",
      "Epoch 1588/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 149061585973.5915 - val_loss: 170633280499.0380\n",
      "Epoch 1589/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 150462696095.9100 - val_loss: 171210158096.1305\n",
      "Epoch 1590/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 146958087184.2791 - val_loss: 169030837097.3524\n",
      "Epoch 1591/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 143916579384.1846 - val_loss: 162477914536.5783\n",
      "Epoch 1592/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 142995054144.8284 - val_loss: 164872830159.1044\n",
      "Epoch 1593/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 139934330024.8419 - val_loss: 159418010477.3851\n",
      "Epoch 1594/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 135434478785.0444 - val_loss: 161821253227.5848\n",
      "Epoch 1595/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 132260606885.5284 - val_loss: 161820658090.8827\n",
      "Epoch 1596/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 132073745650.6021 - val_loss: 155528347788.5659\n",
      "Epoch 1597/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 126120936797.7850 - val_loss: 157329153644.7370\n",
      "Epoch 1598/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 127803753828.1238 - val_loss: 138860011287.2596\n",
      "Epoch 1599/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 119150798934.4378 - val_loss: 139349549860.2216\n",
      "Epoch 1600/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 116730168078.5504 - val_loss: 136536426797.2951\n",
      "Epoch 1601/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 110665210106.0934 - val_loss: 131876739904.4501\n",
      "Epoch 1602/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 108117451134.6314 - val_loss: 130347003962.1851\n",
      "Epoch 1603/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 105015842397.6410 - val_loss: 120188069235.8661\n",
      "Epoch 1604/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 103764256072.4637 - val_loss: 122801273769.2984\n",
      "Epoch 1605/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 97163032770.4851 - val_loss: 116296400077.9522\n",
      "Epoch 1606/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 92637145017.6972 - val_loss: 106821563554.4574\n",
      "Epoch 1607/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 93969473715.7906 - val_loss: 105104795150.8343\n",
      "Epoch 1608/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 92926097243.1919 - val_loss: 103384260709.9679\n",
      "Epoch 1609/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 87733486729.7243 - val_loss: 99277700322.6914\n",
      "Epoch 1610/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 84636131203.5295 - val_loss: 98217575519.9190\n",
      "Epoch 1611/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 81206171275.7411 - val_loss: 95117764426.5316\n",
      "Epoch 1612/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 83514816961.4766 - val_loss: 92859226840.1778\n",
      "Epoch 1613/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 79485283625.9223 - val_loss: 95949811734.4675\n",
      "Epoch 1614/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 82690768504.1486 - val_loss: 90434864701.4976\n",
      "Epoch 1615/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 78700504010.4086 - val_loss: 91005286366.0107\n",
      "Epoch 1616/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 76378975990.9240 - val_loss: 99701233982.5778\n",
      "Epoch 1617/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 78032188323.2234 - val_loss: 90832516859.0312\n",
      "Epoch 1618/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 75769611911.1311 - val_loss: 86013057540.1767\n",
      "Epoch 1619/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 74068899355.9482 - val_loss: 85382347836.2014\n",
      "Epoch 1620/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 73995258074.9758 - val_loss: 85546247964.4444\n",
      "Epoch 1621/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 73950367797.0152 - val_loss: 85213291061.1443\n",
      "Epoch 1622/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 74065819915.3810 - val_loss: 83968043382.4585\n",
      "Epoch 1623/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 71784972209.6297 - val_loss: 84889972989.4796\n",
      "Epoch 1624/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 76063320035.1874 - val_loss: 89609140787.9921\n",
      "Epoch 1625/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 73401840789.2493 - val_loss: 88045213359.5634\n",
      "Epoch 1626/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 71854903074.7192 - val_loss: 86578916999.2371\n",
      "Epoch 1627/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 71407088125.4069 - val_loss: 82765619690.2526\n",
      "Epoch 1628/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 73287222474.2645 - val_loss: 93398349553.8138\n",
      "Epoch 1629/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 70711979269.0422 - val_loss: 82316884495.1224\n",
      "Epoch 1630/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 72572563034.1835 - val_loss: 81941864126.5418\n",
      "Epoch 1631/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 68801249584.2611 - val_loss: 85553436148.9103\n",
      "Epoch 1632/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 71162563831.2122 - val_loss: 91454101243.3193\n",
      "Epoch 1633/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 69669950682.3996 - val_loss: 79894662099.9291\n",
      "Epoch 1634/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 69099474937.0850 - val_loss: 84039511151.4734\n",
      "Epoch 1635/7000\n",
      "3554/3554 [==============================] - 0s 115us/step - loss: 72605339343.7389 - val_loss: 82458981053.6776\n",
      "Epoch 1636/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 69371491200.6483 - val_loss: 82434676149.5404\n",
      "Epoch 1637/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 69292215939.6736 - val_loss: 78084846807.1696\n",
      "Epoch 1638/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 68205047153.3776 - val_loss: 82732964022.6205\n",
      "Epoch 1639/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 69499868013.0557 - val_loss: 86361943477.5404\n",
      "Epoch 1640/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 67728795241.1660 - val_loss: 77601267998.8928\n",
      "Epoch 1641/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 67422602184.1035 - val_loss: 79148154311.9753\n",
      "Epoch 1642/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 69367046551.9865 - val_loss: 78144347962.9772\n",
      "Epoch 1643/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 66759556629.0332 - val_loss: 87891579622.0039\n",
      "Epoch 1644/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 67752578166.1317 - val_loss: 76251915711.9100\n",
      "Epoch 1645/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 67833412930.1249 - val_loss: 77161113493.4233\n",
      "Epoch 1646/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 66911989874.0979 - val_loss: 82601908692.3612\n",
      "Epoch 1647/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 66295677859.2234 - val_loss: 79551854655.6579\n",
      "Epoch 1648/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 65792502551.7704 - val_loss: 90415686756.0236\n",
      "Epoch 1649/7000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 65225760281.6432 - val_loss: 91748261523.3350\n",
      "Epoch 1650/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 67759989487.4328 - val_loss: 79555365402.6442\n",
      "Epoch 1651/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 64653944107.0748 - val_loss: 77256602632.3533\n",
      "Epoch 1652/7000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 64795785116.582 - 0s 100us/step - loss: 64689239963.7321 - val_loss: 75060954503.4532\n",
      "Epoch 1653/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 65201933301.0512 - val_loss: 78677575236.9868\n",
      "Epoch 1654/7000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 66135184173.6680 - val_loss: 74556986792.5783\n",
      "Epoch 1655/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 64507783176.7878 - val_loss: 76762169781.8284\n",
      "Epoch 1656/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 64740373480.9499 - val_loss: 77584742020.3567\n",
      "Epoch 1657/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 62556773643.9572 - val_loss: 78435266518.5215\n",
      "Epoch 1658/7000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 63346365916.5605 - val_loss: 84135636101.3648\n",
      "Epoch 1659/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 64294432905.1480 - val_loss: 76141647771.1842\n",
      "Epoch 1660/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 66317398177.3506 - val_loss: 78365840702.2897\n",
      "Epoch 1661/7000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 64601085579.7411 - val_loss: 73815290402.1333\n",
      "Epoch 1662/7000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 62474975658.1384 - val_loss: 95412648101.3378\n",
      "Epoch 1663/7000\n",
      "3554/3554 [==============================] - 0s 109us/step - loss: 63763040538.3635 - val_loss: 72795217153.2242\n",
      "Epoch 1664/7000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 63290354296.4367 - val_loss: 77074822980.7708\n",
      "Epoch 1665/7000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 62591942318.3163 - val_loss: 75111682882.1783\n",
      "Epoch 1666/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 63484247566.1182 - val_loss: 74487072659.6951\n",
      "Epoch 1667/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 62453753184.0900 - val_loss: 73322142469.1128\n",
      "Epoch 1668/7000\n",
      "3554/3554 [==============================] - 1s 141us/step - loss: 63724999515.7682 - val_loss: 74310267403.3778\n",
      "Epoch 1669/7000\n",
      "3554/3554 [==============================] - 0s 123us/step - loss: 63827398607.5948 - val_loss: 75689607570.2549\n",
      "Epoch 1670/7000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 63343218625.1885 - val_loss: 73491963229.9747\n",
      "Epoch 1671/7000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 62290098525.2088 - val_loss: 73679820378.8782\n",
      "Epoch 1672/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 61649890459.0118 - val_loss: 73831392538.5722\n",
      "Epoch 1673/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 61006359550.2712 - val_loss: 71857986809.7350\n",
      "Epoch 1674/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 61940271091.3225 - val_loss: 72475874606.1592\n",
      "Epoch 1675/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 61303918970.0214 - val_loss: 71367803613.3626\n",
      "Epoch 1676/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 60834273341.0827 - val_loss: 71113549361.9758\n",
      "Epoch 1677/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 60071832267.7051 - val_loss: 74438052801.2062\n",
      "Epoch 1678/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 61230111845.9966 - val_loss: 73469561789.4616\n",
      "Epoch 1679/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 60827922773.1413 - val_loss: 75064853156.3297\n",
      "Epoch 1680/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 61006701844.0248 - val_loss: 75525161439.8830\n",
      "Epoch 1681/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 62109411571.1784 - val_loss: 74408042248.2813\n",
      "Epoch 1682/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 60912130298.0934 - val_loss: 70447744051.5601\n",
      "Epoch 1683/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 61679828733.8391 - val_loss: 70403107353.7800\n",
      "Epoch 1684/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 60906115420.0563 - val_loss: 74137987339.8819\n",
      "Epoch 1685/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 60517505901.0557 - val_loss: 74700385971.0200\n",
      "Epoch 1686/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 60505002746.9578 - val_loss: 77379147341.0520\n",
      "Epoch 1687/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 60517089436.7406 - val_loss: 72589648895.1359\n",
      "Epoch 1688/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 61149557656.5627 - val_loss: 72948422396.1834\n",
      "Epoch 1689/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 58813391975.1491 - val_loss: 71711961825.3952\n",
      "Epoch 1690/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 59397521142.3478 - val_loss: 74785995511.8627\n",
      "Epoch 1691/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 59369336668.3444 - val_loss: 70532083721.2174\n",
      "Epoch 1692/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 58878719466.3905 - val_loss: 69452142685.3266\n",
      "Epoch 1693/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 59079070325.2673 - val_loss: 70999669661.7767\n",
      "Epoch 1694/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 60350015395.7997 - val_loss: 75022838250.2526\n",
      "Epoch 1695/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 59056127497.5082 - val_loss: 68853249244.9305\n",
      "Epoch 1696/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 58674589191.2031 - val_loss: 73229768055.8987\n",
      "Epoch 1697/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 60500435050.0304 - val_loss: 71162196218.0231\n",
      "Epoch 1698/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 59613656748.0113 - val_loss: 69938435107.4295\n",
      "Epoch 1699/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 60469272375.4643 - val_loss: 68389397682.8759\n",
      "Epoch 1700/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 59003192028.9927 - val_loss: 73867225223.3811\n",
      "Epoch 1701/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 74us/step - loss: 58795536921.0670 - val_loss: 74279327636.3432\n",
      "Epoch 1702/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 60051348614.2667 - val_loss: 84877125614.1412\n",
      "Epoch 1703/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 58123082950.8070 - val_loss: 67669584592.4006\n",
      "Epoch 1704/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 57423859322.4536 - val_loss: 67541495895.2776\n",
      "Epoch 1705/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 56612206156.3534 - val_loss: 68969354912.0090\n",
      "Epoch 1706/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 59367896591.2707 - val_loss: 72346106969.5820\n",
      "Epoch 1707/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 57057501741.8120 - val_loss: 69801408596.9733\n",
      "Epoch 1708/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 57613643693.0197 - val_loss: 74242682827.5758\n",
      "Epoch 1709/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 57588043699.9347 - val_loss: 72027205854.9468\n",
      "Epoch 1710/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 57022283501.7040 - val_loss: 74880521631.0729\n",
      "Epoch 1711/7000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 59655832805.9246 - val_loss: 67406255254.6475\n",
      "Epoch 1712/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 56111095582.1092 - val_loss: 68757993623.2236\n",
      "Epoch 1713/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 57903100963.1514 - val_loss: 67860284589.4031\n",
      "Epoch 1714/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 56780237934.0642 - val_loss: 71713828723.4340\n",
      "Epoch 1715/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 58284849379.0433 - val_loss: 69315541154.7454\n",
      "Epoch 1716/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 57590454406.8430 - val_loss: 81973390131.1280\n",
      "Epoch 1717/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 56988813237.6635 - val_loss: 68080272005.2208\n",
      "Epoch 1718/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 55682930348.0113 - val_loss: 76251537276.6515\n",
      "Epoch 1719/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 57515718577.9178 - val_loss: 68685168990.6948\n",
      "Epoch 1720/7000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 56478657787.8222 - val_loss: 67536913702.0940\n",
      "Epoch 1721/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 55631820519.9415 - val_loss: 73731121491.8931\n",
      "Epoch 1722/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 55606734921.7603 - val_loss: 76123927094.9446\n",
      "Epoch 1723/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 56133195687.2572 - val_loss: 66950740488.2093\n",
      "Epoch 1724/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 55405020394.5346 - val_loss: 67444969800.9474\n",
      "Epoch 1725/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 56167044254.4693 - val_loss: 69358638329.4470\n",
      "Epoch 1726/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 56386170914.5751 - val_loss: 66200878143.0819\n",
      "Epoch 1727/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 55826153863.8514 - val_loss: 67749800586.4056\n",
      "Epoch 1728/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 57772781482.7147 - val_loss: 71728524764.2824\n",
      "Epoch 1729/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 55861525192.2476 - val_loss: 70452260975.7615\n",
      "Epoch 1730/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 56268448251.1019 - val_loss: 65457674110.3797\n",
      "Epoch 1731/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 55261952118.7079 - val_loss: 66276711426.5924\n",
      "Epoch 1732/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 54854462176.4502 - val_loss: 69722054934.8276\n",
      "Epoch 1733/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 55462520706.3770 - val_loss: 65747875433.2804\n",
      "Epoch 1734/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 57016832290.4311 - val_loss: 71170608218.7342\n",
      "Epoch 1735/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 53654574307.6196 - val_loss: 65022806500.4917\n",
      "Epoch 1736/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 53791659861.4294 - val_loss: 71241514842.0861\n",
      "Epoch 1737/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 54637670414.1182 - val_loss: 65207116782.1412\n",
      "Epoch 1738/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 55068633409.5487 - val_loss: 74847305503.3249\n",
      "Epoch 1739/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 54964406969.8413 - val_loss: 68737398061.0070\n",
      "Epoch 1740/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 53821461648.6393 - val_loss: 69284810855.1201\n",
      "Epoch 1741/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 56263574412.1733 - val_loss: 70615400036.6717\n",
      "Epoch 1742/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 53637544545.6747 - val_loss: 81024922331.6343\n",
      "Epoch 1743/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 59982595970.0889 - val_loss: 69121136130.3044\n",
      "Epoch 1744/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 53506382659.5656 - val_loss: 66421142878.5508\n",
      "Epoch 1745/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 54155078465.2606 - val_loss: 67990522247.1651\n",
      "Epoch 1746/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 59473830784.6483 - val_loss: 68218349578.9457\n",
      "Epoch 1747/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 54766917877.4834 - val_loss: 68639021484.0349\n",
      "Epoch 1748/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 53744049558.2577 - val_loss: 68337445195.6118\n",
      "Epoch 1749/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 53887458273.4586 - val_loss: 66849969426.7949\n",
      "Epoch 1750/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 54812996738.8092 - val_loss: 63965713832.2903\n",
      "Epoch 1751/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 52993462961.1975 - val_loss: 67766004319.7750\n",
      "Epoch 1752/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 53661268596.1148 - val_loss: 68833724991.5139\n",
      "Epoch 1753/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 52515993892.7361 - val_loss: 67117690861.1330\n",
      "Epoch 1754/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 52134303777.4226 - val_loss: 66779524070.6520\n",
      "Epoch 1755/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 53335056148.8891 - val_loss: 67376583616.6301\n",
      "Epoch 1756/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 53054491371.9752 - val_loss: 64069944994.3134\n",
      "Epoch 1757/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 52381284076.5515 - val_loss: 65264447091.6501\n",
      "Epoch 1758/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 53333546220.8396 - val_loss: 65795033110.4675\n",
      "Epoch 1759/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 52923097401.4811 - val_loss: 74108412549.2208\n",
      "Epoch 1760/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 53628412361.5442 - val_loss: 64605330695.7052\n",
      "Epoch 1761/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 51816685592.2026 - val_loss: 67824037052.5255\n",
      "Epoch 1762/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 53041213941.9156 - val_loss: 64908904072.7494\n",
      "Epoch 1763/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 51987878072.4007 - val_loss: 65820295740.9215\n",
      "Epoch 1764/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 52228863592.5898 - val_loss: 63252132355.0245\n",
      "Epoch 1765/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 81us/step - loss: 52713914681.4811 - val_loss: 65753658773.8554\n",
      "Epoch 1766/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 53453035032.4907 - val_loss: 66718704944.4636\n",
      "Epoch 1767/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 51311294147.0613 - val_loss: 65172631645.9027\n",
      "Epoch 1768/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 53333773151.2257 - val_loss: 70755571177.9646\n",
      "Epoch 1769/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 51296778208.8824 - val_loss: 63689502612.2712\n",
      "Epoch 1770/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 53133074225.7017 - val_loss: 63863470571.4048\n",
      "Epoch 1771/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 51218603156.0968 - val_loss: 64998587031.0796\n",
      "Epoch 1772/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 56083850110.3433 - val_loss: 72931457292.7460\n",
      "Epoch 1773/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 52838084181.5734 - val_loss: 65767852877.1241\n",
      "Epoch 1774/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 52160128111.7209 - val_loss: 79121326984.4613\n",
      "Epoch 1775/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 51240137951.0096 - val_loss: 63028306381.4481\n",
      "Epoch 1776/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 50987776946.2060 - val_loss: 84037083860.2892\n",
      "Epoch 1777/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 52028909497.6972 - val_loss: 62822994566.3730\n",
      "Epoch 1778/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 50532976705.6927 - val_loss: 68901465944.4298\n",
      "Epoch 1779/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 51775189890.9533 - val_loss: 63271300907.1347\n",
      "Epoch 1780/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 52015191355.2099 - val_loss: 63560661705.7755\n",
      "Epoch 1781/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 50172992376.5807 - val_loss: 67191365833.6315\n",
      "Epoch 1782/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 50858697363.2324 - val_loss: 63763548290.4844\n",
      "Epoch 1783/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 50083321645.6680 - val_loss: 64601233001.2804\n",
      "Epoch 1784/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 51961962279.3292 - val_loss: 65025885196.8900\n",
      "Epoch 1785/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 51851412761.7873 - val_loss: 68404320415.5769\n",
      "Epoch 1786/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 50064855240.5357 - val_loss: 72882744803.7356\n",
      "Epoch 1787/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 52651816914.7642 - val_loss: 72760292006.6340\n",
      "Epoch 1788/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 50223653516.3174 - val_loss: 64163052009.8925\n",
      "Epoch 1789/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 50151984518.1227 - val_loss: 73955399624.3173\n",
      "Epoch 1790/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 50018873018.9938 - val_loss: 62932572731.4813\n",
      "Epoch 1791/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 51666147393.6927 - val_loss: 62094226307.8526\n",
      "Epoch 1792/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 50468373106.9623 - val_loss: 65326264629.9364\n",
      "Epoch 1793/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 49704894699.1109 - val_loss: 63652302138.2571\n",
      "Epoch 1794/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 49788087479.3922 - val_loss: 61548142408.5153\n",
      "Epoch 1795/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 49940259111.0411 - val_loss: 63388182516.9103\n",
      "Epoch 1796/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 48997736429.5599 - val_loss: 71979817557.0453\n",
      "Epoch 1797/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 50282965025.4226 - val_loss: 61954712533.6574\n",
      "Epoch 1798/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 48945786012.2364 - val_loss: 67354900574.7668\n",
      "Epoch 1799/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 49957265968.6933 - val_loss: 61342892122.4461\n",
      "Epoch 1800/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 49006462582.9961 - val_loss: 62476388818.6329\n",
      "Epoch 1801/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 50772670923.8492 - val_loss: 62120800865.5032\n",
      "Epoch 1802/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 49390479773.7490 - val_loss: 62618561097.3075\n",
      "Epoch 1803/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 51797742144.2521 - val_loss: 65266344652.6560\n",
      "Epoch 1804/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 48756549524.8171 - val_loss: 62133192515.0425\n",
      "Epoch 1805/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 50488112249.5892 - val_loss: 74016173020.2104\n",
      "Epoch 1806/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 50351849070.9285 - val_loss: 67068450514.7049\n",
      "Epoch 1807/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 50609019208.4637 - val_loss: 62445935001.0239\n",
      "Epoch 1808/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 49638797611.6511 - val_loss: 63889921373.3986\n",
      "Epoch 1809/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 51534106947.5656 - val_loss: 61745291499.6208\n",
      "Epoch 1810/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 50980603202.1249 - val_loss: 61120929988.4467\n",
      "Epoch 1811/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 49864908330.9308 - val_loss: 60346789557.0363\n",
      "Epoch 1812/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 48592948780.0833 - val_loss: 64659496000.8101\n",
      "Epoch 1813/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 48638390848.2521 - val_loss: 80167227309.0790\n",
      "Epoch 1814/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 51501726888.8419 - val_loss: 64534376335.9505\n",
      "Epoch 1815/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 49316736507.6781 - val_loss: 80391234450.2549\n",
      "Epoch 1816/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 48676211396.7901 - val_loss: 61991057632.3871\n",
      "Epoch 1817/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 50455114509.3979 - val_loss: 62636157785.7980\n",
      "Epoch 1818/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 48978628769.3506 - val_loss: 61905561670.5710\n",
      "Epoch 1819/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 48742219195.1379 - val_loss: 61727969836.5030\n",
      "Epoch 1820/7000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 47724952557.2718 - val_loss: 68635320776.2633\n",
      "Epoch 1821/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 47754745188.7001 - val_loss: 62212750179.3035\n",
      "Epoch 1822/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 47692667196.9387 - val_loss: 60876525234.8759\n",
      "Epoch 1823/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 49367259939.2954 - val_loss: 66762271975.3001\n",
      "Epoch 1824/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 48644327961.0670 - val_loss: 60309258507.3058\n",
      "Epoch 1825/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 47449625955.5476 - val_loss: 61501270248.7404\n",
      "Epoch 1826/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 47555675596.4254 - val_loss: 61717866798.4473\n",
      "Epoch 1827/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 47943735063.1941 - val_loss: 61145645493.8284\n",
      "Epoch 1828/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 47457347966.0552 - val_loss: 69061981898.6757\n",
      "Epoch 1829/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 99us/step - loss: 48026397989.3123 - val_loss: 68962987015.0571\n",
      "Epoch 1830/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 48212898216.1216 - val_loss: 61073978338.3314\n",
      "Epoch 1831/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 48880675031.5183 - val_loss: 61575466100.7302\n",
      "Epoch 1832/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 49343757226.7147 - val_loss: 60501253037.9072\n",
      "Epoch 1833/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 48615555450.0214 - val_loss: 59964435797.9094\n",
      "Epoch 1834/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 48627141482.3905 - val_loss: 60952791114.8917\n",
      "Epoch 1835/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 49779079759.2347 - val_loss: 63355859913.8475\n",
      "Epoch 1836/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 57162998401.9448 - val_loss: 61064254113.1612\n",
      "Epoch 1837/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 47420346706.8362 - val_loss: 66061404700.9485\n",
      "Epoch 1838/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 47149315716.8261 - val_loss: 60162931625.2985\n",
      "Epoch 1839/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 47441464438.1317 - val_loss: 59758865227.9719\n",
      "Epoch 1840/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 47886914727.6894 - val_loss: 60959614411.7198\n",
      "Epoch 1841/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 47599640151.8784 - val_loss: 76659213538.1153\n",
      "Epoch 1842/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 50817333978.6877 - val_loss: 60286999355.2653\n",
      "Epoch 1843/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 47150481325.0197 - val_loss: 60106491651.6726\n",
      "Epoch 1844/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 48128043232.7383 - val_loss: 60066033059.6816\n",
      "Epoch 1845/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 48428194991.7569 - val_loss: 61013931010.3044\n",
      "Epoch 1846/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 48972296626.4941 - val_loss: 73650915444.9463\n",
      "Epoch 1847/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 47635077819.5701 - val_loss: 64696244288.2340\n",
      "Epoch 1848/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 47406836277.8796 - val_loss: 61493064639.7660\n",
      "Epoch 1849/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 47185166454.7079 - val_loss: 59937593159.3632\n",
      "Epoch 1850/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 47093414456.1846 - val_loss: 62188595063.7547\n",
      "Epoch 1851/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 48133700110.6944 - val_loss: 59428375494.9671\n",
      "Epoch 1852/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 47940031100.1823 - val_loss: 59835999326.4788\n",
      "Epoch 1853/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 48916066610.5661 - val_loss: 61290705318.8501\n",
      "Epoch 1854/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 47858212961.9629 - val_loss: 62041165862.4540\n",
      "Epoch 1855/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 46640870678.3298 - val_loss: 61460189202.7229\n",
      "Epoch 1856/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 47652776670.1452 - val_loss: 59847753891.6096\n",
      "Epoch 1857/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 47592793370.9398 - val_loss: 60074590196.4782\n",
      "Epoch 1858/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 46440130432.3602 - val_loss: 60720420613.9769\n",
      "Epoch 1859/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 47301069778.4761 - val_loss: 60627104995.8436\n",
      "Epoch 1860/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 47472626338.2150 - val_loss: 63577497564.2824\n",
      "Epoch 1861/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 48198546834.8002 - val_loss: 59214150275.7806\n",
      "Epoch 1862/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 46825410940.3264 - val_loss: 60207201188.0416\n",
      "Epoch 1863/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 46917241128.1936 - val_loss: 60257486486.0714\n",
      "Epoch 1864/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 46632322576.4232 - val_loss: 59370700384.0630\n",
      "Epoch 1865/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 46346465818.2195 - val_loss: 60511009976.9249\n",
      "Epoch 1866/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 46114394194.9803 - val_loss: 66511555465.0374\n",
      "Epoch 1867/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 45554707947.5431 - val_loss: 61594926641.3997\n",
      "Epoch 1868/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 47265971532.4975 - val_loss: 59054970024.2183\n",
      "Epoch 1869/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 46125828537.4091 - val_loss: 65764391650.1513\n",
      "Epoch 1870/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 48903941952.6843 - val_loss: 59171634875.0852\n",
      "Epoch 1871/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 46249673741.2538 - val_loss: 77227898780.3364\n",
      "Epoch 1872/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 50175882571.3450 - val_loss: 58655461296.2115\n",
      "Epoch 1873/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 46257799730.4221 - val_loss: 58408677431.3046\n",
      "Epoch 1874/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 46236876211.0704 - val_loss: 60622076713.9826\n",
      "Epoch 1875/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 46856386994.4941 - val_loss: 58981773874.1198\n",
      "Epoch 1876/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 46738129246.3613 - val_loss: 60869980909.8532\n",
      "Epoch 1877/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 45463444305.3956 - val_loss: 67558276787.0200\n",
      "Epoch 1878/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 47658398868.9612 - val_loss: 62962627328.5041\n",
      "Epoch 1879/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 46521859052.5515 - val_loss: 62440786675.2540\n",
      "Epoch 1880/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 45990102792.2116 - val_loss: 60659045631.4959\n",
      "Epoch 1881/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 45709274628.8981 - val_loss: 64623888916.8833\n",
      "Epoch 1882/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 47777272460.3174 - val_loss: 59986825932.5120\n",
      "Epoch 1883/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 46492054758.3568 - val_loss: 59469048272.6166\n",
      "Epoch 1884/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 46420194607.6849 - val_loss: 58719743355.6433\n",
      "Epoch 1885/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 45062439861.6635 - val_loss: 58410198994.4889\n",
      "Epoch 1886/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 47108668186.0754 - val_loss: 70451842839.8357\n",
      "Epoch 1887/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 47743936717.1458 - val_loss: 59662223257.4560\n",
      "Epoch 1888/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 45696518152.0675 - val_loss: 58886544667.1482\n",
      "Epoch 1889/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 45931084994.1970 - val_loss: 59039977213.6236\n",
      "Epoch 1890/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 46456298796.2274 - val_loss: 61695114950.6070\n",
      "Epoch 1891/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 48135593232.5672 - val_loss: 59579722643.6591\n",
      "Epoch 1892/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45461184638.7755 - val_loss: 58905579984.6166\n",
      "Epoch 1893/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 81us/step - loss: 45777697148.3264 - val_loss: 73296873570.6734\n",
      "Epoch 1894/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 46053772954.7237 - val_loss: 69036947390.1817\n",
      "Epoch 1895/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 47241547787.5250 - val_loss: 63659865369.9961\n",
      "Epoch 1896/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 48635529157.2223 - val_loss: 59074658260.2172\n",
      "Epoch 1897/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45246719464.0855 - val_loss: 60271298241.7103\n",
      "Epoch 1898/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 46634304332.7856 - val_loss: 62627032538.6982\n",
      "Epoch 1899/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 45335785367.1221 - val_loss: 63931010085.4459\n",
      "Epoch 1900/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 46751726901.4474 - val_loss: 57585390868.8112\n",
      "Epoch 1901/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 45782124773.9246 - val_loss: 59717770936.4928\n",
      "Epoch 1902/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 47222607825.8998 - val_loss: 58485542500.9598\n",
      "Epoch 1903/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 45872482909.0647 - val_loss: 65470792906.4146\n",
      "Epoch 1904/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 46373309719.4823 - val_loss: 61359526859.7198\n",
      "Epoch 1905/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 46820731491.9797 - val_loss: 67961433756.2644\n",
      "Epoch 1906/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 45526596766.4693 - val_loss: 60170934258.7499\n",
      "Epoch 1907/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 46271727181.5059 - val_loss: 58887809916.0754\n",
      "Epoch 1908/7000\n",
      "3554/3554 [==============================] - 0s 67us/step - loss: 45122021168.6213 - val_loss: 57914121438.9468\n",
      "Epoch 1909/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 44685605044.9432 - val_loss: 59747538057.1094\n",
      "Epoch 1910/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 45316355269.6545 - val_loss: 64894076343.2686\n",
      "Epoch 1911/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 47467646593.3686 - val_loss: 59080802572.3139\n",
      "Epoch 1912/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 45168475923.7366 - val_loss: 58012352049.3997\n",
      "Epoch 1913/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 45621021403.8402 - val_loss: 70106805438.3978\n",
      "Epoch 1914/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 45698905669.5824 - val_loss: 62594644212.7662\n",
      "Epoch 1915/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 45106029418.7507 - val_loss: 57732044443.6883\n",
      "Epoch 1916/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 45381389601.8548 - val_loss: 57772039400.3083\n",
      "Epoch 1917/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 45049032946.0259 - val_loss: 69938965781.5134\n",
      "Epoch 1918/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 48867123292.2003 - val_loss: 57736064546.7094\n",
      "Epoch 1919/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 46163504665.6432 - val_loss: 59520993083.8413\n",
      "Epoch 1920/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45769964970.4266 - val_loss: 58109855628.2059\n",
      "Epoch 1921/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 44191370638.1902 - val_loss: 58811941456.0765\n",
      "Epoch 1922/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 44940306024.0135 - val_loss: 59292409405.9297\n",
      "Epoch 1923/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 43994303879.7074 - val_loss: 61824853306.9772\n",
      "Epoch 1924/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 45777003362.6832 - val_loss: 59217266508.8360\n",
      "Epoch 1925/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 46121436251.6241 - val_loss: 57623295023.5274\n",
      "Epoch 1926/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 45535616875.0388 - val_loss: 59282536172.0529\n",
      "Epoch 1927/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 44824775486.3793 - val_loss: 57956786558.2357\n",
      "Epoch 1928/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 44071472104.0855 - val_loss: 67236542517.1173\n",
      "Epoch 1929/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 44756597442.4851 - val_loss: 63303939903.6850\n",
      "Epoch 1930/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 45342658709.8256 - val_loss: 58255992241.5077\n",
      "Epoch 1931/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 45916335005.4609 - val_loss: 69340714861.0970\n",
      "Epoch 1932/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 45456800277.6095 - val_loss: 57968726784.5041\n",
      "Epoch 1933/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 46103327436.8576 - val_loss: 64731775565.3401\n",
      "Epoch 1934/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 47313017227.3090 - val_loss: 59142218238.4158\n",
      "Epoch 1935/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 44669595246.9285 - val_loss: 70179639302.3370\n",
      "Epoch 1936/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 45425835395.8177 - val_loss: 65384778250.6937\n",
      "Epoch 1937/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 46524064035.5836 - val_loss: 59825469894.3910\n",
      "Epoch 1938/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 45230641351.6714 - val_loss: 59926377806.4202\n",
      "Epoch 1939/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 44486077811.6826 - val_loss: 60186861136.5086\n",
      "Epoch 1940/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 46385949649.1075 - val_loss: 59001581010.0568\n",
      "Epoch 1941/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 43995536553.7783 - val_loss: 58033359624.5693\n",
      "Epoch 1942/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 43778723314.1700 - val_loss: 59068982144.3961\n",
      "Epoch 1943/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 44455412541.2268 - val_loss: 58808661692.3814\n",
      "Epoch 1944/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45759559718.6089 - val_loss: 56887427004.3094\n",
      "Epoch 1945/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 45927929320.0855 - val_loss: 59465890427.2833\n",
      "Epoch 1946/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 43795250614.5279 - val_loss: 62279305916.5255\n",
      "Epoch 1947/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 45180865351.0231 - val_loss: 57684588152.8349\n",
      "Epoch 1948/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 44539986000.6753 - val_loss: 57698314178.3584\n",
      "Epoch 1949/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 44156020384.4862 - val_loss: 64332449212.3454\n",
      "Epoch 1950/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 44142465358.2262 - val_loss: 57506878965.7744\n",
      "Epoch 1951/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 44368795233.0985 - val_loss: 58003927417.8430\n",
      "Epoch 1952/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 44897521776.9454 - val_loss: 71671526319.6354\n",
      "Epoch 1953/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 47165077548.3714 - val_loss: 56933885772.5480\n",
      "Epoch 1954/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 43674678048.9904 - val_loss: 56966873338.3111\n",
      "Epoch 1955/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 44945685119.6398 - val_loss: 57538729509.6619\n",
      "Epoch 1956/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 44046595900.0743 - val_loss: 59643469045.1263\n",
      "Epoch 1957/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 44027040868.8441 - val_loss: 58135177673.1274\n",
      "Epoch 1958/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 44972205106.1339 - val_loss: 65603054881.7193\n",
      "Epoch 1959/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 45015475941.3483 - val_loss: 57590108090.4371\n",
      "Epoch 1960/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 44385025388.7676 - val_loss: 58222439069.8487\n",
      "Epoch 1961/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 44072126711.7884 - val_loss: 57948227283.8571\n",
      "Epoch 1962/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 44107237368.0765 - val_loss: 60806667690.6397\n",
      "Epoch 1963/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 47310053089.0265 - val_loss: 62173391567.4307\n",
      "Epoch 1964/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 45662657808.5672 - val_loss: 57185292952.2318\n",
      "Epoch 1965/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 44315190145.8008 - val_loss: 58026218453.0813\n",
      "Epoch 1966/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 45103627135.6398 - val_loss: 56850554810.0051\n",
      "Epoch 1967/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 43404207774.7575 - val_loss: 56770617083.3193\n",
      "Epoch 1968/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 45370766946.2510 - val_loss: 61865850625.9443\n",
      "Epoch 1969/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45212391632.6033 - val_loss: 60957408036.7978\n",
      "Epoch 1970/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 45982470957.6680 - val_loss: 58461516168.3173\n",
      "Epoch 1971/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 43268240494.0642 - val_loss: 57700776757.2163\n",
      "Epoch 1972/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 43692972909.0557 - val_loss: 57659145352.8214\n",
      "Epoch 1973/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 46594841660.5065 - val_loss: 57909126904.9609\n",
      "Epoch 1974/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 44526196601.1570 - val_loss: 57859470471.5972\n",
      "Epoch 1975/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 43839531152.0630 - val_loss: 58707346407.3722\n",
      "Epoch 1976/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 43048489243.5160 - val_loss: 58330148213.5944\n",
      "Epoch 1977/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 44167261011.7006 - val_loss: 61113898134.0714\n",
      "Epoch 1978/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 45093943827.3044 - val_loss: 61050022184.8304\n",
      "Epoch 1979/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 44956625065.4181 - val_loss: 57134215755.0357\n",
      "Epoch 1980/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 43689394505.6162 - val_loss: 58313390434.2954\n",
      "Epoch 1981/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 42789353060.5560 - val_loss: 62601187738.1761\n",
      "Epoch 1982/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 43511953252.7001 - val_loss: 57064165174.3685\n",
      "Epoch 1983/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 44022621801.1660 - val_loss: 57930833232.7246\n",
      "Epoch 1984/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 43320418015.8739 - val_loss: 57416118687.0008\n",
      "Epoch 1985/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 43616937406.5954 - val_loss: 56586524858.3111\n",
      "Epoch 1986/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 44223339276.2454 - val_loss: 87217774360.4118\n",
      "Epoch 1987/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 46274296666.6156 - val_loss: 58743145520.9677\n",
      "Epoch 1988/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 45699558379.8312 - val_loss: 57814152701.6956\n",
      "Epoch 1989/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 44496686060.4074 - val_loss: 57409524521.9105\n",
      "Epoch 1990/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 43311405020.2724 - val_loss: 57667448714.4776\n",
      "Epoch 1991/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 44739637449.9764 - val_loss: 57701666313.9376\n",
      "Epoch 1992/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 43506770632.2476 - val_loss: 62507294113.9533\n",
      "Epoch 1993/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 43983938665.4541 - val_loss: 59590369019.0312\n",
      "Epoch 1994/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 44861471979.6871 - val_loss: 56908540884.0731\n",
      "Epoch 1995/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 43706377238.4738 - val_loss: 63819104451.8706\n",
      "Epoch 1996/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 46653911203.6556 - val_loss: 58429729592.7089\n",
      "Epoch 1997/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 43718580492.5335 - val_loss: 57523975692.5300\n",
      "Epoch 1998/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 44520835206.8430 - val_loss: 56808950896.3376\n",
      "Epoch 1999/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45359996707.8717 - val_loss: 57238366563.1595\n",
      "Epoch 2000/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 42598120963.7456 - val_loss: 64568953607.1291\n",
      "Epoch 2001/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 44887320529.8998 - val_loss: 58162452255.9010\n",
      "Epoch 2002/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 43781744355.3315 - val_loss: 57494912594.9570\n",
      "Epoch 2003/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 43060916779.5070 - val_loss: 59412378345.1724\n",
      "Epoch 2004/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 43622406998.0056 - val_loss: 58454091408.7426\n",
      "Epoch 2005/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 42577938532.2679 - val_loss: 56355744275.4430\n",
      "Epoch 2006/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 43111857078.8160 - val_loss: 57276386417.6338\n",
      "Epoch 2007/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 46991101490.9983 - val_loss: 56722996326.3280\n",
      "Epoch 2008/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 44469366251.3990 - val_loss: 57194740584.2003\n",
      "Epoch 2009/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 42665627645.6950 - val_loss: 62084050312.5693\n",
      "Epoch 2010/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 45660065264.1531 - val_loss: 56629566984.4973\n",
      "Epoch 2011/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 46721691150.1182 - val_loss: 56179503957.7654\n",
      "Epoch 2012/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 44485140187.2639 - val_loss: 61210438712.2408\n",
      "Epoch 2013/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 42690231070.6854 - val_loss: 57303825142.4225\n",
      "Epoch 2014/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 42899590162.1519 - val_loss: 60238650535.3541\n",
      "Epoch 2015/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 42660883244.5155 - val_loss: 66064926336.9001\n",
      "Epoch 2016/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 43580901831.2392 - val_loss: 58459870688.1710\n",
      "Epoch 2017/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 43296357427.2864 - val_loss: 60979692636.5705\n",
      "Epoch 2018/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 43131487066.6156 - val_loss: 56391085743.8515\n",
      "Epoch 2019/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 43175644688.4232 - val_loss: 57256057784.8529\n",
      "Epoch 2020/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 43285811672.5267 - val_loss: 56659403464.9114\n",
      "Epoch 2021/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 88us/step - loss: 43742276200.5898 - val_loss: 57916483457.8363\n",
      "Epoch 2022/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 45221960718.4063 - val_loss: 58345329068.3229\n",
      "Epoch 2023/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42999352120.0405 - val_loss: 57389974479.0323\n",
      "Epoch 2024/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 43609700281.6972 - val_loss: 56045390658.4664\n",
      "Epoch 2025/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 42234370892.7856 - val_loss: 59522615227.4453\n",
      "Epoch 2026/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 44809679749.2583 - val_loss: 59126889702.4360\n",
      "Epoch 2027/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45693219832.5087 - val_loss: 58667600339.9291\n",
      "Epoch 2028/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 44157065461.4834 - val_loss: 56686185198.2132\n",
      "Epoch 2029/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 42782367696.6033 - val_loss: 63886861901.7001\n",
      "Epoch 2030/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42848382448.1531 - val_loss: 56778410346.0726\n",
      "Epoch 2031/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 43582175899.2999 - val_loss: 58065285391.3384\n",
      "Epoch 2032/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 43203077343.0096 - val_loss: 56597263219.7221\n",
      "Epoch 2033/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42499173496.4367 - val_loss: 56034317854.6768\n",
      "Epoch 2034/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42974812621.0017 - val_loss: 60162784174.4833\n",
      "Epoch 2035/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 45404531867.0118 - val_loss: 56333089077.7924\n",
      "Epoch 2036/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 43957356063.4057 - val_loss: 56739091844.5727\n",
      "Epoch 2037/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42250429891.2054 - val_loss: 60741054454.8546\n",
      "Epoch 2038/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 42979763300.8441 - val_loss: 69126660004.9778\n",
      "Epoch 2039/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 44165713848.5447 - val_loss: 56189843761.3277\n",
      "Epoch 2040/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 42711233268.0428 - val_loss: 59280103725.4391\n",
      "Epoch 2041/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42148601496.4187 - val_loss: 56510686223.5544\n",
      "Epoch 2042/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 43332767492.7541 - val_loss: 57202792821.8824\n",
      "Epoch 2043/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 42762041885.6770 - val_loss: 56693355326.4338\n",
      "Epoch 2044/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 42504700702.6854 - val_loss: 72110621404.6425\n",
      "Epoch 2045/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 43117269918.6134 - val_loss: 58685636763.6883\n",
      "Epoch 2046/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 42393679231.4958 - val_loss: 63416659587.5285\n",
      "Epoch 2047/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42325809455.1086 - val_loss: 57531236965.6079\n",
      "Epoch 2048/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 41964923229.3528 - val_loss: 57104130634.3156\n",
      "Epoch 2049/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 42551517141.6455 - val_loss: 58200927170.7184\n",
      "Epoch 2050/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 42749299922.3320 - val_loss: 57424398213.8689\n",
      "Epoch 2051/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 42278835708.8306 - val_loss: 58617565631.9820\n",
      "Epoch 2052/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 42845513500.9567 - val_loss: 63844678920.1373\n",
      "Epoch 2053/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 42743534383.9730 - val_loss: 64979675062.0163\n",
      "Epoch 2054/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 43973088124.0383 - val_loss: 68231078957.4391\n",
      "Epoch 2055/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 45578893986.2150 - val_loss: 56116465086.3978\n",
      "Epoch 2056/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 45405845521.2876 - val_loss: 58890562535.5162\n",
      "Epoch 2057/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 42315403597.0737 - val_loss: 58829804715.9629\n",
      "Epoch 2058/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 45231237805.7400 - val_loss: 58629291456.1980\n",
      "Epoch 2059/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 45083576747.5791 - val_loss: 55819402036.3522\n",
      "Epoch 2060/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 43184982180.8081 - val_loss: 58299400190.9558\n",
      "Epoch 2061/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 43064696927.6579 - val_loss: 56264647633.9848\n",
      "Epoch 2062/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 41854779469.2178 - val_loss: 59102835091.4070\n",
      "Epoch 2063/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 42726273139.2504 - val_loss: 56763331872.3331\n",
      "Epoch 2064/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 42519266272.3061 - val_loss: 57141903854.3572\n",
      "Epoch 2065/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 41784552700.9747 - val_loss: 57172681939.1370\n",
      "Epoch 2066/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 43547876277.6635 - val_loss: 56750744804.9958\n",
      "Epoch 2067/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 43547684682.4806 - val_loss: 68499306727.0121\n",
      "Epoch 2068/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 44107449435.6241 - val_loss: 57421775821.3761\n",
      "Epoch 2069/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42531579658.5166 - val_loss: 56358016772.5367\n",
      "Epoch 2070/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 42641305730.8092 - val_loss: 60650732532.7662\n",
      "Epoch 2071/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 42051260478.2352 - val_loss: 56110670349.1060\n",
      "Epoch 2072/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 43792044668.1823 - val_loss: 66740985272.1328\n",
      "Epoch 2073/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 42981880494.8925 - val_loss: 55238868908.8990\n",
      "Epoch 2074/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 44794583125.2853 - val_loss: 64669019483.9854\n",
      "Epoch 2075/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 42179546585.1030 - val_loss: 60772603351.7277\n",
      "Epoch 2076/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 42775001052.2724 - val_loss: 56787510431.8650\n",
      "Epoch 2077/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 42248946919.6534 - val_loss: 59587700600.0968\n",
      "Epoch 2078/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 42702483292.3444 - val_loss: 55771417699.6636\n",
      "Epoch 2079/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 43531529241.9314 - val_loss: 55462123120.4816\n",
      "Epoch 2080/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 41677184314.0574 - val_loss: 57011086599.1291\n",
      "Epoch 2081/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 42121410614.1677 - val_loss: 55475407773.7766\n",
      "Epoch 2082/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42671683917.0737 - val_loss: 66867610132.0191\n",
      "Epoch 2083/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 43725184726.6539 - val_loss: 56893313695.2169\n",
      "Epoch 2084/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42177226211.4755 - val_loss: 57348081545.6135\n",
      "Epoch 2085/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 79us/step - loss: 43126446748.4524 - val_loss: 55084386362.9052\n",
      "Epoch 2086/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41885268057.8953 - val_loss: 56335310732.5660\n",
      "Epoch 2087/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 41696696311.3562 - val_loss: 58949871956.2532\n",
      "Epoch 2088/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 42258892522.2465 - val_loss: 60125241719.6107\n",
      "Epoch 2089/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41825528489.1300 - val_loss: 59128312112.3916\n",
      "Epoch 2090/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 42066230859.2009 - val_loss: 57232235474.2008\n",
      "Epoch 2091/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 41519212961.2065 - val_loss: 58625502859.8458\n",
      "Epoch 2092/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 43210121003.9392 - val_loss: 57968792107.7828\n",
      "Epoch 2093/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 45711492150.7440 - val_loss: 58673263212.7370\n",
      "Epoch 2094/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 44367776229.9246 - val_loss: 56269316178.6689\n",
      "Epoch 2095/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 42378442720.8824 - val_loss: 55695630692.1676\n",
      "Epoch 2096/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 44555627358.0732 - val_loss: 64919514194.9570\n",
      "Epoch 2097/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42470862550.0777 - val_loss: 56023119441.6608\n",
      "Epoch 2098/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41874755300.4840 - val_loss: 57497522688.8641\n",
      "Epoch 2099/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 42762292738.5931 - val_loss: 55550065792.1800\n",
      "Epoch 2100/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 42725594837.5014 - val_loss: 55238399403.4588\n",
      "Epoch 2101/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 45706059296.5583 - val_loss: 55466348985.4290\n",
      "Epoch 2102/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 41587103959.5183 - val_loss: 56606589890.2143\n",
      "Epoch 2103/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 42887896573.9831 - val_loss: 55347658863.6174\n",
      "Epoch 2104/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 41949099935.1896 - val_loss: 58769223151.1494\n",
      "Epoch 2105/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 42272498793.4541 - val_loss: 55703265714.9480\n",
      "Epoch 2106/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41422666819.4215 - val_loss: 58385790973.6956\n",
      "Epoch 2107/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 41738779838.7394 - val_loss: 55420597053.5696\n",
      "Epoch 2108/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41201565585.3596 - val_loss: 56470572549.0408\n",
      "Epoch 2109/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 43317237611.3270 - val_loss: 60474246614.3775\n",
      "Epoch 2110/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42513893006.0461 - val_loss: 56242256523.5578\n",
      "Epoch 2111/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 42515614144.3241 - val_loss: 74112573899.7198\n",
      "Epoch 2112/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 45081340866.9173 - val_loss: 57763671103.0819\n",
      "Epoch 2113/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41862191091.8987 - val_loss: 57110621192.3533\n",
      "Epoch 2114/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 42039469078.4738 - val_loss: 59871343155.7041\n",
      "Epoch 2115/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 41824623021.8841 - val_loss: 56040613704.2273\n",
      "Epoch 2116/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41821350053.3844 - val_loss: 55916234447.6084\n",
      "Epoch 2117/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 40853338223.7929 - val_loss: 57589458087.2101\n",
      "Epoch 2118/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 42509218126.2262 - val_loss: 55491020278.0624\n",
      "Epoch 2119/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 41680108339.7907 - val_loss: 60757241780.3342\n",
      "Epoch 2120/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42645861888.2881 - val_loss: 58082138524.4805\n",
      "Epoch 2121/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42239006389.2313 - val_loss: 57383285960.1913\n",
      "Epoch 2122/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 41937373098.7147 - val_loss: 56465604118.0354\n",
      "Epoch 2123/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42067163632.7293 - val_loss: 57231341354.9907\n",
      "Epoch 2124/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 42081705974.7800 - val_loss: 56730966821.0138\n",
      "Epoch 2125/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 41446054561.0625 - val_loss: 55598040419.0155\n",
      "Epoch 2126/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 40885789996.2274 - val_loss: 57709324581.3738\n",
      "Epoch 2127/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 41378139506.5301 - val_loss: 54725679324.4985\n",
      "Epoch 2128/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41626225695.5858 - val_loss: 55350004606.0917\n",
      "Epoch 2129/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 41507797458.7642 - val_loss: 68564518636.6290\n",
      "Epoch 2130/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 43437314663.4373 - val_loss: 55218533609.0284\n",
      "Epoch 2131/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 42114965507.4575 - val_loss: 56790411325.2816\n",
      "Epoch 2132/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41287420539.0298 - val_loss: 56709533935.0774\n",
      "Epoch 2133/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 42147140701.9291 - val_loss: 56083269844.0011\n",
      "Epoch 2134/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 42138968718.6224 - val_loss: 55573755334.5350\n",
      "Epoch 2135/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 41568920367.9730 - val_loss: 64327753416.7674\n",
      "Epoch 2136/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 45472910633.9223 - val_loss: 58269180483.5466\n",
      "Epoch 2137/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 41382527333.8526 - val_loss: 56261327107.2405\n",
      "Epoch 2138/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41495148527.8649 - val_loss: 56050629685.8644\n",
      "Epoch 2139/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41202132687.7389 - val_loss: 58116237036.6290\n",
      "Epoch 2140/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 41547558388.7631 - val_loss: 61627040780.3859\n",
      "Epoch 2141/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 43372238813.4249 - val_loss: 62805432987.3283\n",
      "Epoch 2142/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41691802868.9071 - val_loss: 54985155730.9030\n",
      "Epoch 2143/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 42900365369.6252 - val_loss: 67536197450.5316\n",
      "Epoch 2144/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 42751662500.0878 - val_loss: 56307584734.8028\n",
      "Epoch 2145/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 40547294943.8739 - val_loss: 56031004331.5308\n",
      "Epoch 2146/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 42203983457.0985 - val_loss: 60069142658.1963\n",
      "Epoch 2147/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 43171557523.5205 - val_loss: 56287138675.4340\n",
      "Epoch 2148/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 42517564285.3348 - val_loss: 57861725615.3474\n",
      "Epoch 2149/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41150072766.3073 - val_loss: 57245781419.9629\n",
      "Epoch 2150/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 43833492047.2347 - val_loss: 57348398404.0506\n",
      "Epoch 2151/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41634480259.3855 - val_loss: 58943361423.0864\n",
      "Epoch 2152/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41293993542.5909 - val_loss: 55582324956.2824\n",
      "Epoch 2153/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 40941716353.8008 - val_loss: 58134499119.4554\n",
      "Epoch 2154/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41581435678.6854 - val_loss: 55101774370.4214\n",
      "Epoch 2155/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 44087275356.3444 - val_loss: 78803050506.3696\n",
      "Epoch 2156/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 43150876175.4147 - val_loss: 56219130655.4149\n",
      "Epoch 2157/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 41571650650.4716 - val_loss: 56385930367.1719\n",
      "Epoch 2158/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42119445343.2257 - val_loss: 56498393711.0774\n",
      "Epoch 2159/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41010749914.8317 - val_loss: 55283229951.6399\n",
      "Epoch 2160/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 40998293370.3095 - val_loss: 56249467620.2937\n",
      "Epoch 2161/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 41436387251.9347 - val_loss: 55319368584.7494\n",
      "Epoch 2162/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 41590095918.1002 - val_loss: 69046943618.9885\n",
      "Epoch 2163/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 45051764471.5003 - val_loss: 60720519741.4976\n",
      "Epoch 2164/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 43064392898.7732 - val_loss: 57670101688.4928\n",
      "Epoch 2165/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 44089591831.0501 - val_loss: 55196840442.9592\n",
      "Epoch 2166/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 42756845266.0439 - val_loss: 54664789821.1376\n",
      "Epoch 2167/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41108388397.8120 - val_loss: 56171775926.2605\n",
      "Epoch 2168/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41347020478.4513 - val_loss: 55777718413.7181\n",
      "Epoch 2169/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41962089412.9342 - val_loss: 56607441746.8849\n",
      "Epoch 2170/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 40906051824.8554 - val_loss: 56573023488.3601\n",
      "Epoch 2171/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 40804107023.7029 - val_loss: 57482083057.9938\n",
      "Epoch 2172/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 41563807084.7676 - val_loss: 56442606514.8715\n",
      "Epoch 2173/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41562917192.4637 - val_loss: 55658363160.2678\n",
      "Epoch 2174/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 42219718334.4513 - val_loss: 55903916112.0765\n",
      "Epoch 2175/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41147753566.9735 - val_loss: 57202041726.0917\n",
      "Epoch 2176/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41138070624.2341 - val_loss: 55974002989.5471\n",
      "Epoch 2177/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41334168363.3630 - val_loss: 55890881174.0714\n",
      "Epoch 2178/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41870596085.6275 - val_loss: 55756371459.6366\n",
      "Epoch 2179/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 42565385435.5520 - val_loss: 73800142123.4903\n",
      "Epoch 2180/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41960263440.2791 - val_loss: 56003265378.1513\n",
      "Epoch 2181/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 40171003136.4322 - val_loss: 56217341248.8821\n",
      "Epoch 2182/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 40881500964.4480 - val_loss: 54619091207.8492\n",
      "Epoch 2183/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 43897069028.6280 - val_loss: 61803870875.9764\n",
      "Epoch 2184/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 45066522885.0422 - val_loss: 58094806798.2402\n",
      "Epoch 2185/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 40367328742.9330 - val_loss: 59416190963.0380\n",
      "Epoch 2186/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 44091943371.8492 - val_loss: 59296704435.9561\n",
      "Epoch 2187/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 41907996359.8154 - val_loss: 60315238857.4155\n",
      "Epoch 2188/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41645977085.9831 - val_loss: 60775271094.0444\n",
      "Epoch 2189/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 40928635140.1778 - val_loss: 55613943861.6124\n",
      "Epoch 2190/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 41710557393.7558 - val_loss: 70308271166.3617\n",
      "Epoch 2191/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 43191565955.6736 - val_loss: 58837332275.6321\n",
      "Epoch 2192/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 41763719604.7991 - val_loss: 56314847941.7429\n",
      "Epoch 2193/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 42662728980.0248 - val_loss: 56163630627.1415\n",
      "Epoch 2194/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41421790406.8070 - val_loss: 55274331274.8377\n",
      "Epoch 2195/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 41385764802.9173 - val_loss: 56248550032.1665\n",
      "Epoch 2196/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 40299913150.8835 - val_loss: 55114933431.4847\n",
      "Epoch 2197/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 40641049979.7501 - val_loss: 54567402758.2650\n",
      "Epoch 2198/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 43376340030.8115 - val_loss: 59960300562.1468\n",
      "Epoch 2199/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 42687700482.5931 - val_loss: 56099347602.6149\n",
      "Epoch 2200/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 41774454835.8627 - val_loss: 59136810182.0309\n",
      "Epoch 2201/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 43804585566.7935 - val_loss: 57816807337.6765\n",
      "Epoch 2202/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 41580398693.1322 - val_loss: 59422821889.5842\n",
      "Epoch 2203/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 40054259841.0805 - val_loss: 56518212270.1232\n",
      "Epoch 2204/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 40726901406.7575 - val_loss: 60033177318.4360\n",
      "Epoch 2205/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 41101869406.3613 - val_loss: 66167786993.4537\n",
      "Epoch 2206/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 42145146232.0765 - val_loss: 55074054079.1899\n",
      "Epoch 2207/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 41117592638.8115 - val_loss: 54911328192.1260\n",
      "Epoch 2208/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 40444981294.1002 - val_loss: 56315875442.0658\n",
      "Epoch 2209/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 44350053229.0557 - val_loss: 56228682913.8813\n",
      "Epoch 2210/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 41619674883.6016 - val_loss: 60192443067.0852\n",
      "Epoch 2211/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 45636012805.3303 - val_loss: 57923007965.7947\n",
      "Epoch 2212/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 41061130056.7518 - val_loss: 56886495219.0380\n",
      "Epoch 2213/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 72us/step - loss: 40172605783.4463 - val_loss: 55356287510.6115\n",
      "Epoch 2214/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 43190166363.7682 - val_loss: 56317178885.3288\n",
      "Epoch 2215/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41489842475.6511 - val_loss: 55170442258.5789\n",
      "Epoch 2216/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 40834876768.0900 - val_loss: 62042092806.9851\n",
      "Epoch 2217/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 42232683783.3472 - val_loss: 57812101783.6557\n",
      "Epoch 2218/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 44510489685.8616 - val_loss: 58118332842.3066\n",
      "Epoch 2219/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 40950805715.4845 - val_loss: 63447074469.1938\n",
      "Epoch 2220/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 40907103730.4581 - val_loss: 55543349218.3314\n",
      "Epoch 2221/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 40663334679.1941 - val_loss: 59074134304.6571\n",
      "Epoch 2222/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 40725542198.5999 - val_loss: 59088263267.0875\n",
      "Epoch 2223/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 41001382185.3461 - val_loss: 56055849555.1010\n",
      "Epoch 2224/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 40561407733.7715 - val_loss: 59633906930.5339\n",
      "Epoch 2225/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 42660510656.0360 - val_loss: 55857376516.6807\n",
      "Epoch 2226/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 40464269617.9899 - val_loss: 57911852657.8498\n",
      "Epoch 2227/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 40598557270.8700 - val_loss: 62204135952.2745\n",
      "Epoch 2228/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 42713354067.1244 - val_loss: 56907295600.8416\n",
      "Epoch 2229/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 42010946682.7417 - val_loss: 55723136055.8807\n",
      "Epoch 2230/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 41463297102.9465 - val_loss: 57968807303.6692\n",
      "Epoch 2231/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 41042877278.6494 - val_loss: 54655783844.6897\n",
      "Epoch 2232/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 42038693973.2853 - val_loss: 55130513555.6951\n",
      "Epoch 2233/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 40325413715.7006 - val_loss: 62869563826.9210\n",
      "Epoch 2234/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 41771334679.9145 - val_loss: 58452367258.6082\n",
      "Epoch 2235/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 41738274715.7321 - val_loss: 57809176025.2579\n",
      "Epoch 2236/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 40881234466.2870 - val_loss: 55487907723.0537\n",
      "Epoch 2237/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 41269180002.2510 - val_loss: 54732889963.3688\n",
      "Epoch 2238/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 41181750709.3754 - val_loss: 75530954573.4121\n",
      "Epoch 2239/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 43303160841.2200 - val_loss: 61750191487.3879\n",
      "Epoch 2240/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 40465594431.2234 - val_loss: 54312779100.9665\n",
      "Epoch 2241/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 39760220820.5470 - val_loss: 55910413155.4475\n",
      "Epoch 2242/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 39851615002.9398 - val_loss: 55954335478.9986\n",
      "Epoch 2243/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 40230823724.5155 - val_loss: 68645600348.4985\n",
      "Epoch 2244/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 40805341022.0732 - val_loss: 55100754125.9522\n",
      "Epoch 2245/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 40698951285.0512 - val_loss: 58618734214.0850\n",
      "Epoch 2246/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 41413958836.3669 - val_loss: 55988674224.5716\n",
      "Epoch 2247/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 40856731067.1379 - val_loss: 55688830089.6855\n",
      "Epoch 2248/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 39839240401.7558 - val_loss: 56368907912.9654\n",
      "Epoch 2249/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 40750615657.4541 - val_loss: 55065329632.8911\n",
      "Epoch 2250/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 40191934092.8936 - val_loss: 57122781768.1553\n",
      "Epoch 2251/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 41428388398.9645 - val_loss: 56340854195.2000\n",
      "Epoch 2252/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 40331049124.8081 - val_loss: 55303744420.9778\n",
      "Epoch 2253/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 41077725009.9719 - val_loss: 56086414210.1243\n",
      "Epoch 2254/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 40979460573.1367 - val_loss: 55732923717.7789\n",
      "Epoch 2255/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 42726721667.3855 - val_loss: 54845291830.9446\n",
      "Epoch 2256/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 39603300566.9420 - val_loss: 61596402141.2681\n",
      "Epoch 2257/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 41455116237.2898 - val_loss: 55111939920.4366\n",
      "Epoch 2258/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 44974338362.6337 - val_loss: 54093995301.6619\n",
      "Epoch 2259/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 40459656749.2358 - val_loss: 54105376413.2726\n",
      "Epoch 2260/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 39588655138.5751 - val_loss: 59513374756.2937\n",
      "Epoch 2261/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 41731901939.0343 - val_loss: 54626446068.5502\n",
      "Epoch 2262/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 42129151719.9415 - val_loss: 56607355535.5904\n",
      "Epoch 2263/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41708540063.0456 - val_loss: 54092659568.5536\n",
      "Epoch 2264/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 40147438162.6922 - val_loss: 55355191621.4188\n",
      "Epoch 2265/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 40070914446.7665 - val_loss: 55312889910.1525\n",
      "Epoch 2266/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 40054765500.0023 - val_loss: 55101300095.0999\n",
      "Epoch 2267/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 40672208272.4952 - val_loss: 56165416877.9072\n",
      "Epoch 2268/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41972741626.5256 - val_loss: 67839125295.1674\n",
      "Epoch 2269/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41359437775.5948 - val_loss: 58710960797.9482\n",
      "Epoch 2270/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 40554321971.8627 - val_loss: 56662146599.0301\n",
      "Epoch 2271/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 41023686025.5082 - val_loss: 54140119249.2647\n",
      "Epoch 2272/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 39697003772.6866 - val_loss: 55911838058.3606\n",
      "Epoch 2273/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 40768674584.3466 - val_loss: 54618434287.5454\n",
      "Epoch 2274/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 40383610813.1548 - val_loss: 54183701362.8579\n",
      "Epoch 2275/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 39800801988.7901 - val_loss: 54737962515.4430\n",
      "Epoch 2276/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 40296008995.0073 - val_loss: 58521729742.3842\n",
      "Epoch 2277/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 40389707712.6123 - val_loss: 55008677490.3539\n",
      "Epoch 2278/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 39608838322.6382 - val_loss: 56756861070.4383\n",
      "Epoch 2279/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 41349374143.0276 - val_loss: 58736374727.6512\n",
      "Epoch 2280/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 39542971489.3866 - val_loss: 53892295300.6447\n",
      "Epoch 2281/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 41457307431.3292 - val_loss: 72822676954.4101\n",
      "Epoch 2282/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 40673180383.0096 - val_loss: 67941623690.1086\n",
      "Epoch 2283/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 39839817492.8891 - val_loss: 54234776349.8847\n",
      "Epoch 2284/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 39920624380.6866 - val_loss: 54615390082.9885\n",
      "Epoch 2285/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 39891548788.1148 - val_loss: 57062311155.3980\n",
      "Epoch 2286/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 40442486396.7586 - val_loss: 56970435234.4574\n",
      "Epoch 2287/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 41109529211.6061 - val_loss: 58434404974.4653\n",
      "Epoch 2288/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 40051523233.6387 - val_loss: 59946259089.8948\n",
      "Epoch 2289/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 39726616057.3731 - val_loss: 56630544115.1460\n",
      "Epoch 2290/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 42164229250.8092 - val_loss: 55457940033.5302\n",
      "Epoch 2291/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 42410977518.5684 - val_loss: 58265119231.8560\n",
      "Epoch 2292/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 40850050403.5476 - val_loss: 54755372375.0616\n",
      "Epoch 2293/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 40862343294.7755 - val_loss: 54088835064.9429\n",
      "Epoch 2294/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41122366143.0276 - val_loss: 56368873696.5311\n",
      "Epoch 2295/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 39565598553.4631 - val_loss: 54710430067.5061\n",
      "Epoch 2296/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 39901365162.4266 - val_loss: 56934872313.9511\n",
      "Epoch 2297/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 39083331890.5661 - val_loss: 54676650157.1151\n",
      "Epoch 2298/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 38877352242.5661 - val_loss: 55239239824.9226\n",
      "Epoch 2299/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 39618988173.7580 - val_loss: 54100325452.8360\n",
      "Epoch 2300/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 39799172807.6714 - val_loss: 58503955735.5117\n",
      "Epoch 2301/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 39931323981.5059 - val_loss: 53822960640.7201\n",
      "Epoch 2302/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 39521736981.7535 - val_loss: 58245983053.9882\n",
      "Epoch 2303/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 40294139055.1806 - val_loss: 55579177307.6343\n",
      "Epoch 2304/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 39191988785.2696 - val_loss: 57475508694.1345\n",
      "Epoch 2305/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 40394517165.7400 - val_loss: 57646261370.7072\n",
      "Epoch 2306/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 39571230191.2887 - val_loss: 54655774686.0287\n",
      "Epoch 2307/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 40141952163.0793 - val_loss: 55366919579.3283\n",
      "Epoch 2308/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 39417729768.5177 - val_loss: 65912918994.3449\n",
      "Epoch 2309/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 41282548553.3281 - val_loss: 55975109411.3575\n",
      "Epoch 2310/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 39357460549.1503 - val_loss: 53590365294.7533\n",
      "Epoch 2311/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 39633272002.1970 - val_loss: 59248419823.0053\n",
      "Epoch 2312/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 39768973053.8391 - val_loss: 58821640466.2188\n",
      "Epoch 2313/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 40083548712.0495 - val_loss: 63343021174.5305\n",
      "Epoch 2314/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 40846334796.2093 - val_loss: 54132687392.8371\n",
      "Epoch 2315/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 40071770112.2881 - val_loss: 58124788893.3446\n",
      "Epoch 2316/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 39748086265.9494 - val_loss: 53711223490.2864\n",
      "Epoch 2317/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 41534397541.9966 - val_loss: 54484854268.6875\n",
      "Epoch 2318/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 39235220804.4299 - val_loss: 53770974668.6560\n",
      "Epoch 2319/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 38978816056.4727 - val_loss: 59123219759.5983\n",
      "Epoch 2320/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 40017407098.7417 - val_loss: 57963005312.2880\n",
      "Epoch 2321/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 40444006582.0957 - val_loss: 55469336712.9654\n",
      "Epoch 2322/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 39080547518.7394 - val_loss: 55600157305.6990\n",
      "Epoch 2323/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 39898090716.7046 - val_loss: 57111262901.9004\n",
      "Epoch 2324/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 39324902340.0698 - val_loss: 54943674303.6940\n",
      "Epoch 2325/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 40649467808.3421 - val_loss: 60002866169.7350\n",
      "Epoch 2326/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 39563641850.8137 - val_loss: 60623060627.4070\n",
      "Epoch 2327/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 41083856871.7974 - val_loss: 67995791518.7848\n",
      "Epoch 2328/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 39973466522.2915 - val_loss: 53679538931.8301\n",
      "Epoch 2329/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 40940740425.9043 - val_loss: 55145020760.5018\n",
      "Epoch 2330/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 39069351758.5144 - val_loss: 54047799233.9263\n",
      "Epoch 2331/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 39562086371.1874 - val_loss: 54534471094.6925\n",
      "Epoch 2332/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 38984748680.2836 - val_loss: 63549385654.5845\n",
      "Epoch 2333/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 38873804703.7659 - val_loss: 54944435615.7930\n",
      "Epoch 2334/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 38695957642.8768 - val_loss: 52984500531.4520\n",
      "Epoch 2335/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 38633352674.3230 - val_loss: 67702430960.8056\n",
      "Epoch 2336/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 40660468281.3551 - val_loss: 53194004560.2205\n",
      "Epoch 2337/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 38731735748.7901 - val_loss: 55407830255.4374\n",
      "Epoch 2338/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 38962113787.2459 - val_loss: 53782014696.5963\n",
      "Epoch 2339/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 39133858322.1519 - val_loss: 52573509991.4802\n",
      "Epoch 2340/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 39626596612.4660 - val_loss: 66304199685.8464\n",
      "Epoch 2341/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 72us/step - loss: 38970421383.9955 - val_loss: 53417947862.4495\n",
      "Epoch 2342/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 39678879913.4181 - val_loss: 54494571228.8945\n",
      "Epoch 2343/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 40643168904.8599 - val_loss: 55339720670.0107\n",
      "Epoch 2344/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 38724208332.8576 - val_loss: 53822377356.0979\n",
      "Epoch 2345/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 38638050217.9944 - val_loss: 53231386049.7463\n",
      "Epoch 2346/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 37267213154.4536 - val_loss: 53062244712.5603\n",
      "Epoch 2347/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 37166737260.8036 - val_loss: 52469643098.6622\n",
      "Epoch 2348/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 37742269612.2994 - val_loss: 52286456613.0228\n",
      "Epoch 2349/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 38256105078.9961 - val_loss: 56400134562.4574\n",
      "Epoch 2350/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 37623922139.9842 - val_loss: 53327635890.6599\n",
      "Epoch 2351/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 36325702207.0996 - val_loss: 52325031584.5941\n",
      "Epoch 2352/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 37058950491.2639 - val_loss: 51398998089.7395\n",
      "Epoch 2353/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 35822531268.7901 - val_loss: 54864607032.5648\n",
      "Epoch 2354/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 37163055492.3939 - val_loss: 53001862404.9328\n",
      "Epoch 2355/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 36691229476.4479 - val_loss: 57009750609.8588\n",
      "Epoch 2356/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 35587018772.7451 - val_loss: 52133726224.7066\n",
      "Epoch 2357/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 36131948318.2532 - val_loss: 53093310078.0017\n",
      "Epoch 2358/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 36613747183.5768 - val_loss: 51466034182.3370\n",
      "Epoch 2359/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 35391520187.4260 - val_loss: 52669891376.9586\n",
      "Epoch 2360/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 35742898738.4221 - val_loss: 54429051606.8816\n",
      "Epoch 2361/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 35120888007.3832 - val_loss: 64636915091.4070\n",
      "Epoch 2362/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 41167210723.0433 - val_loss: 51830510849.0802\n",
      "Epoch 2363/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 34802860804.1778 - val_loss: 51974290921.6945\n",
      "Epoch 2364/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 34674170056.5357 - val_loss: 53433365638.8051\n",
      "Epoch 2365/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 35661553179.9482 - val_loss: 48558609703.5342\n",
      "Epoch 2366/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 35579421376.7563 - val_loss: 48090896855.3496\n",
      "Epoch 2367/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 34819658625.8008 - val_loss: 49015853868.5030\n",
      "Epoch 2368/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 33595103952.3151 - val_loss: 51899560796.9665\n",
      "Epoch 2369/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 35125492026.0574 - val_loss: 47663425759.3879\n",
      "Epoch 2370/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 32979356688.7113 - val_loss: 50453021085.5606\n",
      "Epoch 2371/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 32928289191.5453 - val_loss: 52663705535.4059\n",
      "Epoch 2372/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 33061604407.8965 - val_loss: 46693459227.6523\n",
      "Epoch 2373/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 32429248470.5098 - val_loss: 45856384082.2368\n",
      "Epoch 2374/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 33378307192.4367 - val_loss: 46219847700.3072\n",
      "Epoch 2375/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 30300162572.3894 - val_loss: 44562419712.4951\n",
      "Epoch 2376/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 30881946988.7676 - val_loss: 49847585420.1339\n",
      "Epoch 2377/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 31975012568.6708 - val_loss: 46054295696.0945\n",
      "Epoch 2378/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 29230405629.5869 - val_loss: 44427480552.2543\n",
      "Epoch 2379/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 31015533596.2364 - val_loss: 41349088088.0518\n",
      "Epoch 2380/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 29329088203.1289 - val_loss: 53436964720.5536\n",
      "Epoch 2381/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 29444906545.8458 - val_loss: 40542602573.3941\n",
      "Epoch 2382/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 27080767528.3376 - val_loss: 44722911806.5058\n",
      "Epoch 2383/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 27834028548.3219 - val_loss: 42012946600.7854\n",
      "Epoch 2384/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 27169934666.1925 - val_loss: 39635445451.7198\n",
      "Epoch 2385/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 26242151202.1429 - val_loss: 39626076119.4217\n",
      "Epoch 2386/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 26439560174.1362 - val_loss: 38387069498.1851\n",
      "Epoch 2387/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 26533257285.1503 - val_loss: 37675843007.2664\n",
      "Epoch 2388/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 27825147802.5796 - val_loss: 38341373744.1575\n",
      "Epoch 2389/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 25039467577.0490 - val_loss: 36782524442.9052\n",
      "Epoch 2390/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 26224603244.3354 - val_loss: 36533307479.2956\n",
      "Epoch 2391/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 25417298752.1080 - val_loss: 35978845253.4909\n",
      "Epoch 2392/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 24574815165.7310 - val_loss: 37153058033.6698\n",
      "Epoch 2393/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 23565091365.4564 - val_loss: 36382137418.9997\n",
      "Epoch 2394/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 23986642912.8824 - val_loss: 36211152133.2568\n",
      "Epoch 2395/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 24302150687.6939 - val_loss: 38225871259.0582\n",
      "Epoch 2396/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 23218396746.0484 - val_loss: 35302487173.0408\n",
      "Epoch 2397/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 26407194203.1919 - val_loss: 37281565436.3994\n",
      "Epoch 2398/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 23122953338.7417 - val_loss: 35351751652.9958\n",
      "Epoch 2399/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 22366274024.6618 - val_loss: 33397606736.5626\n",
      "Epoch 2400/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 23654642269.0647 - val_loss: 38009651232.2610\n",
      "Epoch 2401/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 22743833786.4356 - val_loss: 39758144330.5316\n",
      "Epoch 2402/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 22576127483.6781 - val_loss: 34552071834.7702\n",
      "Epoch 2403/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 23835834019.9437 - val_loss: 34880492828.5885\n",
      "Epoch 2404/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 21374084549.5104 - val_loss: 32155312018.2909\n",
      "Epoch 2405/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 72us/step - loss: 21783048878.8925 - val_loss: 33041898030.3032\n",
      "Epoch 2406/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 20957468999.3112 - val_loss: 33379203591.7052\n",
      "Epoch 2407/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 20605190150.9150 - val_loss: 31655754284.5750\n",
      "Epoch 2408/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 21246302086.9871 - val_loss: 30716222376.2183\n",
      "Epoch 2409/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 21653048647.8875 - val_loss: 31753742642.4439\n",
      "Epoch 2410/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 21270644567.8784 - val_loss: 36017199722.1446\n",
      "Epoch 2411/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 20332391182.5504 - val_loss: 30594106356.6942\n",
      "Epoch 2412/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 20540537540.7901 - val_loss: 31001314204.4804\n",
      "Epoch 2413/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 20154203975.5993 - val_loss: 32885819566.8433\n",
      "Epoch 2414/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 21697064162.4671 - val_loss: 31843292944.2025\n",
      "Epoch 2415/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 19970590880.4862 - val_loss: 29720460059.5083\n",
      "Epoch 2416/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 19750115386.7777 - val_loss: 32689875573.8104\n",
      "Epoch 2417/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 20230057277.5149 - val_loss: 31434103610.7612\n",
      "Epoch 2418/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 19470208497.3056 - val_loss: 29988342738.8489\n",
      "Epoch 2419/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 18652631800.6528 - val_loss: 29038701298.5339\n",
      "Epoch 2420/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 18979629741.7400 - val_loss: 30889839385.9961\n",
      "Epoch 2421/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 19334171527.9955 - val_loss: 28857416915.2090\n",
      "Epoch 2422/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 18633153593.0490 - val_loss: 29120724172.5840\n",
      "Epoch 2423/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 19903621993.3101 - val_loss: 48603236986.2751\n",
      "Epoch 2424/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 19784212890.2915 - val_loss: 28520581037.4751\n",
      "Epoch 2425/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 18222321508.6280 - val_loss: 29992386958.7983\n",
      "Epoch 2426/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 19942366417.7558 - val_loss: 31156374984.8034\n",
      "Epoch 2427/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 18577942702.0281 - val_loss: 28267152009.6135\n",
      "Epoch 2428/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 19008809453.8481 - val_loss: 29090763724.1519\n",
      "Epoch 2429/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 17947419271.1311 - val_loss: 29549223832.3038\n",
      "Epoch 2430/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 17705141814.4558 - val_loss: 28530664261.1308\n",
      "Epoch 2431/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 18941082848.1621 - val_loss: 31575656756.2082\n",
      "Epoch 2432/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 17970680383.0996 - val_loss: 27739041825.9893\n",
      "Epoch 2433/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 18624018677.4834 - val_loss: 29233652337.0577\n",
      "Epoch 2434/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 18663223814.6269 - val_loss: 27036157791.5589\n",
      "Epoch 2435/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 18450913349.2943 - val_loss: 27622422142.7398\n",
      "Epoch 2436/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 17472508890.5436 - val_loss: 26986813674.3246\n",
      "Epoch 2437/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 17602307290.9758 - val_loss: 26246954469.3558\n",
      "Epoch 2438/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 17187414593.5487 - val_loss: 27073885228.3589\n",
      "Epoch 2439/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 16729878210.5256 - val_loss: 26486888252.5615\n",
      "Epoch 2440/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 17492320813.9561 - val_loss: 25873281839.3114\n",
      "Epoch 2441/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 17044511489.8728 - val_loss: 27134867245.0070\n",
      "Epoch 2442/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 18177806882.2870 - val_loss: 25822463116.7100\n",
      "Epoch 2443/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 18880963821.9921 - val_loss: 28835737262.4833\n",
      "Epoch 2444/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 16742530681.3011 - val_loss: 25296827139.2405\n",
      "Epoch 2445/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 16875486001.1255 - val_loss: 25941497753.4560\n",
      "Epoch 2446/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 16668250639.8469 - val_loss: 27256752164.8698\n",
      "Epoch 2447/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 16394816821.4474 - val_loss: 26174054521.8430\n",
      "Epoch 2448/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 16650517894.1227 - val_loss: 29878000485.6799\n",
      "Epoch 2449/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 17221429287.1851 - val_loss: 26134319780.4017\n",
      "Epoch 2450/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 17664868214.8520 - val_loss: 25420724366.5823\n",
      "Epoch 2451/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 17563361195.0028 - val_loss: 26724338147.1955\n",
      "Epoch 2452/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 16530381514.5526 - val_loss: 25879926776.5828\n",
      "Epoch 2453/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 17412999224.4727 - val_loss: 30740599377.0847\n",
      "Epoch 2454/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 16935209720.6528 - val_loss: 29919407592.5243\n",
      "Epoch 2455/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 16146450009.7513 - val_loss: 24956119601.1837\n",
      "Epoch 2456/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 16445637238.9961 - val_loss: 27018738037.5944\n",
      "Epoch 2457/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 15626714193.2515 - val_loss: 25790738320.3826\n",
      "Epoch 2458/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 16921040751.0726 - val_loss: 28039903591.4802\n",
      "Epoch 2459/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 15954781708.9657 - val_loss: 26369916526.1772\n",
      "Epoch 2460/7000\n",
      "3554/3554 [==============================] - 0s 116us/step - loss: 16849942803.4485 - val_loss: 23868804548.3747\n",
      "Epoch 2461/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 15654257007.6488 - val_loss: 35438473393.5077\n",
      "Epoch 2462/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 17298915621.8886 - val_loss: 24672245753.0149\n",
      "Epoch 2463/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 16160005361.4496 - val_loss: 23990648221.2006\n",
      "Epoch 2464/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 15871773491.7186 - val_loss: 25314576506.4191\n",
      "Epoch 2465/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 15539657889.9268 - val_loss: 24592049989.2748\n",
      "Epoch 2466/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 16097014003.2504 - val_loss: 24097420938.1176\n",
      "Epoch 2467/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 16441722665.0580 - val_loss: 24638700009.7485\n",
      "Epoch 2468/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 15808538015.4778 - val_loss: 24483666094.9873\n",
      "Epoch 2469/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 77us/step - loss: 15134335688.2476 - val_loss: 24185723069.6776\n",
      "Epoch 2470/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 15353004209.1975 - val_loss: 23273642523.6523\n",
      "Epoch 2471/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 15359398748.9207 - val_loss: 29519408084.1451\n",
      "Epoch 2472/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 14989733756.0383 - val_loss: 23288055617.3142\n",
      "Epoch 2473/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 15240232566.7079 - val_loss: 23518944955.6613\n",
      "Epoch 2474/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 15377310522.8137 - val_loss: 23206879708.4264\n",
      "Epoch 2475/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 15130085201.9719 - val_loss: 22742718597.0768\n",
      "Epoch 2476/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 14608664154.1835 - val_loss: 26573500593.7958\n",
      "Epoch 2477/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 15718879957.5014 - val_loss: 22706510548.8653\n",
      "Epoch 2478/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 16175301980.6325 - val_loss: 25569555109.4819\n",
      "Epoch 2479/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 14972819520.5402 - val_loss: 24261843984.4186\n",
      "Epoch 2480/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 16277018494.9195 - val_loss: 23625480131.5105\n",
      "Epoch 2481/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 15365747389.8751 - val_loss: 23067905309.3086\n",
      "Epoch 2482/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 14897614613.7535 - val_loss: 26414016584.0113\n",
      "Epoch 2483/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 14644973967.3427 - val_loss: 22885812021.5044\n",
      "Epoch 2484/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 14705041719.7524 - val_loss: 27167150859.4498\n",
      "Epoch 2485/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 15551366308.8081 - val_loss: 23970200983.8717\n",
      "Epoch 2486/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 14788824554.7867 - val_loss: 22247353237.7114\n",
      "Epoch 2487/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 15197960413.8571 - val_loss: 23635786731.8368\n",
      "Epoch 2488/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 14890773100.9116 - val_loss: 23316897094.6430\n",
      "Epoch 2489/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 14826941263.0006 - val_loss: 21970284369.0127\n",
      "Epoch 2490/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 14150554121.5082 - val_loss: 22484503612.7775\n",
      "Epoch 2491/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 14268278133.4114 - val_loss: 22885000270.0602\n",
      "Epoch 2492/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 13859251365.3844 - val_loss: 24019506883.7266\n",
      "Epoch 2493/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 15050140881.7558 - val_loss: 22088505072.3736\n",
      "Epoch 2494/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 14120746374.2307 - val_loss: 23895117332.8113\n",
      "Epoch 2495/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 13861471686.9510 - val_loss: 24218863076.4917\n",
      "Epoch 2496/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 14317377287.7794 - val_loss: 22158676746.0096\n",
      "Epoch 2497/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 14196245075.5566 - val_loss: 22605612123.8864\n",
      "Epoch 2498/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 13980385129.8863 - val_loss: 26797450987.7648\n",
      "Epoch 2499/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 14184403174.5008 - val_loss: 26394053087.5949\n",
      "Epoch 2500/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 14946406236.0563 - val_loss: 24168251008.3241\n",
      "Epoch 2501/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 13820546419.6826 - val_loss: 22058938327.9617\n",
      "Epoch 2502/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 14046345738.0844 - val_loss: 23216621924.0236\n",
      "Epoch 2503/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 13907472415.6939 - val_loss: 21661885423.2934\n",
      "Epoch 2504/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 13042452133.5284 - val_loss: 21964112628.9823\n",
      "Epoch 2505/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 12717621833.4721 - val_loss: 25327595093.8374\n",
      "Epoch 2506/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 12682377690.9758 - val_loss: 22511918586.6712\n",
      "Epoch 2507/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 13370950597.7985 - val_loss: 22186133986.3314\n",
      "Epoch 2508/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 13775052963.6556 - val_loss: 20347179802.7162\n",
      "Epoch 2509/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 12981369906.1339 - val_loss: 19562884836.0596\n",
      "Epoch 2510/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 12673556235.9392 - val_loss: 24084283361.6113\n",
      "Epoch 2511/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 12769920094.7935 - val_loss: 21508406382.3212\n",
      "Epoch 2512/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 13330342896.4412 - val_loss: 22016793847.4307\n",
      "Epoch 2513/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 13896899987.3765 - val_loss: 21912767866.3471\n",
      "Epoch 2514/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 13108481573.1683 - val_loss: 20191915929.6000\n",
      "Epoch 2515/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 11983563028.6010 - val_loss: 19965644263.0841\n",
      "Epoch 2516/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 12236247715.3675 - val_loss: 22211811528.3353\n",
      "Epoch 2517/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 13966293436.8666 - val_loss: 24737627704.7449\n",
      "Epoch 2518/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 13202292392.5537 - val_loss: 18089707155.3350\n",
      "Epoch 2519/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 11897335646.0732 - val_loss: 17779595869.9027\n",
      "Epoch 2520/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 11281165235.3585 - val_loss: 18798821625.9511\n",
      "Epoch 2521/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 11692311578.3635 - val_loss: 20055957546.4866\n",
      "Epoch 2522/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 12287974983.1671 - val_loss: 17446609029.7969\n",
      "Epoch 2523/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 11423796313.0310 - val_loss: 17801319655.5882\n",
      "Epoch 2524/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 10689816108.0833 - val_loss: 18082364222.1457\n",
      "Epoch 2525/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 10573455309.8661 - val_loss: 19777816554.3966\n",
      "Epoch 2526/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 10499782059.6511 - val_loss: 16890765847.7637\n",
      "Epoch 2527/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 10143990930.6562 - val_loss: 16303234839.9797\n",
      "Epoch 2528/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 10369494994.4761 - val_loss: 16386514215.3902\n",
      "Epoch 2529/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 9883616846.0822 - val_loss: 17233545435.5623\n",
      "Epoch 2530/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 10684293705.4721 - val_loss: 24613281666.9885\n",
      "Epoch 2531/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 10184534045.9651 - val_loss: 15653079580.3724\n",
      "Epoch 2532/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 10301250298.9578 - val_loss: 16037259753.0284\n",
      "Epoch 2533/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 9620560636.0383 - val_loss: 17865772091.6253\n",
      "Epoch 2534/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 9476831005.4609 - val_loss: 15113752483.9696\n",
      "Epoch 2535/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 9612265543.4553 - val_loss: 15524372874.1176\n",
      "Epoch 2536/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 9976330357.6995 - val_loss: 15615250368.3421\n",
      "Epoch 2537/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 9143363756.0113 - val_loss: 15212510154.8557\n",
      "Epoch 2538/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 8978665229.9741 - val_loss: 15511086579.4700\n",
      "Epoch 2539/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 8731760258.3770 - val_loss: 14416358221.1961\n",
      "Epoch 2540/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 8980615672.7968 - val_loss: 21689769618.9030\n",
      "Epoch 2541/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 8704159398.2487 - val_loss: 14188209928.8574\n",
      "Epoch 2542/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 8507121953.8548 - val_loss: 14000534940.6245\n",
      "Epoch 2543/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 8698562201.5712 - val_loss: 19649110279.8492\n",
      "Epoch 2544/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 9397435899.6781 - val_loss: 15004878896.1035\n",
      "Epoch 2545/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 10132953115.6601 - val_loss: 13892627834.2031\n",
      "Epoch 2546/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 8336420802.9173 - val_loss: 19099746185.3255\n",
      "Epoch 2547/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 8935991498.2645 - val_loss: 17545908522.1266\n",
      "Epoch 2548/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 8614158370.8633 - val_loss: 15909350985.0194\n",
      "Epoch 2549/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 8197852599.2482 - val_loss: 13525428123.2563\n",
      "Epoch 2550/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 8161194030.1002 - val_loss: 13755633134.9333\n",
      "Epoch 2551/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 7780210740.7271 - val_loss: 13630746397.1646\n",
      "Epoch 2552/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 8154645007.8469 - val_loss: 14134772095.1719\n",
      "Epoch 2553/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 8131568067.9257 - val_loss: 15099110291.2990\n",
      "Epoch 2554/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 7457988076.8396 - val_loss: 12955517874.6599\n",
      "Epoch 2555/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 8072586957.2898 - val_loss: 15756393530.6172\n",
      "Epoch 2556/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 7753270070.3118 - val_loss: 12573991341.9072\n",
      "Epoch 2557/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 7542587816.1216 - val_loss: 14335144522.7477\n",
      "Epoch 2558/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 7455337289.9043 - val_loss: 12984921379.2135\n",
      "Epoch 2559/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 7530477321.5082 - val_loss: 12479948334.1592\n",
      "Epoch 2560/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 7569822709.0512 - val_loss: 15250858216.7944\n",
      "Epoch 2561/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 7716237808.7293 - val_loss: 11853276829.6326\n",
      "Epoch 2562/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 7099685401.9313 - val_loss: 19319569456.3736\n",
      "Epoch 2563/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 8072643812.3039 - val_loss: 12357062252.6650\n",
      "Epoch 2564/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 6786053645.3078 - val_loss: 13231065480.2453\n",
      "Epoch 2565/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 7038917924.9342 - val_loss: 11658095778.7454\n",
      "Epoch 2566/7000\n",
      "3554/3554 [==============================] - 0s 118us/step - loss: 6635889560.8509 - val_loss: 18774907230.2627\n",
      "Epoch 2567/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 7995313317.3844 - val_loss: 12560230157.7902\n",
      "Epoch 2568/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 7178742414.0461 - val_loss: 13476895475.3980\n",
      "Epoch 2569/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 7347900053.5374 - val_loss: 13088396511.2349\n",
      "Epoch 2570/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 6853706928.9094 - val_loss: 12522955078.7781\n",
      "Epoch 2571/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 6641590953.7062 - val_loss: 11515129136.5356\n",
      "Epoch 2572/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 6746943596.3354 - val_loss: 14633532898.1513\n",
      "Epoch 2573/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 6747747302.6449 - val_loss: 11334470879.3789\n",
      "Epoch 2574/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 6674856103.1131 - val_loss: 11788149924.7617\n",
      "Epoch 2575/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 6688048240.8014 - val_loss: 11328414019.5826\n",
      "Epoch 2576/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 6143148970.1384 - val_loss: 10935536819.5961\n",
      "Epoch 2577/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 5972389936.7653 - val_loss: 11348803157.5494\n",
      "Epoch 2578/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 6775380473.6612 - val_loss: 13498854445.2951\n",
      "Epoch 2579/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 6656068680.6078 - val_loss: 14204629967.0323\n",
      "Epoch 2580/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 6753800178.3500 - val_loss: 10518048631.8267\n",
      "Epoch 2581/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 5934682197.8616 - val_loss: 13129361469.3536\n",
      "Epoch 2582/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 6374120991.6939 - val_loss: 11426846561.5662\n",
      "Epoch 2583/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 5842114348.8756 - val_loss: 10582146509.3761\n",
      "Epoch 2584/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 5720925017.1750 - val_loss: 10691556296.2633\n",
      "Epoch 2585/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 6702853827.9977 - val_loss: 11613475096.3398\n",
      "Epoch 2586/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 6039180138.8002 - val_loss: 10667571523.2585\n",
      "Epoch 2587/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 6270910433.1705 - val_loss: 12625397121.1162\n",
      "Epoch 2588/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 6155677030.1047 - val_loss: 10401264412.4444\n",
      "Epoch 2589/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 5563045752.2926 - val_loss: 10431867755.8729\n",
      "Epoch 2590/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 6262022558.9015 - val_loss: 16300530894.5283\n",
      "Epoch 2591/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 5957226453.9336 - val_loss: 12107525427.3440\n",
      "Epoch 2592/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 5591030667.4530 - val_loss: 10086852561.3367\n",
      "Epoch 2593/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 5805299254.4558 - val_loss: 11499024525.5741\n",
      "Epoch 2594/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 5970110107.8762 - val_loss: 23921250719.4689\n",
      "Epoch 2595/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 6705076884.0968 - val_loss: 9889040942.3932\n",
      "Epoch 2596/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 6645782888.7338 - val_loss: 10281448265.3975\n",
      "Epoch 2597/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 74us/step - loss: 6273206686.1812 - val_loss: 9921570797.3491\n",
      "Epoch 2598/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 5217769089.6567 - val_loss: 9960189758.9378\n",
      "Epoch 2599/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 5317713700.4479 - val_loss: 11236423970.5654\n",
      "Epoch 2600/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 5584746173.2988 - val_loss: 9691542805.0273\n",
      "Epoch 2601/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 5210776869.0242 - val_loss: 10372725082.7792\n",
      "Epoch 2602/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 5689367804.3984 - val_loss: 9718239964.7865\n",
      "Epoch 2603/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 5172814508.0113 - val_loss: 10010023040.6121\n",
      "Epoch 2604/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 5166864623.0006 - val_loss: 11301193177.5460\n",
      "Epoch 2605/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 5594115160.1666 - val_loss: 10098315729.4807\n",
      "Epoch 2606/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 5585739384.0045 - val_loss: 11231491764.2442\n",
      "Epoch 2607/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 6073993791.0996 - val_loss: 14980759317.5314\n",
      "Epoch 2608/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 5421947760.4412 - val_loss: 9087609869.5381\n",
      "Epoch 2609/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 5475552841.4721 - val_loss: 9460229974.4855\n",
      "Epoch 2610/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 5051160120.4727 - val_loss: 9361446865.8408\n",
      "Epoch 2611/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 4897370081.7468 - val_loss: 9233988015.9955\n",
      "Epoch 2612/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 5089738968.9589 - val_loss: 11606363383.5747\n",
      "Epoch 2613/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 5255838550.4378 - val_loss: 9522484103.8852\n",
      "Epoch 2614/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 4925430915.3855 - val_loss: 10850316212.2802\n",
      "Epoch 2615/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 5116217656.2566 - val_loss: 9219287331.9336\n",
      "Epoch 2616/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 5202106240.3602 - val_loss: 11254398818.1513\n",
      "Epoch 2617/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 5199170619.1199 - val_loss: 9549147164.0844\n",
      "Epoch 2618/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 4870014687.8739 - val_loss: 10946540999.3271\n",
      "Epoch 2619/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 5009283287.8784 - val_loss: 13982505906.2278\n",
      "Epoch 2620/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 4992714968.3827 - val_loss: 8568367913.7665\n",
      "Epoch 2621/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 5011785828.2679 - val_loss: 11987494839.2686\n",
      "Epoch 2622/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 5862934457.6972 - val_loss: 9002491648.3601\n",
      "Epoch 2623/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 5010674871.1041 - val_loss: 8555554197.1353\n",
      "Epoch 2624/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 4709717518.6944 - val_loss: 9041966717.4976\n",
      "Epoch 2625/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 4827185784.8689 - val_loss: 17442386153.8925\n",
      "Epoch 2626/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 5314995419.5521 - val_loss: 9364989302.7466\n",
      "Epoch 2627/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 5045299354.4356 - val_loss: 8734363724.9080\n",
      "Epoch 2628/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 4912346717.7850 - val_loss: 9269729252.3477\n",
      "Epoch 2629/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 4758063577.2200 - val_loss: 8430901754.5992\n",
      "Epoch 2630/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 4770955979.7051 - val_loss: 9060292831.7390\n",
      "Epoch 2631/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 4816758113.8188 - val_loss: 10451795260.4985\n",
      "Epoch 2632/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 5022641630.8655 - val_loss: 8537455685.9589\n",
      "Epoch 2633/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 4988924200.1936 - val_loss: 8193169836.6110\n",
      "Epoch 2634/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 4198236487.4553 - val_loss: 10274541861.0858\n",
      "Epoch 2635/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 4576499070.0551 - val_loss: 8356433180.5885\n",
      "Epoch 2636/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 4405529165.7940 - val_loss: 10106377964.6110\n",
      "Epoch 2637/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 4800734799.9550 - val_loss: 8770731985.1207\n",
      "Epoch 2638/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 4350554887.0591 - val_loss: 8096385669.6529\n",
      "Epoch 2639/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4249462347.8852 - val_loss: 11072324928.1620\n",
      "Epoch 2640/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 4530932283.6421 - val_loss: 9275304284.6785\n",
      "Epoch 2641/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 4470515518.2352 - val_loss: 8482594524.2464\n",
      "Epoch 2642/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 4488168002.2150 - val_loss: 9239595356.1744\n",
      "Epoch 2643/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 4225600029.5329 - val_loss: 7703361707.7108\n",
      "Epoch 2644/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 4443705678.2262 - val_loss: 8070084891.0402\n",
      "Epoch 2645/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 4643462631.9415 - val_loss: 8534492345.3930\n",
      "Epoch 2646/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 5185098250.9488 - val_loss: 8655989011.6591\n",
      "Epoch 2647/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 4171465512.7698 - val_loss: 7955159365.4188\n",
      "Epoch 2648/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 4214380759.5183 - val_loss: 8156794907.7963\n",
      "Epoch 2649/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4417280968.3196 - val_loss: 8113362539.5848\n",
      "Epoch 2650/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 4003617568.5582 - val_loss: 7993424558.6273\n",
      "Epoch 2651/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 4268450160.5492 - val_loss: 8504909887.6219\n",
      "Epoch 2652/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 4137184566.1677 - val_loss: 8717058876.4895\n",
      "Epoch 2653/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 4387152356.3399 - val_loss: 12592069122.8444\n",
      "Epoch 2654/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4390740860.8306 - val_loss: 7770854633.1004\n",
      "Epoch 2655/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 5239148179.0884 - val_loss: 7982521246.2447\n",
      "Epoch 2656/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 4048346427.7411 - val_loss: 7965043198.9018\n",
      "Epoch 2657/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3918203880.9499 - val_loss: 8139298508.2239\n",
      "Epoch 2658/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3962948574.0011 - val_loss: 11277461440.9361\n",
      "Epoch 2659/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 4467321473.0805 - val_loss: 7456065925.4549\n",
      "Epoch 2660/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 4341940722.4221 - val_loss: 8830081622.3055\n",
      "Epoch 2661/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 4962457211.1739 - val_loss: 10492240820.6762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2662/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 4676048258.9533 - val_loss: 7759752019.8211\n",
      "Epoch 2663/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 4114986763.3810 - val_loss: 7796702000.4636\n",
      "Epoch 2664/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 3779231397.3844 - val_loss: 7750228876.0619\n",
      "Epoch 2665/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 3767523922.4761 - val_loss: 7303304140.1519\n",
      "Epoch 2666/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3624568567.6443 - val_loss: 7664370622.5418\n",
      "Epoch 2667/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 4148336697.6252 - val_loss: 7578873043.7491\n",
      "Epoch 2668/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4384710965.9876 - val_loss: 8524047290.1491\n",
      "Epoch 2669/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4062622105.4992 - val_loss: 7455299408.4366\n",
      "Epoch 2670/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3996239582.7214 - val_loss: 9625727134.7848\n",
      "Epoch 2671/7000\n",
      "3554/3554 [==============================] - 0s 113us/step - loss: 4125025360.6033 - val_loss: 10374298539.0807\n",
      "Epoch 2672/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3889822951.1221 - val_loss: 7072913181.4886\n",
      "Epoch 2673/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3571744542.7214 - val_loss: 8361730894.7083\n",
      "Epoch 2674/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3491739183.2527 - val_loss: 7253049272.8889\n",
      "Epoch 2675/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3934114804.4750 - val_loss: 7420030345.9015\n",
      "Epoch 2676/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3873515744.7383 - val_loss: 10011645261.1241\n",
      "Epoch 2677/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 5062765718.4018 - val_loss: 7551177279.6759\n",
      "Epoch 2678/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 4457338319.3067 - val_loss: 6887726980.6087\n",
      "Epoch 2679/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3579515389.2628 - val_loss: 7203747190.6745\n",
      "Epoch 2680/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 3752433464.7608 - val_loss: 6937029372.3634\n",
      "Epoch 2681/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3483638947.7164 - val_loss: 7750879775.9010\n",
      "Epoch 2682/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 3441203731.4845 - val_loss: 7184222514.3899\n",
      "Epoch 2683/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3714805448.6078 - val_loss: 7851623562.1176\n",
      "Epoch 2684/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3772125379.7456 - val_loss: 7526476552.8574\n",
      "Epoch 2685/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4070954788.8801 - val_loss: 8879275667.0290\n",
      "Epoch 2686/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3903023909.4564 - val_loss: 7231519247.9685\n",
      "Epoch 2687/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3447972609.5847 - val_loss: 8060997190.2830\n",
      "Epoch 2688/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3546811075.8537 - val_loss: 7057525491.3980\n",
      "Epoch 2689/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 4080734082.5211 - val_loss: 7901303704.6639\n",
      "Epoch 2690/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 4217199520.0540 - val_loss: 8937871167.9460\n",
      "Epoch 2691/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 3564826847.0096 - val_loss: 7012652097.3862\n",
      "Epoch 2692/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3533743656.0495 - val_loss: 9440945354.7837\n",
      "Epoch 2693/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 4042073795.3495 - val_loss: 6913790563.2675\n",
      "Epoch 2694/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3543465866.7327 - val_loss: 8655363443.2900\n",
      "Epoch 2695/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 3828111361.7288 - val_loss: 6978473213.7316\n",
      "Epoch 2696/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3755810144.9544 - val_loss: 8183713638.9041\n",
      "Epoch 2697/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 3530118061.5959 - val_loss: 10996144717.7361\n",
      "Epoch 2698/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 4432664048.8734 - val_loss: 7106090209.1792\n",
      "Epoch 2699/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3317281371.9122 - val_loss: 7123359565.1601\n",
      "Epoch 2700/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 3825213875.9347 - val_loss: 6804004796.3094\n",
      "Epoch 2701/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3440731055.1446 - val_loss: 6605648215.2776\n",
      "Epoch 2702/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3515021909.2853 - val_loss: 7251101144.5378\n",
      "Epoch 2703/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3967370346.7507 - val_loss: 11511425536.1440\n",
      "Epoch 2704/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3982146855.7614 - val_loss: 6670421940.1001\n",
      "Epoch 2705/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3214882214.3928 - val_loss: 6338275929.0059\n",
      "Epoch 2706/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3329238684.0923 - val_loss: 8203162242.7724\n",
      "Epoch 2707/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3392081927.4913 - val_loss: 6487849528.3128\n",
      "Epoch 2708/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 3323167933.8751 - val_loss: 6534453228.0889\n",
      "Epoch 2709/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 3659755018.9848 - val_loss: 7218366287.3564\n",
      "Epoch 2710/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3508646239.0816 - val_loss: 6617659414.1795\n",
      "Epoch 2711/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3195339619.9797 - val_loss: 6726649105.7148\n",
      "Epoch 2712/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3270500696.7428 - val_loss: 6527060161.8903\n",
      "Epoch 2713/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3494639599.5768 - val_loss: 6832307594.6937\n",
      "Epoch 2714/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3321075160.5267 - val_loss: 6573082211.9156\n",
      "Epoch 2715/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3159462665.7963 - val_loss: 7271684431.2664\n",
      "Epoch 2716/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3337453074.0079 - val_loss: 7406464827.6973\n",
      "Epoch 2717/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3353865168.7473 - val_loss: 8241522252.6920\n",
      "Epoch 2718/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3417255821.0377 - val_loss: 6473013175.5567\n",
      "Epoch 2719/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 3278677782.0416 - val_loss: 7618428278.8816\n",
      "Epoch 2720/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3862377541.4024 - val_loss: 6283897399.0526\n",
      "Epoch 2721/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 3199518287.2347 - val_loss: 6354834948.3927\n",
      "Epoch 2722/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3124718383.8829 - val_loss: 6126781727.4869\n",
      "Epoch 2723/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3020782814.5864 - val_loss: 6993505225.9916\n",
      "Epoch 2724/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3323675930.4356 - val_loss: 7470969333.5584\n",
      "Epoch 2725/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 3367244846.6764 - val_loss: 9382477239.4127\n",
      "Epoch 2726/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 4570718164.4930 - val_loss: 6098438651.3553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2727/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3126476056.6348 - val_loss: 6724275438.1052\n",
      "Epoch 2728/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 3511126437.1683 - val_loss: 6133677947.8233\n",
      "Epoch 2729/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3003299073.0805 - val_loss: 6061609426.3449\n",
      "Epoch 2730/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3058645648.6393 - val_loss: 7104699227.5803\n",
      "Epoch 2731/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3700095319.1491 - val_loss: 6109340434.9030\n",
      "Epoch 2732/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2978439862.9600 - val_loss: 6684464384.7201\n",
      "Epoch 2733/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 3271189873.9539 - val_loss: 9327896037.1038\n",
      "Epoch 2734/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 4174257945.4992 - val_loss: 7870160134.6970\n",
      "Epoch 2735/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 3880919893.7535 - val_loss: 6028871548.6515\n",
      "Epoch 2736/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3018656166.2487 - val_loss: 8123878191.6354\n",
      "Epoch 2737/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2899578227.9707 - val_loss: 6027654874.5541\n",
      "Epoch 2738/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3169139366.2487 - val_loss: 6163506409.6765\n",
      "Epoch 2739/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2829550883.6556 - val_loss: 6588413548.1249\n",
      "Epoch 2740/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3102929529.3011 - val_loss: 6002079245.1060\n",
      "Epoch 2741/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 3623453003.2009 - val_loss: 8844077086.0287\n",
      "Epoch 2742/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 3233981906.7642 - val_loss: 8656596141.2951\n",
      "Epoch 2743/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 4460663165.7670 - val_loss: 7163904470.9536\n",
      "Epoch 2744/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2939511002.3995 - val_loss: 6745676174.5103\n",
      "Epoch 2745/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2937220189.9291 - val_loss: 6994540857.5370\n",
      "Epoch 2746/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2889247449.7873 - val_loss: 6233378599.4802\n",
      "Epoch 2747/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3315399593.2741 - val_loss: 6286194304.9812\n",
      "Epoch 2748/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2916945382.9330 - val_loss: 5701398547.1550\n",
      "Epoch 2749/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3167600434.7552 - val_loss: 5908962849.8813\n",
      "Epoch 2750/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2899407097.0850 - val_loss: 7255669214.7128\n",
      "Epoch 2751/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2898664567.2842 - val_loss: 6461916222.3617\n",
      "Epoch 2752/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3016559968.9904 - val_loss: 6566647465.7305\n",
      "Epoch 2753/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2811159500.7136 - val_loss: 5844489524.5682\n",
      "Epoch 2754/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2857022036.9972 - val_loss: 7676512322.7544\n",
      "Epoch 2755/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3961843976.4997 - val_loss: 9171698707.2990\n",
      "Epoch 2756/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2966259399.3832 - val_loss: 6903980152.8349\n",
      "Epoch 2757/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 3018763198.3073 - val_loss: 6467215212.0889\n",
      "Epoch 2758/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2812484752.6393 - val_loss: 12021405888.7021\n",
      "Epoch 2759/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3134891886.2802 - val_loss: 5720547996.9845\n",
      "Epoch 2760/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2734238988.1013 - val_loss: 5869307008.8281\n",
      "Epoch 2761/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 3025198793.0400 - val_loss: 6122167910.6160\n",
      "Epoch 2762/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2863153719.4643 - val_loss: 8357800603.5443\n",
      "Epoch 2763/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 3429537308.3264 - val_loss: 6594040148.8293\n",
      "Epoch 2764/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2694380635.6241 - val_loss: 5558012896.6751\n",
      "Epoch 2765/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2824914349.6680 - val_loss: 10523770420.3882\n",
      "Epoch 2766/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3297460208.9094 - val_loss: 6083357923.1775\n",
      "Epoch 2767/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3050925622.4558 - val_loss: 6286644917.9004\n",
      "Epoch 2768/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2901034180.1418 - val_loss: 5873263255.9797\n",
      "Epoch 2769/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2842130157.7040 - val_loss: 5970125463.5837\n",
      "Epoch 2770/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2813892665.2651 - val_loss: 6517341867.3868\n",
      "Epoch 2771/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2879849654.4198 - val_loss: 5839382156.5840\n",
      "Epoch 2772/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 3201178874.9578 - val_loss: 5831288538.9142\n",
      "Epoch 2773/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 3024409436.0563 - val_loss: 5875753197.8532\n",
      "Epoch 2774/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2667490173.4789 - val_loss: 10078577349.4188\n",
      "Epoch 2775/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 3360983336.1666 - val_loss: 5781285739.6568\n",
      "Epoch 2776/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2761517425.2335 - val_loss: 10704048344.1778\n",
      "Epoch 2777/7000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 3201894483.2324 - val_loss: 5464446145.2242\n",
      "Epoch 2778/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2745868634.7957 - val_loss: 5839722304.8326\n",
      "Epoch 2779/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2717902849.1525 - val_loss: 6450508447.0188\n",
      "Epoch 2780/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2929052939.8132 - val_loss: 5904298318.3842\n",
      "Epoch 2781/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2905739016.7878 - val_loss: 6615041411.4205\n",
      "Epoch 2782/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2721817272.6888 - val_loss: 6528176157.0520\n",
      "Epoch 2783/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2817650955.3810 - val_loss: 8167656838.8771\n",
      "Epoch 2784/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2811375138.0709 - val_loss: 5496894967.7187\n",
      "Epoch 2785/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2538376184.0225 - val_loss: 5550385435.3643\n",
      "Epoch 2786/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2620931354.9398 - val_loss: 7843928567.9437\n",
      "Epoch 2787/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2735645980.3084 - val_loss: 6677617855.5499\n",
      "Epoch 2788/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 2861898173.5869 - val_loss: 5486204353.5122\n",
      "Epoch 2789/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2539622612.6370 - val_loss: 10428616686.9873\n",
      "Epoch 2790/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 3637219129.4811 - val_loss: 6212865831.5342\n",
      "Epoch 2791/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2589524726.0597 - val_loss: 6719884479.2619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2792/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2712181992.2296 - val_loss: 5478788041.5955\n",
      "Epoch 2793/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 3170069250.0169 - val_loss: 5790580085.5944\n",
      "Epoch 2794/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2668765136.0270 - val_loss: 5338279296.4141\n",
      "Epoch 2795/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2955730370.6291 - val_loss: 6155325246.5058\n",
      "Epoch 2796/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2827286573.8571 - val_loss: 5201222189.9792\n",
      "Epoch 2797/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2589391639.0501 - val_loss: 5874412115.7851\n",
      "Epoch 2798/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2467750631.9415 - val_loss: 5729422523.9224\n",
      "Epoch 2799/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2589110339.5656 - val_loss: 7988434883.6546\n",
      "Epoch 2800/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2640236742.2307 - val_loss: 5620120951.2686\n",
      "Epoch 2801/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2948969551.5498 - val_loss: 5713711075.2315\n",
      "Epoch 2802/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2720583114.9848 - val_loss: 5491341227.6028\n",
      "Epoch 2803/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2990330074.9398 - val_loss: 5653920207.7525\n",
      "Epoch 2804/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2442197272.0585 - val_loss: 5589373189.4909\n",
      "Epoch 2805/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2527884672.9364 - val_loss: 5224612258.6374\n",
      "Epoch 2806/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2569464654.8025 - val_loss: 5401704109.6371\n",
      "Epoch 2807/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2525589156.2319 - val_loss: 7257739645.8037\n",
      "Epoch 2808/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2858984466.4401 - val_loss: 7445466083.7716\n",
      "Epoch 2809/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2611180786.0259 - val_loss: 5218363862.9626\n",
      "Epoch 2810/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2539434084.4119 - val_loss: 8255681777.0217\n",
      "Epoch 2811/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2819825526.8520 - val_loss: 6329268443.7783\n",
      "Epoch 2812/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 3160697559.0861 - val_loss: 5430374096.6886\n",
      "Epoch 2813/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2562343884.3534 - val_loss: 6238237028.8878\n",
      "Epoch 2814/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2563386881.0264 - val_loss: 7327393034.8467\n",
      "Epoch 2815/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2353255498.9848 - val_loss: 5054666408.0743\n",
      "Epoch 2816/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2459895274.3275 - val_loss: 6812573879.9167\n",
      "Epoch 2817/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2409280264.2116 - val_loss: 6284780523.6928\n",
      "Epoch 2818/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2890522862.7845 - val_loss: 7710534018.4124\n",
      "Epoch 2819/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 3260040539.5172 - val_loss: 5253999067.5623\n",
      "Epoch 2820/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2491691839.0816 - val_loss: 7220868443.0942\n",
      "Epoch 2821/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2758685120.4100 - val_loss: 5403802627.6006\n",
      "Epoch 2822/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2587616092.9207 - val_loss: 5815567118.4743\n",
      "Epoch 2823/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2831930883.3134 - val_loss: 5086210036.9103\n",
      "Epoch 2824/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2525667773.4429 - val_loss: 7391950890.5767\n",
      "Epoch 2825/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2675628090.0574 - val_loss: 5060931231.1449\n",
      "Epoch 2826/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2385979816.5537 - val_loss: 5090355689.8385\n",
      "Epoch 2827/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2432716996.7901 - val_loss: 4936799979.4048\n",
      "Epoch 2828/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2791823767.0501 - val_loss: 5234465069.6191\n",
      "Epoch 2829/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2424029944.6528 - val_loss: 5370215728.5356\n",
      "Epoch 2830/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2561395553.2425 - val_loss: 5941578274.0973\n",
      "Epoch 2831/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2865459003.5701 - val_loss: 5797338820.8068\n",
      "Epoch 2832/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2368617098.1564 - val_loss: 5437285417.3705\n",
      "Epoch 2833/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2544149453.6500 - val_loss: 5488754444.9620\n",
      "Epoch 2834/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2764051569.0895 - val_loss: 6084040703.7120\n",
      "Epoch 2835/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2653500893.5689 - val_loss: 5419705354.9277\n",
      "Epoch 2836/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2397840716.2093 - val_loss: 5207257202.0568\n",
      "Epoch 2837/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2512011856.4592 - val_loss: 5017455813.2388\n",
      "Epoch 2838/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2270046458.7417 - val_loss: 5005665799.4532\n",
      "Epoch 2839/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 2447335111.8875 - val_loss: 5001864842.4056\n",
      "Epoch 2840/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2540389384.4277 - val_loss: 5222127992.4928\n",
      "Epoch 2841/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2286702175.9460 - val_loss: 4971321301.0093\n",
      "Epoch 2842/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2133643264.1441 - val_loss: 6016282914.3404\n",
      "Epoch 2843/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2584337677.1097 - val_loss: 5896071395.7806\n",
      "Epoch 2844/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2661666425.0129 - val_loss: 6107975814.8051\n",
      "Epoch 2845/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2540742405.3663 - val_loss: 5288933032.0743\n",
      "Epoch 2846/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2575093929.2200 - val_loss: 6080862927.9685\n",
      "Epoch 2847/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2419987441.8818 - val_loss: 6591181548.7730\n",
      "Epoch 2848/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 2435962058.3365 - val_loss: 5769858072.3398\n",
      "Epoch 2849/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2265212646.4918 - val_loss: 7215304946.8219\n",
      "Epoch 2850/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2440107388.2724 - val_loss: 6325324477.9657\n",
      "Epoch 2851/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2221435072.8284 - val_loss: 5550138671.8155\n",
      "Epoch 2852/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2361334852.8621 - val_loss: 6194726927.6895\n",
      "Epoch 2853/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2344374248.9499 - val_loss: 5240980990.8838\n",
      "Epoch 2854/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2369090338.1429 - val_loss: 5213459580.5795\n",
      "Epoch 2855/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2491812702.3883 - val_loss: 4882044262.7241\n",
      "Epoch 2856/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2189564295.2752 - val_loss: 5706572141.7992\n",
      "Epoch 2857/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 72us/step - loss: 2290932804.7631 - val_loss: 5895266991.2754\n",
      "Epoch 2858/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2151082882.2330 - val_loss: 5444136177.7778\n",
      "Epoch 2859/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2276076210.3140 - val_loss: 4857078270.2537\n",
      "Epoch 2860/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2477731244.8036 - val_loss: 6452650127.6624\n",
      "Epoch 2861/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2492815987.6826 - val_loss: 5402263089.7935\n",
      "Epoch 2862/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2322860508.6325 - val_loss: 5607715440.1935\n",
      "Epoch 2863/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2380920649.6162 - val_loss: 5293885112.8529\n",
      "Epoch 2864/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2372130641.5397 - val_loss: 6977168407.4757\n",
      "Epoch 2865/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2333289383.1041 - val_loss: 5325833303.9977\n",
      "Epoch 2866/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2289042280.5357 - val_loss: 4972976772.3927\n",
      "Epoch 2867/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2198825418.4086 - val_loss: 5157553673.8745\n",
      "Epoch 2868/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2104249426.2600 - val_loss: 5179967727.0774\n",
      "Epoch 2869/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2413720917.1412 - val_loss: 12030757270.2875\n",
      "Epoch 2870/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 4704924099.2054 - val_loss: 4988945618.5969\n",
      "Epoch 2871/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2164739509.6230 - val_loss: 4707605567.6579\n",
      "Epoch 2872/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2119579353.1750 - val_loss: 4999095903.8110\n",
      "Epoch 2873/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2069784457.0039 - val_loss: 5123740732.1834\n",
      "Epoch 2874/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2373322929.1975 - val_loss: 4724191341.9972\n",
      "Epoch 2875/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2606123790.8385 - val_loss: 4821402119.8312\n",
      "Epoch 2876/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2129824187.1379 - val_loss: 4936926161.8903\n",
      "Epoch 2877/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2101926441.0580 - val_loss: 5065312716.6560\n",
      "Epoch 2878/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2573888540.7541 - val_loss: 6374459100.0484\n",
      "Epoch 2879/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2138714364.6866 - val_loss: 6237127490.7949\n",
      "Epoch 2880/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2531959485.5869 - val_loss: 4912575553.4942\n",
      "Epoch 2881/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 2415746954.4446 - val_loss: 4618230190.3752\n",
      "Epoch 2882/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2080975113.6522 - val_loss: 5540313252.7617\n",
      "Epoch 2883/7000\n",
      "3554/3554 [==============================] - 1s 142us/step - loss: 2156689852.2904 - val_loss: 7445715451.4633\n",
      "Epoch 2884/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2382070184.1216 - val_loss: 4870954575.3834\n",
      "Epoch 2885/7000\n",
      "3554/3554 [==============================] - 0s 122us/step - loss: 2294554011.4620 - val_loss: 7108109151.4869\n",
      "Epoch 2886/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 2499677734.8970 - val_loss: 5032980234.4416\n",
      "Epoch 2887/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2290079019.3630 - val_loss: 7894159016.7404\n",
      "Epoch 2888/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2223196735.6759 - val_loss: 5380468864.4141\n",
      "Epoch 2889/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2123545879.7704 - val_loss: 4544583313.4267\n",
      "Epoch 2890/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2216371485.6770 - val_loss: 4567590587.9134\n",
      "Epoch 2891/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2033214887.2572 - val_loss: 4698716969.2264\n",
      "Epoch 2892/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2216595391.3157 - val_loss: 7011214360.7719\n",
      "Epoch 2893/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2154173863.5453 - val_loss: 7161396595.0380\n",
      "Epoch 2894/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 3451233093.0782 - val_loss: 4753047997.3536\n",
      "Epoch 2895/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2296502902.8520 - val_loss: 4924738940.0034\n",
      "Epoch 2896/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 2126074778.5796 - val_loss: 4666675277.8262\n",
      "Epoch 2897/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2270594230.7440 - val_loss: 4623292377.5820\n",
      "Epoch 2898/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 2044623044.0158 - val_loss: 7272342745.5460\n",
      "Epoch 2899/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2603476044.6415 - val_loss: 5450229476.1947\n",
      "Epoch 2900/7000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 2111865028.0338 - val_loss: 5120559665.1882\n",
      "Epoch 2901/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 2012163762.8182 - val_loss: 5083253141.2793\n",
      "Epoch 2902/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 2274907062.3838 - val_loss: 4406546827.5938\n",
      "Epoch 2903/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 2249189371.8222 - val_loss: 5750722143.9190\n",
      "Epoch 2904/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 2145087161.8413 - val_loss: 4792994654.7668\n",
      "Epoch 2905/7000\n",
      "3554/3554 [==============================] - 0s 134us/step - loss: 2000954914.9353 - val_loss: 4957933942.7466\n",
      "Epoch 2906/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2213101358.6404 - val_loss: 4491574518.2785\n",
      "Epoch 2907/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 1979547129.3371 - val_loss: 4643046234.3741\n",
      "Epoch 2908/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 2090909659.5521 - val_loss: 5015746673.0757\n",
      "Epoch 2909/7000\n",
      "3554/3554 [==============================] - 0s 129us/step - loss: 2320968696.0765 - val_loss: 4720500945.9848\n",
      "Epoch 2910/7000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 2087719629.5059 - val_loss: 4694148384.3691\n",
      "Epoch 2911/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 2074077097.1300 - val_loss: 5362610914.8714\n",
      "Epoch 2912/7000\n",
      "3554/3554 [==============================] - 0s 130us/step - loss: 2229084873.7873 - val_loss: 4971626195.1550\n",
      "Epoch 2913/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2058095858.8903 - val_loss: 5246042220.1136\n",
      "Epoch 2914/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2187891784.4007 - val_loss: 4827905157.2658\n",
      "Epoch 2915/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2028260094.7034 - val_loss: 5182525783.3677\n",
      "Epoch 2916/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2404598387.5385 - val_loss: 4678123615.5049\n",
      "Epoch 2917/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2365821762.8452 - val_loss: 5272639940.3027\n",
      "Epoch 2918/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 2457882852.4840 - val_loss: 7695714635.1077\n",
      "Epoch 2919/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2286001680.8554 - val_loss: 5465877556.8473\n",
      "Epoch 2920/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 2443856109.9921 - val_loss: 6042091867.1662\n",
      "Epoch 2921/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 2391379381.9156 - val_loss: 5171784307.0020\n",
      "Epoch 2922/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 93us/step - loss: 2104000172.5875 - val_loss: 4483271530.9187\n",
      "Epoch 2923/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2316157591.8424 - val_loss: 4877102036.1001\n",
      "Epoch 2924/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2200768392.9994 - val_loss: 4656447341.0610\n",
      "Epoch 2925/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1904572383.8109 - val_loss: 4405196693.1893\n",
      "Epoch 2926/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1798447282.9263 - val_loss: 5763306771.6591\n",
      "Epoch 2927/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 2047461987.1154 - val_loss: 4328876031.4239\n",
      "Epoch 2928/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1955772279.4688 - val_loss: 4403066458.6082\n",
      "Epoch 2929/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1966910824.1576 - val_loss: 4682240412.9125\n",
      "Epoch 2930/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2023437335.2662 - val_loss: 5119186257.3187\n",
      "Epoch 2931/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2053299795.1964 - val_loss: 4338229258.9817\n",
      "Epoch 2932/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2094128255.3157 - val_loss: 4368478744.6278\n",
      "Epoch 2933/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2094757397.9696 - val_loss: 4835176180.1902\n",
      "Epoch 2934/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2143155274.0484 - val_loss: 4590552098.8309\n",
      "Epoch 2935/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2220271486.3433 - val_loss: 5628828870.7511\n",
      "Epoch 2936/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2463109496.9842 - val_loss: 5503893496.9969\n",
      "Epoch 2937/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2001136945.7017 - val_loss: 4342442102.1795\n",
      "Epoch 2938/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2154057397.3393 - val_loss: 4212395019.2338\n",
      "Epoch 2939/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2025551749.7625 - val_loss: 4389041998.8163\n",
      "Epoch 2940/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 2098006715.5701 - val_loss: 5561736194.6644\n",
      "Epoch 2941/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2112415836.3804 - val_loss: 4303300003.2495\n",
      "Epoch 2942/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1852023434.8768 - val_loss: 4545411854.5463\n",
      "Epoch 2943/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1959060419.4935 - val_loss: 4588983606.3325\n",
      "Epoch 2944/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2541592384.5402 - val_loss: 5117024138.6397\n",
      "Epoch 2945/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1957052873.4721 - val_loss: 4167450187.1797\n",
      "Epoch 2946/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2125648041.4181 - val_loss: 5280161236.2532\n",
      "Epoch 2947/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2219471840.0180 - val_loss: 5823215783.6062\n",
      "Epoch 2948/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2173056740.4570 - val_loss: 5067776833.2062\n",
      "Epoch 2949/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1944710922.2285 - val_loss: 4857725374.5418\n",
      "Epoch 2950/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2067695341.1998 - val_loss: 4268923414.6835\n",
      "Epoch 2951/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2038019175.0051 - val_loss: 5038873180.1744\n",
      "Epoch 2952/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 2391682374.7349 - val_loss: 4486268915.1100\n",
      "Epoch 2953/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2127723007.1356 - val_loss: 5272042573.1961\n",
      "Epoch 2954/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2460660101.2583 - val_loss: 4791644491.8639\n",
      "Epoch 2955/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1958027197.5689 - val_loss: 4232011269.9409\n",
      "Epoch 2956/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1731870120.8689 - val_loss: 4346399760.8236\n",
      "Epoch 2957/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1923308582.6089 - val_loss: 6155744633.5550\n",
      "Epoch 2958/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2432783873.5847 - val_loss: 6021570193.3907\n",
      "Epoch 2959/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2261602655.3697 - val_loss: 4451026105.5010\n",
      "Epoch 2960/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1837961659.7141 - val_loss: 4229662664.7314\n",
      "Epoch 2961/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 2119057363.9167 - val_loss: 4508435015.5387\n",
      "Epoch 2962/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1937932896.0540 - val_loss: 5196301287.0121\n",
      "Epoch 2963/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2025035298.5751 - val_loss: 4365170760.2633\n",
      "Epoch 2964/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1807962462.5053 - val_loss: 4594883024.8326\n",
      "Epoch 2965/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2217972875.4530 - val_loss: 4505869757.0295\n",
      "Epoch 2966/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1786762010.9398 - val_loss: 4491962442.4596\n",
      "Epoch 2967/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1878331581.7738 - val_loss: 4429817133.4481\n",
      "Epoch 2968/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1924389687.4643 - val_loss: 4784973745.4267\n",
      "Epoch 2969/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2240395168.1981 - val_loss: 4129162042.0051\n",
      "Epoch 2970/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1783471875.1694 - val_loss: 5771603724.8900\n",
      "Epoch 2971/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1861265899.5431 - val_loss: 4399681216.7471\n",
      "Epoch 2972/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1819229705.7963 - val_loss: 4118292632.0158\n",
      "Epoch 2973/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2009677085.3528 - val_loss: 4554912164.8968\n",
      "Epoch 2974/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1830059283.8807 - val_loss: 4224855633.8768\n",
      "Epoch 2975/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2350368811.5521 - val_loss: 4609932217.6990\n",
      "Epoch 2976/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1875950575.4328 - val_loss: 4702799688.7674\n",
      "Epoch 2977/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2010255918.5684 - val_loss: 4169104853.6754\n",
      "Epoch 2978/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1824885573.6545 - val_loss: 4493836033.8993\n",
      "Epoch 2979/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1868556363.8334 - val_loss: 4749904435.9921\n",
      "Epoch 2980/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1809119578.1835 - val_loss: 4242191836.6065\n",
      "Epoch 2981/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1809727338.9308 - val_loss: 5148413437.1916\n",
      "Epoch 2982/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 2026122364.5065 - val_loss: 4965351108.5367\n",
      "Epoch 2983/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 2006650363.8222 - val_loss: 4816840044.1609\n",
      "Epoch 2984/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1860870240.5943 - val_loss: 4242201601.2962\n",
      "Epoch 2985/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1894893812.0788 - val_loss: 4500947838.3077\n",
      "Epoch 2986/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1779507238.3208 - val_loss: 4356622755.8616\n",
      "Epoch 2987/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1835282524.7766 - val_loss: 6283597198.9423\n",
      "Epoch 2988/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 2434890369.9449 - val_loss: 4267989535.8650\n",
      "Epoch 2989/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1920192874.2105 - val_loss: 3994703311.4644\n",
      "Epoch 2990/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1969655075.0906 - val_loss: 4372270269.3716\n",
      "Epoch 2991/7000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 1768627031.7344 - val_loss: 4103057998.0602\n",
      "Epoch 2992/7000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 2168888651.9212 - val_loss: 6418833263.2574\n",
      "Epoch 2993/7000\n",
      "3554/3554 [==============================] - 1s 146us/step - loss: 2257854875.6061 - val_loss: 4562991164.1474\n",
      "Epoch 2994/7000\n",
      "3554/3554 [==============================] - 0s 132us/step - loss: 1719131623.3382 - val_loss: 4089546602.6487\n",
      "Epoch 2995/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1749248668.5965 - val_loss: 4433447976.9564\n",
      "Epoch 2996/7000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 1865764404.2949 - val_loss: 4547518868.2892\n",
      "Epoch 2997/7000\n",
      "3554/3554 [==============================] - 0s 125us/step - loss: 1865389444.7181 - val_loss: 5076125705.5842\n",
      "Epoch 2998/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 1858065925.9426 - val_loss: 4047648444.3814\n",
      "Epoch 2999/7000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 1665039014.2487 - val_loss: 4675561647.8459\n",
      "Epoch 3000/7000\n",
      "3554/3554 [==============================] - 0s 139us/step - loss: 2076775756.3534 - val_loss: 5295303176.7561\n",
      "Epoch 3001/7000\n",
      "3554/3554 [==============================] - 0s 127us/step - loss: 1889329902.5684 - val_loss: 3882936622.7353\n",
      "Epoch 3002/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1939125438.4513 - val_loss: 6390430760.6864\n",
      "Epoch 3003/7000\n",
      "3554/3554 [==============================] - 0s 140us/step - loss: 2163910682.0754 - val_loss: 6425360876.1249\n",
      "Epoch 3004/7000\n",
      "3554/3554 [==============================] - 1s 145us/step - loss: 2521811417.3641 - val_loss: 4069413422.7713\n",
      "Epoch 3005/7000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 1831811122.1339 - val_loss: 10780384667.6163\n",
      "Epoch 3006/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 2381610583.5903 - val_loss: 3973802011.2743\n",
      "Epoch 3007/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1636515708.6866 - val_loss: 4478391686.8051\n",
      "Epoch 3008/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 1830433261.5599 - val_loss: 3941617281.8723\n",
      "Epoch 3009/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1709614102.2577 - val_loss: 4632113909.6844\n",
      "Epoch 3010/7000\n",
      "3554/3554 [==============================] - 0s 139us/step - loss: 2245684985.3731 - val_loss: 3940442503.6872\n",
      "Epoch 3011/7000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 1936720968.6078 - val_loss: 3987091434.5046\n",
      "Epoch 3012/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1727913631.1896 - val_loss: 3981161020.3454\n",
      "Epoch 3013/7000\n",
      "3554/3554 [==============================] - 0s 135us/step - loss: 1542418514.1159 - val_loss: 4555180399.3294\n",
      "Epoch 3014/7000\n",
      "3554/3554 [==============================] - 0s 136us/step - loss: 1747152561.7738 - val_loss: 5091707458.0748\n",
      "Epoch 3015/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 1887702295.3382 - val_loss: 4664658429.3446\n",
      "Epoch 3016/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1673473137.2335 - val_loss: 4286689883.6703\n",
      "Epoch 3017/7000\n",
      "3554/3554 [==============================] - 0s 126us/step - loss: 1621884085.5914 - val_loss: 4451984962.8264\n",
      "Epoch 3018/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 1976109511.9055 - val_loss: 4071142725.0228\n",
      "Epoch 3019/7000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 1770063779.9077 - val_loss: 4859305158.1750\n",
      "Epoch 3020/7000\n",
      "3554/3554 [==============================] - 0s 128us/step - loss: 1784986425.3371 - val_loss: 4046139726.9963\n",
      "Epoch 3021/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 1977878851.3180 - val_loss: 5753503801.3930\n",
      "Epoch 3022/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1799494536.1396 - val_loss: 4439069156.1001\n",
      "Epoch 3023/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 2532183435.6151 - val_loss: 3987609354.7657\n",
      "Epoch 3024/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 1566278807.2662 - val_loss: 7935776813.6551\n",
      "Epoch 3025/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 2030819886.2825 - val_loss: 3958030434.8354\n",
      "Epoch 3026/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1717288270.5143 - val_loss: 3973822210.3044\n",
      "Epoch 3027/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1667359181.0017 - val_loss: 4744443519.9010\n",
      "Epoch 3028/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1884940697.7513 - val_loss: 4276655213.9702\n",
      "Epoch 3029/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1852348492.9297 - val_loss: 4676488161.6833\n",
      "Epoch 3030/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1975487154.9983 - val_loss: 4627765349.2658\n",
      "Epoch 3031/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1854329637.1683 - val_loss: 5500826476.1609\n",
      "Epoch 3032/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 2064394700.5695 - val_loss: 5032233158.9581\n",
      "Epoch 3033/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1793360706.9893 - val_loss: 4234410899.4430\n",
      "Epoch 3034/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1657719007.4057 - val_loss: 4031058031.0594\n",
      "Epoch 3035/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1576403792.9308 - val_loss: 3861042241.0622\n",
      "Epoch 3036/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1710371210.5526 - val_loss: 4024484168.0293\n",
      "Epoch 3037/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1902082657.3146 - val_loss: 3723012875.1707\n",
      "Epoch 3038/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1639211190.9600 - val_loss: 3814920968.8934\n",
      "Epoch 3039/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 2011756063.1176 - val_loss: 3778047137.8093\n",
      "Epoch 3040/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1899643762.1024 - val_loss: 4477793233.6248\n",
      "Epoch 3041/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1713187403.0568 - val_loss: 3924661859.7986\n",
      "Epoch 3042/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1676557434.3815 - val_loss: 4534183204.7752\n",
      "Epoch 3043/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1700176880.1080 - val_loss: 3769761299.5871\n",
      "Epoch 3044/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1724181467.4440 - val_loss: 3920540604.0754\n",
      "Epoch 3045/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1712440833.3686 - val_loss: 4949291806.4608\n",
      "Epoch 3046/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1757362626.0529 - val_loss: 4358342538.4056\n",
      "Epoch 3047/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1788215555.4395 - val_loss: 4206078188.4850\n",
      "Epoch 3048/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1690972446.3973 - val_loss: 5974118742.1975\n",
      "Epoch 3049/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1987276254.3973 - val_loss: 4603398068.6042\n",
      "Epoch 3050/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1610431611.6421 - val_loss: 4265350202.0771\n",
      "Epoch 3051/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1859986318.6224 - val_loss: 3763540852.6762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3052/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1757145744.6393 - val_loss: 3790876737.6923\n",
      "Epoch 3053/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1740428638.3253 - val_loss: 4077682518.4135\n",
      "Epoch 3054/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1761394426.8137 - val_loss: 4087583007.2349\n",
      "Epoch 3055/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1749911551.4237 - val_loss: 4085037834.9727\n",
      "Epoch 3056/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1892904750.8205 - val_loss: 4712523958.1885\n",
      "Epoch 3057/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 2159488111.3607 - val_loss: 4209455688.4793\n",
      "Epoch 3058/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1711399098.6427 - val_loss: 4293915676.5885\n",
      "Epoch 3059/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1748893447.3472 - val_loss: 3773619557.6079\n",
      "Epoch 3060/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1590399250.0079 - val_loss: 3988785728.8101\n",
      "Epoch 3061/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1537477837.2358 - val_loss: 4286624197.1668\n",
      "Epoch 3062/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1563989817.9494 - val_loss: 4471637751.2146\n",
      "Epoch 3063/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1723190315.9392 - val_loss: 6506264427.3688\n",
      "Epoch 3064/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2015653844.1328 - val_loss: 3729795523.1865\n",
      "Epoch 3065/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1598977537.6927 - val_loss: 4039955446.4225\n",
      "Epoch 3066/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1708748959.1896 - val_loss: 3897129324.5570\n",
      "Epoch 3067/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 2008978625.3326 - val_loss: 4579683598.7983\n",
      "Epoch 3068/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1761310090.3725 - val_loss: 4262816928.3691\n",
      "Epoch 3069/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1598353823.3337 - val_loss: 3902453884.2644\n",
      "Epoch 3070/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1617034110.4311 - val_loss: 3999386028.8000\n",
      "Epoch 3071/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1746165455.1266 - val_loss: 4462675936.7471\n",
      "Epoch 3072/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1671746336.3421 - val_loss: 4419665364.3972\n",
      "Epoch 3073/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1776529320.4097 - val_loss: 11651096523.5038\n",
      "Epoch 3074/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 2372953118.5414 - val_loss: 6577318118.4540\n",
      "Epoch 3075/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1904481780.4029 - val_loss: 3988020847.7750\n",
      "Epoch 3076/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1677390578.3084 - val_loss: 4108737649.1657\n",
      "Epoch 3077/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1678762645.0332 - val_loss: 4708924712.2003\n",
      "Epoch 3078/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1755125336.8509 - val_loss: 3822561322.0186\n",
      "Epoch 3079/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1644664625.9178 - val_loss: 3905606480.0045\n",
      "Epoch 3080/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1869357143.8784 - val_loss: 3648167524.6717\n",
      "Epoch 3081/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1707796150.0957 - val_loss: 5561651695.4374\n",
      "Epoch 3082/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1964066276.0518 - val_loss: 4552829812.1181\n",
      "Epoch 3083/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 2026070945.0625 - val_loss: 4268650419.1640\n",
      "Epoch 3084/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1999656473.2110 - val_loss: 6238800367.7255\n",
      "Epoch 3085/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1593327083.2549 - val_loss: 3729252740.1406\n",
      "Epoch 3086/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1574048082.6922 - val_loss: 4143622407.2371\n",
      "Epoch 3087/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1620327813.2943 - val_loss: 3862552779.8278\n",
      "Epoch 3088/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1817646086.5549 - val_loss: 3772976962.2774\n",
      "Epoch 3089/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1500424199.6353 - val_loss: 3665784066.0163\n",
      "Epoch 3090/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1742877417.4541 - val_loss: 3871987931.7153\n",
      "Epoch 3091/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1655697575.7614 - val_loss: 3911013647.7345\n",
      "Epoch 3092/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1474602812.8666 - val_loss: 4615813349.6439\n",
      "Epoch 3093/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1716588085.5194 - val_loss: 4925441176.9159\n",
      "Epoch 3094/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1511796091.6421 - val_loss: 3809686694.8366\n",
      "Epoch 3095/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1729865901.0917 - val_loss: 3768460605.3176\n",
      "Epoch 3096/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1700116612.3219 - val_loss: 6020516490.4056\n",
      "Epoch 3097/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1560022602.2690 - val_loss: 4352595668.7212\n",
      "Epoch 3098/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1548847014.6809 - val_loss: 6344869489.7778\n",
      "Epoch 3099/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1728157374.3950 - val_loss: 3589246186.4506\n",
      "Epoch 3100/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1708221319.5633 - val_loss: 3731033980.9485\n",
      "Epoch 3101/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1852618537.4181 - val_loss: 4860810544.5356\n",
      "Epoch 3102/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1722087755.7772 - val_loss: 5153471926.5485\n",
      "Epoch 3103/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1752631196.3084 - val_loss: 4326472379.0245\n",
      "Epoch 3104/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1755771582.2352 - val_loss: 4042692235.3058\n",
      "Epoch 3105/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1503615392.0000 - val_loss: 3566744218.2301\n",
      "Epoch 3106/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1578870974.7394 - val_loss: 4558725127.5612\n",
      "Epoch 3107/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2606315525.1863 - val_loss: 7321118336.6661\n",
      "Epoch 3108/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 2020965537.8638 - val_loss: 3871180828.4309\n",
      "Epoch 3109/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1519011637.3033 - val_loss: 3678978305.4942\n",
      "Epoch 3110/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 1777563721.7603 - val_loss: 3950938572.3274\n",
      "Epoch 3111/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1532740507.7862 - val_loss: 5150011590.7015\n",
      "Epoch 3112/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1396999190.3658 - val_loss: 3558981530.3831\n",
      "Epoch 3113/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1579732471.7884 - val_loss: 3590952757.6844\n",
      "Epoch 3114/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1670607266.0709 - val_loss: 4384512539.1482\n",
      "Epoch 3115/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1577762268.7046 - val_loss: 3680718182.9581\n",
      "Epoch 3116/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1545193780.7991 - val_loss: 4007859493.0138\n",
      "Epoch 3117/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 73us/step - loss: 1485086735.2527 - val_loss: 3634033551.4779\n",
      "Epoch 3118/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1589920649.8323 - val_loss: 3889190145.5842\n",
      "Epoch 3119/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1682421165.5959 - val_loss: 6072139211.1257\n",
      "Epoch 3120/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2582098494.7394 - val_loss: 4005624188.5255\n",
      "Epoch 3121/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1688479469.4159 - val_loss: 3905901557.4504\n",
      "Epoch 3122/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1499789652.2769 - val_loss: 3433745975.4847\n",
      "Epoch 3123/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1602914260.0608 - val_loss: 4335178963.7851\n",
      "Epoch 3124/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1602881928.5357 - val_loss: 3672178590.3167\n",
      "Epoch 3125/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1589605043.3044 - val_loss: 5046556952.7336\n",
      "Epoch 3126/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1594953993.0400 - val_loss: 5746397901.3041\n",
      "Epoch 3127/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1588523720.6078 - val_loss: 3702827732.6155\n",
      "Epoch 3128/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1571309062.6269 - val_loss: 3732143852.1789\n",
      "Epoch 3129/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2099552902.8070 - val_loss: 3774120383.7300\n",
      "Epoch 3130/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1522754918.3928 - val_loss: 3752896152.8079\n",
      "Epoch 3131/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1417314536.9499 - val_loss: 4958289167.1944\n",
      "Epoch 3132/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1480877453.9741 - val_loss: 4634610462.8928\n",
      "Epoch 3133/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1472603524.7541 - val_loss: 4050396067.6186\n",
      "Epoch 3134/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1594622493.0647 - val_loss: 5499074432.0360\n",
      "Epoch 3135/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2011812330.1024 - val_loss: 3634655353.2669\n",
      "Epoch 3136/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1692852020.4389 - val_loss: 3681228130.6644\n",
      "Epoch 3137/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1597648016.2690 - val_loss: 3525000433.8138\n",
      "Epoch 3138/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1412379206.6809 - val_loss: 4407812863.2439\n",
      "Epoch 3139/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1640753179.3720 - val_loss: 4140858342.7241\n",
      "Epoch 3140/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1678238075.9662 - val_loss: 5286104544.4231\n",
      "Epoch 3141/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1572613381.0782 - val_loss: 4043949036.2689\n",
      "Epoch 3142/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1483040481.6027 - val_loss: 3862670295.5297\n",
      "Epoch 3143/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1399339697.4857 - val_loss: 4137244784.5536\n",
      "Epoch 3144/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 2045461467.4080 - val_loss: 4323538607.7075\n",
      "Epoch 3145/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1939512131.8537 - val_loss: 4967128707.2045\n",
      "Epoch 3146/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1461078588.2904 - val_loss: 3631002464.8551\n",
      "Epoch 3147/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1515902065.2335 - val_loss: 3600784297.3165\n",
      "Epoch 3148/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1557587041.9268 - val_loss: 4001241057.7733\n",
      "Epoch 3149/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 1554007786.3905 - val_loss: 3709680390.6970\n",
      "Epoch 3150/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1615497517.0917 - val_loss: 3452490456.8979\n",
      "Epoch 3151/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1388030216.4997 - val_loss: 3550377694.2627\n",
      "Epoch 3152/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1420569098.5166 - val_loss: 4635582827.1167\n",
      "Epoch 3153/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1595180921.4631 - val_loss: 6520913484.9080\n",
      "Epoch 3154/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1570883574.2938 - val_loss: 4481708776.2903\n",
      "Epoch 3155/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 1672924371.8807 - val_loss: 3516099542.7286\n",
      "Epoch 3156/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1422343447.2302 - val_loss: 4506983532.5930\n",
      "Epoch 3157/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1404449339.4710 - val_loss: 3924421756.6155\n",
      "Epoch 3158/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1488982880.5582 - val_loss: 5542472485.8779\n",
      "Epoch 3159/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1584243043.3118 - val_loss: 3905780352.6931\n",
      "Epoch 3160/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1388517936.4772 - val_loss: 4165653031.4982\n",
      "Epoch 3161/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1525653755.5701 - val_loss: 3724851249.8678\n",
      "Epoch 3162/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1531547817.1660 - val_loss: 3671286140.5255\n",
      "Epoch 3163/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1643156286.8948 - val_loss: 4520281563.8143\n",
      "Epoch 3164/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1546763444.1508 - val_loss: 4115467685.2478\n",
      "Epoch 3165/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1612060124.1351 - val_loss: 5711562236.9755\n",
      "Epoch 3166/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1514167650.0349 - val_loss: 3920172670.4068\n",
      "Epoch 3167/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1625210676.0068 - val_loss: 4834091964.8855\n",
      "Epoch 3168/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1791494173.1007 - val_loss: 4880692933.4549\n",
      "Epoch 3169/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1807727728.7653 - val_loss: 4525320107.3283\n",
      "Epoch 3170/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1436322619.5205 - val_loss: 3768098322.7747\n",
      "Epoch 3171/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1506900972.0473 - val_loss: 4602959470.8073\n",
      "Epoch 3172/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1460582455.5363 - val_loss: 3461934296.5108\n",
      "Epoch 3173/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1322189708.3174 - val_loss: 3695802897.0599\n",
      "Epoch 3174/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1425369368.0585 - val_loss: 5615755569.3997\n",
      "Epoch 3175/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1717695901.7490 - val_loss: 3863316808.3938\n",
      "Epoch 3176/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1797673030.5909 - val_loss: 3909166807.5297\n",
      "Epoch 3177/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1514808047.9010 - val_loss: 3950025475.5331\n",
      "Epoch 3178/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1434525532.1103 - val_loss: 3464533340.6605\n",
      "Epoch 3179/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1287095239.8154 - val_loss: 4885274963.1978\n",
      "Epoch 3180/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1765661398.5098 - val_loss: 6889670755.0875\n",
      "Epoch 3181/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1737237690.7057 - val_loss: 3748068411.1752\n",
      "Epoch 3182/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1558101361.5217 - val_loss: 4982678188.6830\n",
      "Epoch 3183/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 1570190022.8115 - val_loss: 3785264955.4813\n",
      "Epoch 3184/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1394702991.3067 - val_loss: 4310884078.6633\n",
      "Epoch 3185/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1813519221.0580 - val_loss: 3641922427.3328\n",
      "Epoch 3186/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1402232732.4524 - val_loss: 7538393831.7322\n",
      "Epoch 3187/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1593660469.3033 - val_loss: 3831736833.1522\n",
      "Epoch 3188/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1881757253.5104 - val_loss: 3736743382.4495\n",
      "Epoch 3189/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1535889068.4074 - val_loss: 4312659975.0886\n",
      "Epoch 3190/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1462551372.6415 - val_loss: 3525357234.1558\n",
      "Epoch 3191/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1527647327.0816 - val_loss: 4289660190.0737\n",
      "Epoch 3192/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1688712326.4108 - val_loss: 4505463646.4068\n",
      "Epoch 3193/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1595460707.4035 - val_loss: 6330007437.9342\n",
      "Epoch 3194/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1727841002.4626 - val_loss: 4576042596.9238\n",
      "Epoch 3195/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1488492397.5599 - val_loss: 3568859001.6900\n",
      "Epoch 3196/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1621379048.8059 - val_loss: 3942806137.5370\n",
      "Epoch 3197/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1570886957.9561 - val_loss: 3759007797.8824\n",
      "Epoch 3198/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1631705093.6185 - val_loss: 3865114263.5297\n",
      "Epoch 3199/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1341383702.0416 - val_loss: 3390463067.1842\n",
      "Epoch 3200/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1483077176.1846 - val_loss: 4597149973.0093\n",
      "Epoch 3201/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1365592992.9184 - val_loss: 3906588819.0110\n",
      "Epoch 3202/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1793241209.1390 - val_loss: 4278748038.3730\n",
      "Epoch 3203/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1519694928.8914 - val_loss: 4279475854.9198\n",
      "Epoch 3204/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1319318940.4885 - val_loss: 3554995530.2796\n",
      "Epoch 3205/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1385779486.5864 - val_loss: 3619155699.3125\n",
      "Epoch 3206/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1378220691.8087 - val_loss: 3542454877.3086\n",
      "Epoch 3207/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1349200214.0597 - val_loss: 4000761618.3989\n",
      "Epoch 3208/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1694187713.0805 - val_loss: 4669664044.1924\n",
      "Epoch 3209/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1520440821.6455 - val_loss: 3401500137.9691\n",
      "Epoch 3210/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1434649201.2335 - val_loss: 4885059620.8518\n",
      "Epoch 3211/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1767308502.8340 - val_loss: 5319303666.8940\n",
      "Epoch 3212/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1568215260.7946 - val_loss: 3367117822.2717\n",
      "Epoch 3213/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1559680376.6528 - val_loss: 3279302824.4433\n",
      "Epoch 3214/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1387085518.5864 - val_loss: 3265157696.7291\n",
      "Epoch 3215/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1333771581.4789 - val_loss: 3460140015.2304\n",
      "Epoch 3216/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1429679430.8070 - val_loss: 4354655117.1060\n",
      "Epoch 3217/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1536453788.4524 - val_loss: 4213446778.9412\n",
      "Epoch 3218/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1513767871.2077 - val_loss: 4916245792.2520\n",
      "Epoch 3219/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1565829431.8965 - val_loss: 3488423842.1738\n",
      "Epoch 3220/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1796411468.0653 - val_loss: 3466005114.6712\n",
      "Epoch 3221/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1390440663.4102 - val_loss: 3482952842.6757\n",
      "Epoch 3222/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1775266819.1694 - val_loss: 3678977089.5662\n",
      "Epoch 3223/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1359340217.0129 - val_loss: 3369522265.6630\n",
      "Epoch 3224/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1423714159.6488 - val_loss: 3574797876.8923\n",
      "Epoch 3225/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1386645135.3427 - val_loss: 3627860962.1862\n",
      "Epoch 3226/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1340415679.7324 - val_loss: 3562061076.9913\n",
      "Epoch 3227/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1299136735.7299 - val_loss: 3627933461.5314\n",
      "Epoch 3228/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1453168408.4007 - val_loss: 4284898539.5488\n",
      "Epoch 3229/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1689621304.6168 - val_loss: 6357631677.3896\n",
      "Epoch 3230/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1443100604.6866 - val_loss: 3898131969.5122\n",
      "Epoch 3231/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1273770908.6123 - val_loss: 3529603904.4141\n",
      "Epoch 3232/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1297852814.1902 - val_loss: 3998626358.2425\n",
      "Epoch 3233/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1423801604.5200 - val_loss: 3366849104.1755\n",
      "Epoch 3234/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1444480114.0979 - val_loss: 6497489298.7184\n",
      "Epoch 3235/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1978871289.9494 - val_loss: 3782702853.1038\n",
      "Epoch 3236/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1641866528.9904 - val_loss: 4252217813.9094\n",
      "Epoch 3237/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1376187410.6021 - val_loss: 3409824836.2397\n",
      "Epoch 3238/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1385684597.9876 - val_loss: 4525881062.7781\n",
      "Epoch 3239/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1582177720.1666 - val_loss: 3453191204.8653\n",
      "Epoch 3240/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1316588189.6050 - val_loss: 3243733026.3944\n",
      "Epoch 3241/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1360368548.9522 - val_loss: 3716648020.6132\n",
      "Epoch 3242/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1653224078.3343 - val_loss: 7781857540.9328\n",
      "Epoch 3243/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 2426857333.1232 - val_loss: 3285729311.8650\n",
      "Epoch 3244/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1497414160.7113 - val_loss: 3519786519.3496\n",
      "Epoch 3245/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1533294426.4716 - val_loss: 3564312745.5865\n",
      "Epoch 3246/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1342763609.0310 - val_loss: 3435495093.2343\n",
      "Epoch 3247/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 75us/step - loss: 1382488591.6308 - val_loss: 3496140555.8368\n",
      "Epoch 3248/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1402309911.6984 - val_loss: 3334749712.8416\n",
      "Epoch 3249/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 1399758463.3517 - val_loss: 3300515456.2520\n",
      "Epoch 3250/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1237066382.8385 - val_loss: 3246037592.9564\n",
      "Epoch 3251/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1293694280.4637 - val_loss: 4212214520.1868\n",
      "Epoch 3252/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1278683563.8897 - val_loss: 3215581029.8239\n",
      "Epoch 3253/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1307625407.6759 - val_loss: 3274728952.9609\n",
      "Epoch 3254/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1298964960.1621 - val_loss: 4053533461.0273\n",
      "Epoch 3255/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1601392994.7575 - val_loss: 3516240062.6295\n",
      "Epoch 3256/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1286464357.2043 - val_loss: 3392979848.0833\n",
      "Epoch 3257/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1271005658.2555 - val_loss: 3819147928.7449\n",
      "Epoch 3258/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1332913490.9826 - val_loss: 3224364634.7162\n",
      "Epoch 3259/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1440250526.1812 - val_loss: 3949216339.2450\n",
      "Epoch 3260/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1531919872.2319 - val_loss: 3355468013.3401\n",
      "Epoch 3261/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1606055718.5189 - val_loss: 3304203883.4318\n",
      "Epoch 3262/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1559398560.3782 - val_loss: 3811385179.1662\n",
      "Epoch 3263/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1363005954.1970 - val_loss: 3186567241.1814\n",
      "Epoch 3264/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1314806516.2409 - val_loss: 3688549412.7077\n",
      "Epoch 3265/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1627120083.4665 - val_loss: 3271403017.5775\n",
      "Epoch 3266/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1370949061.6185 - val_loss: 3354729368.0203\n",
      "Epoch 3267/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1549545103.1986 - val_loss: 3558933986.3674\n",
      "Epoch 3268/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1213777930.4446 - val_loss: 3267572851.5060\n",
      "Epoch 3269/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1294647373.0017 - val_loss: 3588017322.0501\n",
      "Epoch 3270/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 1304345882.9848 - val_loss: 4612765454.3662\n",
      "Epoch 3271/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1676381438.5594 - val_loss: 3198450398.0917\n",
      "Epoch 3272/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1727029996.6956 - val_loss: 4289782513.3007\n",
      "Epoch 3273/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1622983663.5048 - val_loss: 3196664143.5049\n",
      "Epoch 3274/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1330616021.4294 - val_loss: 5187407545.6090\n",
      "Epoch 3275/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1584475373.9921 - val_loss: 4436423087.4194\n",
      "Epoch 3276/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1517794164.9071 - val_loss: 4067600962.3944\n",
      "Epoch 3277/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1354894047.2257 - val_loss: 3354210745.0689\n",
      "Epoch 3278/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1237208097.4226 - val_loss: 3517328666.5001\n",
      "Epoch 3279/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1287363757.5239 - val_loss: 3814536812.3049\n",
      "Epoch 3280/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1424621927.6533 - val_loss: 4343553673.4335\n",
      "Epoch 3281/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1345994675.2504 - val_loss: 3217724562.6149\n",
      "Epoch 3282/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 1278071749.9696 - val_loss: 3219502777.5370\n",
      "Epoch 3283/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1357097683.5926 - val_loss: 3341909706.1176\n",
      "Epoch 3284/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1402022594.8272 - val_loss: 3214157219.6816\n",
      "Epoch 3285/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1485177050.5616 - val_loss: 3371652205.3311\n",
      "Epoch 3286/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1428973154.8272 - val_loss: 3327329212.9575\n",
      "Epoch 3287/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1238676699.7681 - val_loss: 4446611710.9918\n",
      "Epoch 3288/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1515704058.3815 - val_loss: 4948091397.1848\n",
      "Epoch 3289/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1414370775.2302 - val_loss: 4260684337.9758\n",
      "Epoch 3290/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1409396032.3602 - val_loss: 3796250313.0554\n",
      "Epoch 3291/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1619666304.8644 - val_loss: 7111759419.0852\n",
      "Epoch 3292/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1483086376.7698 - val_loss: 4573329943.0436\n",
      "Epoch 3293/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 1484553600.7923 - val_loss: 3685674676.6942\n",
      "Epoch 3294/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1865383965.2808 - val_loss: 3436483012.8878\n",
      "Epoch 3295/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1450040720.3512 - val_loss: 3212318209.4222\n",
      "Epoch 3296/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1301846252.2634 - val_loss: 4068957521.0217\n",
      "Epoch 3297/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1251443990.3298 - val_loss: 3444671203.1325\n",
      "Epoch 3298/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1394647227.8042 - val_loss: 3369495602.2188\n",
      "Epoch 3299/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1338542060.8936 - val_loss: 3912568438.0084\n",
      "Epoch 3300/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1342040722.2600 - val_loss: 4101853204.9598\n",
      "Epoch 3301/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1164506251.4530 - val_loss: 3204635708.8765\n",
      "Epoch 3302/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1214136520.4997 - val_loss: 3690796944.6886\n",
      "Epoch 3303/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1298710117.6365 - val_loss: 5160164917.2276\n",
      "Epoch 3304/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1416760901.1863 - val_loss: 4463115967.6940\n",
      "Epoch 3305/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1245479604.3669 - val_loss: 7517153120.4411\n",
      "Epoch 3306/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1931988352.0720 - val_loss: 3703351334.0489\n",
      "Epoch 3307/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1433547453.7693 - val_loss: 3447258864.1395\n",
      "Epoch 3308/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1352289194.4986 - val_loss: 3437372818.4686\n",
      "Epoch 3309/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1213798620.7046 - val_loss: 3887579648.3691\n",
      "Epoch 3310/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1478232327.4192 - val_loss: 4132567073.0532\n",
      "Epoch 3311/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1381851291.7223 - val_loss: 5736889922.3224\n",
      "Epoch 3312/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1674829633.2606 - val_loss: 3167355643.6253\n",
      "Epoch 3313/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1297388427.0929 - val_loss: 3266790696.2925\n",
      "Epoch 3314/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1345711995.0929 - val_loss: 4869076371.0875\n",
      "Epoch 3315/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1344104485.2043 - val_loss: 3502682663.4397\n",
      "Epoch 3316/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1427865318.0129 - val_loss: 4555995857.8588\n",
      "Epoch 3317/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1349482165.1593 - val_loss: 5062998272.1800\n",
      "Epoch 3318/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1597288223.1176 - val_loss: 3090777614.3640\n",
      "Epoch 3319/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1372876960.9904 - val_loss: 3663236497.4087\n",
      "Epoch 3320/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1288748931.2414 - val_loss: 7007486368.6211\n",
      "Epoch 3321/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1720622523.1379 - val_loss: 3909477714.0838\n",
      "Epoch 3322/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1365200908.4389 - val_loss: 3296012444.3454\n",
      "Epoch 3323/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1363088604.2724 - val_loss: 6852521239.2934\n",
      "Epoch 3324/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1449563920.0630 - val_loss: 3326738259.4340\n",
      "Epoch 3325/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1120906663.9595 - val_loss: 3201673621.7249\n",
      "Epoch 3326/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1073254206.0191 - val_loss: 3015822282.9727\n",
      "Epoch 3327/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1270994964.4569 - val_loss: 4562064380.8135\n",
      "Epoch 3328/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1566375195.4451 - val_loss: 3385470451.8661\n",
      "Epoch 3329/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1125955917.1458 - val_loss: 3231179763.9561\n",
      "Epoch 3330/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1396613057.9088 - val_loss: 3170519010.9795\n",
      "Epoch 3331/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1205881399.0321 - val_loss: 3771928666.1401\n",
      "Epoch 3332/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1617489200.4052 - val_loss: 3270310386.5879\n",
      "Epoch 3333/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1322967691.7411 - val_loss: 3398542091.0987\n",
      "Epoch 3334/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1381338208.3872 - val_loss: 3057930776.5738\n",
      "Epoch 3335/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1572203051.9302 - val_loss: 5010383228.1744\n",
      "Epoch 3336/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1376951918.4963 - val_loss: 3220903384.7719\n",
      "Epoch 3337/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1313643296.7023 - val_loss: 3283436670.7308\n",
      "Epoch 3338/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1190012344.0045 - val_loss: 3213608645.7474\n",
      "Epoch 3339/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1197501603.0613 - val_loss: 3746747939.8616\n",
      "Epoch 3340/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1263435896.8689 - val_loss: 3227161493.2073\n",
      "Epoch 3341/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1207310427.3360 - val_loss: 4213264611.6276\n",
      "Epoch 3342/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1691227722.3208 - val_loss: 3098694901.1623\n",
      "Epoch 3343/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1164807508.6370 - val_loss: 3636758781.1916\n",
      "Epoch 3344/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 1377179205.2583 - val_loss: 3285923502.4383\n",
      "Epoch 3345/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1123678735.0906 - val_loss: 3508132422.0309\n",
      "Epoch 3346/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1325933904.4412 - val_loss: 3202476201.2084\n",
      "Epoch 3347/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1326059812.4840 - val_loss: 5274246752.7111\n",
      "Epoch 3348/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1206860311.6263 - val_loss: 3550091608.1418\n",
      "Epoch 3349/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1481297747.3405 - val_loss: 3301540735.8560\n",
      "Epoch 3350/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1261380956.3444 - val_loss: 3912939802.4416\n",
      "Epoch 3351/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1457029655.0501 - val_loss: 3500829725.7001\n",
      "Epoch 3352/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1282070257.1615 - val_loss: 3949536217.5100\n",
      "Epoch 3353/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1341089067.9932 - val_loss: 3144343399.3271\n",
      "Epoch 3354/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1209541973.5014 - val_loss: 4170969135.3834\n",
      "Epoch 3355/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1386653205.9876 - val_loss: 3183337688.5108\n",
      "Epoch 3356/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1263027762.2060 - val_loss: 3246382403.7266\n",
      "Epoch 3357/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1560865781.3393 - val_loss: 3402666283.4408\n",
      "Epoch 3358/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1365430775.0006 - val_loss: 3103969169.4267\n",
      "Epoch 3359/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1412269309.7310 - val_loss: 3137428305.3367\n",
      "Epoch 3360/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1297544999.5453 - val_loss: 3019935290.4911\n",
      "Epoch 3361/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1316249207.8604 - val_loss: 3262111439.8425\n",
      "Epoch 3362/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1298014349.0557 - val_loss: 3036662556.6245\n",
      "Epoch 3363/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1320704327.0951 - val_loss: 2981071248.7516\n",
      "Epoch 3364/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1248477670.6989 - val_loss: 3016934800.9316\n",
      "Epoch 3365/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1581984971.4170 - val_loss: 3368542035.9381\n",
      "Epoch 3366/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1554485377.5127 - val_loss: 3846383654.2740\n",
      "Epoch 3367/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1136531549.5599 - val_loss: 3588633189.8149\n",
      "Epoch 3368/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1269895213.6320 - val_loss: 3055744461.9544\n",
      "Epoch 3369/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1402087169.1525 - val_loss: 3442693661.9567\n",
      "Epoch 3370/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1116696568.8689 - val_loss: 3416984418.8039\n",
      "Epoch 3371/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1291563192.6888 - val_loss: 4432122333.0025\n",
      "Epoch 3372/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1239578888.6618 - val_loss: 3000601805.1601\n",
      "Epoch 3373/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1136743534.8925 - val_loss: 5395490519.6017\n",
      "Epoch 3374/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1390658645.5734 - val_loss: 3379465478.4270\n",
      "Epoch 3375/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1370893198.7304 - val_loss: 6123265094.8591\n",
      "Epoch 3376/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1541833735.7074 - val_loss: 3319568393.7215\n",
      "Epoch 3377/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 72us/step - loss: 1168293450.4806 - val_loss: 3213044304.9406\n",
      "Epoch 3378/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1084225526.5279 - val_loss: 3249971193.1955\n",
      "Epoch 3379/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1207472361.5982 - val_loss: 4233231848.4073\n",
      "Epoch 3380/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1613461097.8503 - val_loss: 3150038970.1041\n",
      "Epoch 3381/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1148826349.4159 - val_loss: 4364290763.6118\n",
      "Epoch 3382/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1472760323.5836 - val_loss: 3035392390.4270\n",
      "Epoch 3383/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1213356568.2026 - val_loss: 5519792699.4093\n",
      "Epoch 3384/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1429945632.9184 - val_loss: 3456897515.4903\n",
      "Epoch 3385/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1245709161.7017 - val_loss: 3096348233.6720\n",
      "Epoch 3386/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1073027310.2442 - val_loss: 6275358293.9814\n",
      "Epoch 3387/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1568968339.0073 - val_loss: 3425533558.4405\n",
      "Epoch 3388/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1329372455.3292 - val_loss: 3733822712.3128\n",
      "Epoch 3389/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1325233884.8486 - val_loss: 3169961467.5488\n",
      "Epoch 3390/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1110482840.1846 - val_loss: 3083892368.0225\n",
      "Epoch 3391/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1015562594.4671 - val_loss: 2942109661.6056\n",
      "Epoch 3392/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1298566380.1193 - val_loss: 4972496816.4996\n",
      "Epoch 3393/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1417530318.8745 - val_loss: 4713751680.4321\n",
      "Epoch 3394/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1180882313.0039 - val_loss: 3255875668.0011\n",
      "Epoch 3395/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1282836954.4716 - val_loss: 3062583055.3564\n",
      "Epoch 3396/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1149645917.4249 - val_loss: 3277617534.4518\n",
      "Epoch 3397/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1421704940.5515 - val_loss: 5207250309.0408\n",
      "Epoch 3398/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1394660755.4665 - val_loss: 2947232379.2833\n",
      "Epoch 3399/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1135286043.8447 - val_loss: 3734156739.2956\n",
      "Epoch 3400/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1194856897.6612 - val_loss: 3587019396.7887\n",
      "Epoch 3401/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1226308852.9792 - val_loss: 3748805425.3817\n",
      "Epoch 3402/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1284303591.3652 - val_loss: 3214513222.6790\n",
      "Epoch 3403/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1144566899.9358 - val_loss: 3572122949.7429\n",
      "Epoch 3404/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1306769945.9674 - val_loss: 3018743526.7601\n",
      "Epoch 3405/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1197566395.2639 - val_loss: 3242666197.3423\n",
      "Epoch 3406/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1304110223.5678 - val_loss: 2935333288.3263\n",
      "Epoch 3407/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1430600002.7732 - val_loss: 3119163605.6574\n",
      "Epoch 3408/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1111921735.0681 - val_loss: 2913025868.9350\n",
      "Epoch 3409/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1118772074.6652 - val_loss: 5209575536.3376\n",
      "Epoch 3410/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1457927309.1998 - val_loss: 3212662915.6366\n",
      "Epoch 3411/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1056102190.1182 - val_loss: 3045230225.4087\n",
      "Epoch 3412/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1131155318.9240 - val_loss: 4942972293.0048\n",
      "Epoch 3413/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1331686534.1227 - val_loss: 3644084823.4532\n",
      "Epoch 3414/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1357999130.8317 - val_loss: 2959085759.7390\n",
      "Epoch 3415/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 1198936770.9173 - val_loss: 3069563585.5302\n",
      "Epoch 3416/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1231125351.6083 - val_loss: 3033791872.9226\n",
      "Epoch 3417/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1419503358.4873 - val_loss: 2965844924.0304\n",
      "Epoch 3418/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1172103136.3782 - val_loss: 3304422196.3162\n",
      "Epoch 3419/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1144893108.2769 - val_loss: 5118640311.5567\n",
      "Epoch 3420/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1788706899.4845 - val_loss: 4329914048.7021\n",
      "Epoch 3421/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1152732347.1019 - val_loss: 3495748976.2475\n",
      "Epoch 3422/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1186841701.5194 - val_loss: 3390118738.8309\n",
      "Epoch 3423/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1074805154.3185 - val_loss: 3056943643.6703\n",
      "Epoch 3424/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1219370224.3151 - val_loss: 4228551791.6174\n",
      "Epoch 3425/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1346037186.4851 - val_loss: 5296964329.4042\n",
      "Epoch 3426/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1350967512.3804 - val_loss: 3066569451.0987\n",
      "Epoch 3427/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1179595792.1981 - val_loss: 3156669840.0315\n",
      "Epoch 3428/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1060039198.0371 - val_loss: 3157539320.3308\n",
      "Epoch 3429/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1183873651.4665 - val_loss: 3444321713.8599\n",
      "Epoch 3430/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1434748764.0383 - val_loss: 3921299267.6636\n",
      "Epoch 3431/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 1165564891.6961 - val_loss: 3097015287.1966\n",
      "Epoch 3432/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1147235430.2577 - val_loss: 5487946708.3612\n",
      "Epoch 3433/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1369111963.1739 - val_loss: 4030388782.0332\n",
      "Epoch 3434/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1309471481.2651 - val_loss: 2947450633.0374\n",
      "Epoch 3435/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1235822995.1784 - val_loss: 4536812971.2225\n",
      "Epoch 3436/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1514773382.7169 - val_loss: 2918428254.9468\n",
      "Epoch 3437/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1041458439.6353 - val_loss: 3259589770.7297\n",
      "Epoch 3438/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1376628599.1581 - val_loss: 3661762638.5080\n",
      "Epoch 3439/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1219196950.7980 - val_loss: 2970927218.7904\n",
      "Epoch 3440/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1339033669.9426 - val_loss: 2933007093.7519\n",
      "Epoch 3441/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1021239344.4772 - val_loss: 3341862098.6284\n",
      "Epoch 3442/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1353226100.5470 - val_loss: 2914240831.2979\n",
      "Epoch 3443/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1162396993.0445 - val_loss: 2956030908.8630\n",
      "Epoch 3444/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1087296381.6860 - val_loss: 3539918217.5595\n",
      "Epoch 3445/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1238499550.7935 - val_loss: 2894974796.1429\n",
      "Epoch 3446/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1243998794.9128 - val_loss: 7211878098.4169\n",
      "Epoch 3447/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1770134592.2521 - val_loss: 3627952252.7775\n",
      "Epoch 3448/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1761822763.7231 - val_loss: 3356332682.7927\n",
      "Epoch 3449/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1138467226.7957 - val_loss: 3035295620.4107\n",
      "Epoch 3450/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1153825692.2183 - val_loss: 3279227828.7572\n",
      "Epoch 3451/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1169196884.8531 - val_loss: 3493503701.7474\n",
      "Epoch 3452/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 1378869016.2026 - val_loss: 3581057531.5713\n",
      "Epoch 3453/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1208672413.6770 - val_loss: 3732478024.6594\n",
      "Epoch 3454/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1176051954.2150 - val_loss: 3296190054.5980\n",
      "Epoch 3455/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1171841721.1750 - val_loss: 2877765155.3305\n",
      "Epoch 3456/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1248156592.1531 - val_loss: 3710459258.4821\n",
      "Epoch 3457/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1455464583.7074 - val_loss: 3225138261.7654\n",
      "Epoch 3458/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1206613637.9786 - val_loss: 3892926829.2771\n",
      "Epoch 3459/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1222570331.6961 - val_loss: 5395769288.6413\n",
      "Epoch 3460/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1253738182.5189 - val_loss: 3014912001.8813\n",
      "Epoch 3461/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 1310055004.9747 - val_loss: 2901704934.0985\n",
      "Epoch 3462/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1075594856.7901 - val_loss: 3217623196.0844\n",
      "Epoch 3463/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1076031554.1970 - val_loss: 3626815023.4734\n",
      "Epoch 3464/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1179670662.7349 - val_loss: 5527768007.1111\n",
      "Epoch 3465/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1143990714.9623 - val_loss: 2969962335.4419\n",
      "Epoch 3466/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1154923026.5481 - val_loss: 2989731639.3767\n",
      "Epoch 3467/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1041462990.4423 - val_loss: 4161148033.1882\n",
      "Epoch 3468/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1547504883.1784 - val_loss: 3056468623.5904\n",
      "Epoch 3469/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1055847423.8559 - val_loss: 4616572902.7961\n",
      "Epoch 3470/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1435036114.1880 - val_loss: 4244828036.2487\n",
      "Epoch 3471/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1258838698.9488 - val_loss: 3002540039.5882\n",
      "Epoch 3472/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 1087813193.2560 - val_loss: 5254182129.4537\n",
      "Epoch 3473/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1165602938.3815 - val_loss: 2868670163.0830\n",
      "Epoch 3474/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 1101167945.2560 - val_loss: 4110534969.1589\n",
      "Epoch 3475/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1384684307.3765 - val_loss: 5696655648.6211\n",
      "Epoch 3476/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1129488835.7456 - val_loss: 3000312064.8641\n",
      "Epoch 3477/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1206224502.0236 - val_loss: 3184034179.0470\n",
      "Epoch 3478/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 984678685.6770 - val_loss: 3058624454.0309\n",
      "Epoch 3479/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1121729766.7169 - val_loss: 6201820938.6937\n",
      "Epoch 3480/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1297871974.7530 - val_loss: 4189722216.4523\n",
      "Epoch 3481/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1138237809.2696 - val_loss: 3442528349.0070\n",
      "Epoch 3482/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1268114316.7991 - val_loss: 2903227431.6512\n",
      "Epoch 3483/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1116772090.8137 - val_loss: 5611789393.5887\n",
      "Epoch 3484/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1326803594.9848 - val_loss: 3235330500.2127\n",
      "Epoch 3485/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1047408191.1356 - val_loss: 2874750902.4765\n",
      "Epoch 3486/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1346407888.6033 - val_loss: 3871449231.0504\n",
      "Epoch 3487/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1259546521.8773 - val_loss: 3129006166.7015\n",
      "Epoch 3488/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1287943739.4980 - val_loss: 3225494860.8540\n",
      "Epoch 3489/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1089235896.0765 - val_loss: 3989498275.9606\n",
      "Epoch 3490/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 1223747827.4215 - val_loss: 3964768867.9381\n",
      "Epoch 3491/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1211347022.0822 - val_loss: 3487660031.7547\n",
      "Epoch 3492/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 989133468.4524 - val_loss: 3646631155.4340\n",
      "Epoch 3493/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1364844990.0191 - val_loss: 3316276649.9826\n",
      "Epoch 3494/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1486611800.2746 - val_loss: 4463915957.5044\n",
      "Epoch 3495/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1127388111.3067 - val_loss: 2912250834.3269\n",
      "Epoch 3496/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1112559895.3562 - val_loss: 3601430604.4579\n",
      "Epoch 3497/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 1240499137.6207 - val_loss: 3489213320.1958\n",
      "Epoch 3498/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 1016572323.5476 - val_loss: 3183913751.2686\n",
      "Epoch 3499/7000\n",
      "3554/3554 [==============================] - 0s 117us/step - loss: 1165959773.6050 - val_loss: 3333039284.3162\n",
      "Epoch 3500/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 1393014377.3101 - val_loss: 3495669698.3989\n",
      "Epoch 3501/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1237635492.0743 - val_loss: 5011379112.2183\n",
      "Epoch 3502/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1117645429.1007 - val_loss: 2802855158.2830\n",
      "Epoch 3503/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 1041412152.9589 - val_loss: 4222580246.0354\n",
      "Epoch 3504/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1125427652.6460 - val_loss: 3059842518.3730\n",
      "Epoch 3505/7000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 1010123222.98 - 0s 129us/step - loss: 1011926444.7136 - val_loss: 2828031848.8529\n",
      "Epoch 3506/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 973410714.2915 - val_loss: 2974678441.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3507/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 1009920291.1514 - val_loss: 3148125341.3806\n",
      "Epoch 3508/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 1352340474.9578 - val_loss: 3980959733.4143\n",
      "Epoch 3509/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 1803167418.1294 - val_loss: 3812259971.2765\n",
      "Epoch 3510/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1118840220.5515 - val_loss: 3187152671.2349\n",
      "Epoch 3511/7000\n",
      "3554/3554 [==============================] - 1s 141us/step - loss: 1326699245.2718 - val_loss: 3317293051.3013\n",
      "Epoch 3512/7000\n",
      "3554/3554 [==============================] - 0s 112us/step - loss: 1188990609.8413 - val_loss: 3509895026.8759\n",
      "Epoch 3513/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 1158960124.4299 - val_loss: 2757587205.8599\n",
      "Epoch 3514/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1115436081.0355 - val_loss: 4721253911.9797\n",
      "Epoch 3515/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 1425036627.4125 - val_loss: 3202659709.9477\n",
      "Epoch 3516/7000\n",
      "3554/3554 [==============================] - 0s 128us/step - loss: 1139153648.7428 - val_loss: 3665026650.0861\n",
      "Epoch 3517/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 1083380208.0630 - val_loss: 3199387118.0242\n",
      "Epoch 3518/7000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 1085159097.96 - 0s 100us/step - loss: 1079103793.3416 - val_loss: 2792553550.0287\n",
      "Epoch 3519/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 995025596.5065 - val_loss: 5342092232.7314\n",
      "Epoch 3520/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1424046838.4918 - val_loss: 2761624224.6346\n",
      "Epoch 3521/7000\n",
      "3554/3554 [==============================] - 1s 142us/step - loss: 1144540533.1953 - val_loss: 3234707286.5575\n",
      "Epoch 3522/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 1075925171.9347 - val_loss: 4083575361.5302\n",
      "Epoch 3523/7000\n",
      "3554/3554 [==============================] - 0s 119us/step - loss: 1305544479.6939 - val_loss: 3059364414.8478\n",
      "Epoch 3524/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1022593753.6342 - val_loss: 2882904011.2563\n",
      "Epoch 3525/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 1000033135.5768 - val_loss: 3170779324.3814\n",
      "Epoch 3526/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 977235327.6759 - val_loss: 3531213459.5691\n",
      "Epoch 3527/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1063734243.6376 - val_loss: 3164152312.9294\n",
      "Epoch 3528/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1126718203.7501 - val_loss: 2883266800.3578\n",
      "Epoch 3529/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1080356002.6472 - val_loss: 2946303727.8515\n",
      "Epoch 3530/7000\n",
      "3554/3554 [==============================] - 0s 106us/step - loss: 1170293139.8087 - val_loss: 2834391579.6456\n",
      "Epoch 3531/7000\n",
      "3554/3554 [==============================] - 0s 120us/step - loss: 984266594.3950 - val_loss: 2936096453.9859\n",
      "Epoch 3532/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1159354151.4102 - val_loss: 3099592245.9409\n",
      "Epoch 3533/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 1379715946.1024 - val_loss: 2967445645.5921\n",
      "Epoch 3534/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1167840738.3950 - val_loss: 2980427322.0160\n",
      "Epoch 3535/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1350781683.6871 - val_loss: 3723841184.2790\n",
      "Epoch 3536/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1022809014.1317 - val_loss: 3119429592.2970\n",
      "Epoch 3537/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 987200107.0388 - val_loss: 3215967320.8439\n",
      "Epoch 3538/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 989112957.7850 - val_loss: 4134107661.0970\n",
      "Epoch 3539/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1178643549.3528 - val_loss: 8299006982.2537\n",
      "Epoch 3540/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 2092755018.3905 - val_loss: 2754303642.0692\n",
      "Epoch 3541/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1325881078.6359 - val_loss: 4640993235.7581\n",
      "Epoch 3542/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1174557861.8886 - val_loss: 3836286660.5457\n",
      "Epoch 3543/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 963753301.2853 - val_loss: 2798594991.6354\n",
      "Epoch 3544/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1021683748.1958 - val_loss: 3332814718.2177\n",
      "Epoch 3545/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1316679402.5346 - val_loss: 3462584379.5623\n",
      "Epoch 3546/7000\n",
      "3554/3554 [==============================] - 0s 108us/step - loss: 1160819358.9375 - val_loss: 3475658684.7595\n",
      "Epoch 3547/7000\n",
      "3554/3554 [==============================] - 0s 111us/step - loss: 1237086254.1902 - val_loss: 2919820000.2745\n",
      "Epoch 3548/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 1148440397.9381 - val_loss: 2982616271.3384\n",
      "Epoch 3549/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1009818609.3056 - val_loss: 4397982690.6509\n",
      "Epoch 3550/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1165405741.2358 - val_loss: 2787237154.4034\n",
      "Epoch 3551/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 927695613.9831 - val_loss: 2861818314.2436\n",
      "Epoch 3552/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1461992477.0287 - val_loss: 2805378955.1977\n",
      "Epoch 3553/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 969820087.7164 - val_loss: 3052628140.2149\n",
      "Epoch 3554/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1002599634.3860 - val_loss: 3091757238.8726\n",
      "Epoch 3555/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1090883902.3433 - val_loss: 2746375908.6211\n",
      "Epoch 3556/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1017337833.9223 - val_loss: 3129106766.8883\n",
      "Epoch 3557/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1112437116.9026 - val_loss: 2810030556.2194\n",
      "Epoch 3558/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 975571436.1193 - val_loss: 3088333564.8450\n",
      "Epoch 3559/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1101974469.7985 - val_loss: 3315566020.4467\n",
      "Epoch 3560/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1146049555.7727 - val_loss: 2846110093.7451\n",
      "Epoch 3561/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1061699683.2594 - val_loss: 4199703561.9376\n",
      "Epoch 3562/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1128241637.2043 - val_loss: 2838958451.3778\n",
      "Epoch 3563/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1017615596.3354 - val_loss: 4174247097.4650\n",
      "Epoch 3564/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1182694163.3044 - val_loss: 2910508797.1691\n",
      "Epoch 3565/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1030138751.2077 - val_loss: 3197183393.2557\n",
      "Epoch 3566/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1107064806.7349 - val_loss: 3011762400.7831\n",
      "Epoch 3567/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 949866991.2887 - val_loss: 3424498988.3589\n",
      "Epoch 3568/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1665416475.6601 - val_loss: 3360371984.3691\n",
      "Epoch 3569/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1049840742.6449 - val_loss: 3098157218.1693\n",
      "Epoch 3570/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1086916584.3016 - val_loss: 3223104848.4006\n",
      "Epoch 3571/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1260350070.4108 - val_loss: 3020924226.5924\n",
      "Epoch 3572/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 938060361.7603 - val_loss: 2964722762.2436\n",
      "Epoch 3573/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 943290077.0647 - val_loss: 2715143048.8259\n",
      "Epoch 3574/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1269414265.0129 - val_loss: 4001460792.6729\n",
      "Epoch 3575/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 2056787040.9949 - val_loss: 3951647635.4070\n",
      "Epoch 3576/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1002037741.3078 - val_loss: 3027299601.5072\n",
      "Epoch 3577/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 934297346.5211 - val_loss: 3671291620.9789\n",
      "Epoch 3578/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1036253840.9868 - val_loss: 2902447575.9257\n",
      "Epoch 3579/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 982023272.5537 - val_loss: 2928593634.3674\n",
      "Epoch 3580/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1053240720.3512 - val_loss: 3645838643.6321\n",
      "Epoch 3581/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1439854201.7333 - val_loss: 5542296978.3989\n",
      "Epoch 3582/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1243212492.2814 - val_loss: 3249938904.6954\n",
      "Epoch 3583/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1143984026.0934 - val_loss: 3489836795.3193\n",
      "Epoch 3584/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 980615832.8149 - val_loss: 3050337131.7018\n",
      "Epoch 3585/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1279349612.7091 - val_loss: 2914947743.1539\n",
      "Epoch 3586/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1031363932.2724 - val_loss: 3587599478.6745\n",
      "Epoch 3587/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1069771573.6635 - val_loss: 2896766024.9474\n",
      "Epoch 3588/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1077166257.3573 - val_loss: 3136670470.2582\n",
      "Epoch 3589/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1090798713.8773 - val_loss: 3193162122.6577\n",
      "Epoch 3590/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1481094711.7164 - val_loss: 2889473945.9421\n",
      "Epoch 3591/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 984687873.8728 - val_loss: 2959283600.9719\n",
      "Epoch 3592/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 1064263333.9876 - val_loss: 3761112335.9865\n",
      "Epoch 3593/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1169643748.0158 - val_loss: 2737614987.9359\n",
      "Epoch 3594/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 914996579.5476 - val_loss: 2733468974.3797\n",
      "Epoch 3595/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 963493162.8588 - val_loss: 3044528091.6343\n",
      "Epoch 3596/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1019178064.0990 - val_loss: 2699532894.5980\n",
      "Epoch 3597/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1071915963.4620 - val_loss: 4095292810.7747\n",
      "Epoch 3598/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1314406921.3101 - val_loss: 3694379987.4273\n",
      "Epoch 3599/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1059034684.4705 - val_loss: 2710706696.7752\n",
      "Epoch 3600/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 937655571.5805 - val_loss: 2858671244.4354\n",
      "Epoch 3601/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 904832466.7552 - val_loss: 3457559270.3460\n",
      "Epoch 3602/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1008734140.7226 - val_loss: 3647981332.6312\n",
      "Epoch 3603/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1384202717.1728 - val_loss: 3092294930.2549\n",
      "Epoch 3604/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1182081114.5076 - val_loss: 3251517103.7075\n",
      "Epoch 3605/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 967593805.7940 - val_loss: 3273946176.1620\n",
      "Epoch 3606/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1324010046.3253 - val_loss: 2688427308.5975\n",
      "Epoch 3607/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 963048702.1992 - val_loss: 3268219045.8059\n",
      "Epoch 3608/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1005716342.3860 - val_loss: 3546230328.2408\n",
      "Epoch 3609/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1031724426.7327 - val_loss: 3125203450.4911\n",
      "Epoch 3610/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1226732528.2611 - val_loss: 2922235996.7235\n",
      "Epoch 3611/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 972660881.4316 - val_loss: 2766945320.9114\n",
      "Epoch 3612/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1006621850.2195 - val_loss: 2899927812.8765\n",
      "Epoch 3613/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1137582060.3804 - val_loss: 5808950695.4622\n",
      "Epoch 3614/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1171015535.9910 - val_loss: 2879479438.7623\n",
      "Epoch 3615/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1355578729.5982 - val_loss: 2999393163.0177\n",
      "Epoch 3616/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1002202189.8661 - val_loss: 3325365438.1817\n",
      "Epoch 3617/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1223735031.2842 - val_loss: 6078718980.5637\n",
      "Epoch 3618/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1122681208.7968 - val_loss: 2777369549.0790\n",
      "Epoch 3619/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 947475098.3635 - val_loss: 2761888277.7474\n",
      "Epoch 3620/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 902644748.0293 - val_loss: 2819948478.9468\n",
      "Epoch 3621/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 946328967.9235 - val_loss: 2730472798.3257\n",
      "Epoch 3622/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 988592954.8250 - val_loss: 3003769067.8233\n",
      "Epoch 3623/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1057351478.9600 - val_loss: 3985359000.0518\n",
      "Epoch 3624/7000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 1187247969.1165 - val_loss: 2804485438.9266\n",
      "Epoch 3625/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 851931541.3934 - val_loss: 3298989807.7862\n",
      "Epoch 3626/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1158680529.4136 - val_loss: 3166852513.5572\n",
      "Epoch 3627/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1025254827.8132 - val_loss: 3139706158.5733\n",
      "Epoch 3628/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 930115332.1778 - val_loss: 5382251585.5662\n",
      "Epoch 3629/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1348762322.0439 - val_loss: 4076189099.9629\n",
      "Epoch 3630/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1247535596.1913 - val_loss: 2994239705.3030\n",
      "Epoch 3631/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1009525229.7760 - val_loss: 3184835970.1873\n",
      "Epoch 3632/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1196438224.0990 - val_loss: 3621149297.8948\n",
      "Epoch 3633/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 1300745815.7164 - val_loss: 2742333372.7955\n",
      "Epoch 3634/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1017388874.3365 - val_loss: 2928646744.6999\n",
      "Epoch 3635/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 989553398.5639 - val_loss: 2775490172.8315\n",
      "Epoch 3636/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 75us/step - loss: 1012481113.7513 - val_loss: 2970320238.0062\n",
      "Epoch 3637/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 894680634.4536 - val_loss: 2723229456.0765\n",
      "Epoch 3638/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1060667756.6235 - val_loss: 2950170910.4506\n",
      "Epoch 3639/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1110620928.1936 - val_loss: 2801956639.0008\n",
      "Epoch 3640/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 963537003.0748 - val_loss: 3367245535.7075\n",
      "Epoch 3641/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1359429785.5172 - val_loss: 2709019670.9738\n",
      "Epoch 3642/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 949888900.5200 - val_loss: 2960878249.3705\n",
      "Epoch 3643/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1159738573.8661 - val_loss: 2776808341.1173\n",
      "Epoch 3644/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 901083194.3095 - val_loss: 2767893271.2754\n",
      "Epoch 3645/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1118198271.9280 - val_loss: 2706024922.1401\n",
      "Epoch 3646/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 934166989.8481 - val_loss: 2650881014.8726\n",
      "Epoch 3647/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1006371082.8047 - val_loss: 2974468370.8669\n",
      "Epoch 3648/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1499472938.3185 - val_loss: 3192179044.9069\n",
      "Epoch 3649/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1058293653.2583 - val_loss: 2815433328.4186\n",
      "Epoch 3650/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1064950909.1908 - val_loss: 2839184282.4641\n",
      "Epoch 3651/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1053035526.4288 - val_loss: 3959052531.6861\n",
      "Epoch 3652/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1075964694.1857 - val_loss: 3140125660.4489\n",
      "Epoch 3653/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 932129780.3669 - val_loss: 2689580695.1426\n",
      "Epoch 3654/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 940077376.1801 - val_loss: 3079558447.3114\n",
      "Epoch 3655/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1012949743.0726 - val_loss: 3211727120.7606\n",
      "Epoch 3656/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1066115230.9015 - val_loss: 3752482591.9010\n",
      "Epoch 3657/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 959248551.2572 - val_loss: 2719123585.5392\n",
      "Epoch 3658/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 906708934.0597 - val_loss: 3087021756.2014\n",
      "Epoch 3659/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1213901619.1604 - val_loss: 2751645750.8906\n",
      "Epoch 3660/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 966676669.1187 - val_loss: 3495384937.4515\n",
      "Epoch 3661/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1073292074.6427 - val_loss: 2873911782.7871\n",
      "Epoch 3662/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1344958906.9218 - val_loss: 2863839650.8940\n",
      "Epoch 3663/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1015486888.5537 - val_loss: 2770679356.2914\n",
      "Epoch 3664/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1300142784.9004 - val_loss: 2759399754.0456\n",
      "Epoch 3665/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1041033623.5183 - val_loss: 3167408464.6706\n",
      "Epoch 3666/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1015926979.4935 - val_loss: 2945653030.5800\n",
      "Epoch 3667/7000\n",
      "3554/3554 [==============================] - 0s 102us/step - loss: 1155605156.8441 - val_loss: 2825438337.5392\n",
      "Epoch 3668/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1007987386.1294 - val_loss: 4233841422.8343\n",
      "Epoch 3669/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1115008615.4373 - val_loss: 2916712127.3879\n",
      "Epoch 3670/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1117156471.0163 - val_loss: 3164982851.3665\n",
      "Epoch 3671/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 919756404.1148 - val_loss: 3758203520.0360\n",
      "Epoch 3672/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1097192298.6066 - val_loss: 3453948150.6385\n",
      "Epoch 3673/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1224797953.6792 - val_loss: 3858285096.5513\n",
      "Epoch 3674/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1130905511.1851 - val_loss: 4074743686.7331\n",
      "Epoch 3675/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1167754192.0540 - val_loss: 3097989946.6532\n",
      "Epoch 3676/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 933526347.0388 - val_loss: 2817089917.3165\n",
      "Epoch 3677/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 880807613.6950 - val_loss: 2748746409.7215\n",
      "Epoch 3678/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 913249286.7530 - val_loss: 2959888072.1913\n",
      "Epoch 3679/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 994346337.0264 - val_loss: 4545373995.3508\n",
      "Epoch 3680/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1177466388.4569 - val_loss: 2839598849.5842\n",
      "Epoch 3681/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1034571525.4384 - val_loss: 2738861502.1097\n",
      "Epoch 3682/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1050524605.3708 - val_loss: 3133811467.9539\n",
      "Epoch 3683/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 924106635.0568 - val_loss: 3354017840.0068\n",
      "Epoch 3684/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 969244104.7608 - val_loss: 3326670866.9120\n",
      "Epoch 3685/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1105766591.2527 - val_loss: 3097678831.3868\n",
      "Epoch 3686/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1117370530.8633 - val_loss: 2790923285.0588\n",
      "Epoch 3687/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 946214710.8880 - val_loss: 4007082757.3468\n",
      "Epoch 3688/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1087151875.3089 - val_loss: 2741793168.4771\n",
      "Epoch 3689/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 859704631.6083 - val_loss: 3337171244.2959\n",
      "Epoch 3690/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 983196518.8250 - val_loss: 4756420437.0093\n",
      "Epoch 3691/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1151815703.9865 - val_loss: 5909197978.5316\n",
      "Epoch 3692/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1023107186.3860 - val_loss: 4048186582.9986\n",
      "Epoch 3693/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1432127981.7580 - val_loss: 2630722482.6599\n",
      "Epoch 3694/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 791991757.8481 - val_loss: 2810939255.5387\n",
      "Epoch 3695/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 896407342.3523 - val_loss: 2919939359.1089\n",
      "Epoch 3696/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 967596781.7040 - val_loss: 2905489014.6835\n",
      "Epoch 3697/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 859805038.8205 - val_loss: 3328691092.9643\n",
      "Epoch 3698/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 965627804.4524 - val_loss: 3836530898.5789\n",
      "Epoch 3699/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1146464234.6427 - val_loss: 2630517990.1390\n",
      "Epoch 3700/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1057082847.2977 - val_loss: 2947610749.8757\n",
      "Epoch 3701/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1009302045.2364 - val_loss: 2684543780.0776\n",
      "Epoch 3702/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 887152476.8486 - val_loss: 3024729352.9474\n",
      "Epoch 3703/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1046639568.8846 - val_loss: 3583182925.3834\n",
      "Epoch 3704/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 905286475.9212 - val_loss: 3912380580.0878\n",
      "Epoch 3705/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1154515525.0118 - val_loss: 2672789509.4954\n",
      "Epoch 3706/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1079773101.6635 - val_loss: 3226756603.2473\n",
      "Epoch 3707/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 889738054.0146 - val_loss: 5670251566.0872\n",
      "Epoch 3708/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1828576884.4209 - val_loss: 2570440939.3148\n",
      "Epoch 3709/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 928790173.7490 - val_loss: 4006855167.4959\n",
      "Epoch 3710/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 879449544.3917 - val_loss: 3013525023.7159\n",
      "Epoch 3711/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 878791713.4226 - val_loss: 2828119327.6624\n",
      "Epoch 3712/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 928569680.6753 - val_loss: 2740601798.9041\n",
      "Epoch 3713/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 984985060.4840 - val_loss: 2803063538.8489\n",
      "Epoch 3714/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 891030578.6922 - val_loss: 2716226979.0515\n",
      "Epoch 3715/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 830053122.0506 - val_loss: 3173980522.0906\n",
      "Epoch 3716/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 976192508.0383 - val_loss: 2652103808.4771\n",
      "Epoch 3717/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 917140414.5954 - val_loss: 2907177526.8726\n",
      "Epoch 3718/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1282878892.7721 - val_loss: 2652340642.3674\n",
      "Epoch 3719/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 977042759.4204 - val_loss: 2685762050.8174\n",
      "Epoch 3720/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 831635674.4356 - val_loss: 2961752103.7412\n",
      "Epoch 3721/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 1054888879.8739 - val_loss: 2720954212.6852\n",
      "Epoch 3722/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 954330030.9645 - val_loss: 3041740313.1679\n",
      "Epoch 3723/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1019417989.6545 - val_loss: 3126003314.7859\n",
      "Epoch 3724/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 822035156.1598 - val_loss: 2915796965.8037\n",
      "Epoch 3725/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 902146621.4429 - val_loss: 2868070809.6900\n",
      "Epoch 3726/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1116904353.8548 - val_loss: 4185325830.0489\n",
      "Epoch 3727/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1167490940.9026 - val_loss: 2646697181.7812\n",
      "Epoch 3728/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 851291859.4485 - val_loss: 2628764023.3226\n",
      "Epoch 3729/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1366973025.5667 - val_loss: 2771283493.0858\n",
      "Epoch 3730/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1076104416.4502 - val_loss: 2682251810.0793\n",
      "Epoch 3731/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1078423265.8908 - val_loss: 3351748116.4512\n",
      "Epoch 3732/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1075857281.3146 - val_loss: 5123255126.4135\n",
      "Epoch 3733/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 926959326.5774 - val_loss: 3700200030.6948\n",
      "Epoch 3734/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1069736750.6404 - val_loss: 2641075579.3260\n",
      "Epoch 3735/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 864174572.2634 - val_loss: 3442387378.9142\n",
      "Epoch 3736/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1083150392.1846 - val_loss: 3514608302.5013\n",
      "Epoch 3737/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1214829930.1024 - val_loss: 3029095226.8332\n",
      "Epoch 3738/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1060509043.6106 - val_loss: 2796732435.4205\n",
      "Epoch 3739/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 880796948.2409 - val_loss: 2634803269.8914\n",
      "Epoch 3740/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 848053250.3770 - val_loss: 4811021682.4979\n",
      "Epoch 3741/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1211468660.7271 - val_loss: 3825667472.6796\n",
      "Epoch 3742/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 907562415.6128 - val_loss: 2938347498.1378\n",
      "Epoch 3743/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 875979585.0805 - val_loss: 3232536256.9902\n",
      "Epoch 3744/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 801720257.2606 - val_loss: 4771476470.7286\n",
      "Epoch 3745/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1046501450.2285 - val_loss: 2805660926.1547\n",
      "Epoch 3746/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 944118505.6702 - val_loss: 2768419354.9592\n",
      "Epoch 3747/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1021912542.1452 - val_loss: 5159135549.3626\n",
      "Epoch 3748/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1108259957.6455 - val_loss: 2810884732.7685\n",
      "Epoch 3749/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 920943848.9859 - val_loss: 3467010694.8771\n",
      "Epoch 3750/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 988322015.4418 - val_loss: 2618397487.7120\n",
      "Epoch 3751/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 873001857.7828 - val_loss: 2988650926.4326\n",
      "Epoch 3752/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 910194764.8396 - val_loss: 3384758493.9589\n",
      "Epoch 3753/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 940486184.5537 - val_loss: 3103653109.8509\n",
      "Epoch 3754/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1012766498.8272 - val_loss: 2941967188.0371\n",
      "Epoch 3755/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 858449139.2212 - val_loss: 3131107533.9522\n",
      "Epoch 3756/7000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 896279859.0703 - val_loss: 2851450505.5415\n",
      "Epoch 3757/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1045886813.9291 - val_loss: 2918350508.5997\n",
      "Epoch 3758/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 919932074.4266 - val_loss: 2891457726.6228\n",
      "Epoch 3759/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 972603722.7237 - val_loss: 2965477740.6110\n",
      "Epoch 3760/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 912412978.4221 - val_loss: 5770810632.0653\n",
      "Epoch 3761/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1439733352.5898 - val_loss: 3486496149.3153\n",
      "Epoch 3762/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1212389753.3731 - val_loss: 3115640041.8565\n",
      "Epoch 3763/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 843883795.7727 - val_loss: 2688579215.0414\n",
      "Epoch 3764/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 948193261.6320 - val_loss: 2962891708.8045\n",
      "Epoch 3765/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 976537367.0771 - val_loss: 3223441396.5097\n",
      "Epoch 3766/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 86us/step - loss: 847503705.6792 - val_loss: 2778200621.2231\n",
      "Epoch 3767/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 955308495.1806 - val_loss: 2492954783.4712\n",
      "Epoch 3768/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 784890387.1514 - val_loss: 2584748433.3232\n",
      "Epoch 3769/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 856124311.9550 - val_loss: 2524259577.7935\n",
      "Epoch 3770/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 928331428.3759 - val_loss: 3282629632.9924\n",
      "Epoch 3771/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1077011531.6331 - val_loss: 3318180712.0563\n",
      "Epoch 3772/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 973616250.5931 - val_loss: 2747110575.5184\n",
      "Epoch 3773/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 922560529.8638 - val_loss: 3909351791.2079\n",
      "Epoch 3774/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1324855107.3450 - val_loss: 6326115826.6059\n",
      "Epoch 3775/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1101066972.9567 - val_loss: 2602465844.0326\n",
      "Epoch 3776/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 870446727.5993 - val_loss: 2740318811.3463\n",
      "Epoch 3777/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 829695770.1474 - val_loss: 3096414294.6419\n",
      "Epoch 3778/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1204706469.3844 - val_loss: 6877873925.2928\n",
      "Epoch 3779/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 1112088997.3303 - val_loss: 2970565748.1181\n",
      "Epoch 3780/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 956563799.8064 - val_loss: 3958555071.9820\n",
      "Epoch 3781/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 932762889.7963 - val_loss: 2705097081.6990\n",
      "Epoch 3782/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 953898683.8526 - val_loss: 2612673065.9736\n",
      "Epoch 3783/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 904048740.3219 - val_loss: 2661767957.9544\n",
      "Epoch 3784/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 826137806.0101 - val_loss: 2687691150.4833\n",
      "Epoch 3785/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 948807861.7175 - val_loss: 2681771650.5204\n",
      "Epoch 3786/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 869337220.5380 - val_loss: 6518547003.1752\n",
      "Epoch 3787/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1961115622.1047 - val_loss: 2682162850.2549\n",
      "Epoch 3788/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1260766321.7997 - val_loss: 2980773385.4965\n",
      "Epoch 3789/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 753943164.4524 - val_loss: 2553323839.1662\n",
      "Epoch 3790/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 762175313.1075 - val_loss: 2651735518.5598\n",
      "Epoch 3791/7000\n",
      "3554/3554 [==============================] - 0s 68us/step - loss: 817789640.1035 - val_loss: 2956368712.1406\n",
      "Epoch 3792/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 877664604.8486 - val_loss: 3074886121.3806\n",
      "Epoch 3793/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 882479889.6477 - val_loss: 4174815368.8214\n",
      "Epoch 3794/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 821057770.6787 - val_loss: 5568639939.6546\n",
      "Epoch 3795/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1140619169.9268 - val_loss: 2938610940.5435\n",
      "Epoch 3796/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 894634778.3365 - val_loss: 2674593491.4880\n",
      "Epoch 3797/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 794580928.6393 - val_loss: 4675120184.7359\n",
      "Epoch 3798/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 868142533.5104 - val_loss: 3027041509.2388\n",
      "Epoch 3799/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1115722468.3759 - val_loss: 2896108324.9958\n",
      "Epoch 3800/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1309172208.7293 - val_loss: 3421327186.5969\n",
      "Epoch 3801/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1062786685.9111 - val_loss: 6232478827.2968\n",
      "Epoch 3802/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1085595670.5098 - val_loss: 2507920559.7153\n",
      "Epoch 3803/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 756830537.6522 - val_loss: 2589211849.1038\n",
      "Epoch 3804/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 806377647.9370 - val_loss: 3174917974.9176\n",
      "Epoch 3805/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 960812636.9927 - val_loss: 2787317674.4506\n",
      "Epoch 3806/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 873152250.5976 - val_loss: 2703599264.1035\n",
      "Epoch 3807/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 868827506.0979 - val_loss: 8169164356.1586\n",
      "Epoch 3808/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1570632150.9420 - val_loss: 3622756305.4447\n",
      "Epoch 3809/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1082117753.2651 - val_loss: 2630659761.0397\n",
      "Epoch 3810/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 779305977.1750 - val_loss: 2509245858.9570\n",
      "Epoch 3811/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 818678690.6472 - val_loss: 3187948380.0664\n",
      "Epoch 3812/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1305592274.9443 - val_loss: 2602991058.0681\n",
      "Epoch 3813/7000\n",
      "3554/3554 [==============================] - 0s 69us/step - loss: 851380926.2893 - val_loss: 3078369099.6782\n",
      "Epoch 3814/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1151383084.4750 - val_loss: 2890281315.0515\n",
      "Epoch 3815/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 810403275.2729 - val_loss: 5740127964.2824\n",
      "Epoch 3816/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1271508130.2938 - val_loss: 2628530996.3837\n",
      "Epoch 3817/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1014993895.5723 - val_loss: 2872337430.2961\n",
      "Epoch 3818/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1046225220.5020 - val_loss: 3317399432.3533\n",
      "Epoch 3819/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 819574024.4637 - val_loss: 2621304169.9736\n",
      "Epoch 3820/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 866726990.8205 - val_loss: 2627589581.9972\n",
      "Epoch 3821/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 852617534.7755 - val_loss: 2810577677.6281\n",
      "Epoch 3822/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 948962338.7957 - val_loss: 2671252540.4068\n",
      "Epoch 3823/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 790235720.4876 - val_loss: 2543362608.5972\n",
      "Epoch 3824/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 963846498.6111 - val_loss: 3405315718.5530\n",
      "Epoch 3825/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1087806381.1187 - val_loss: 2730700012.6830\n",
      "Epoch 3826/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1022406218.6246 - val_loss: 3112409381.6709\n",
      "Epoch 3827/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1007233601.6927 - val_loss: 2781439783.2484\n",
      "Epoch 3828/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 851085944.1486 - val_loss: 4467231729.8858\n",
      "Epoch 3829/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1075755011.3765 - val_loss: 3040196392.3443\n",
      "Epoch 3830/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 907723886.0281 - val_loss: 2778266579.8571\n",
      "Epoch 3831/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 963359288.7608 - val_loss: 3395163845.0228\n",
      "Epoch 3832/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1114254662.5909 - val_loss: 5016447547.4183\n",
      "Epoch 3833/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1226859407.3427 - val_loss: 2662206254.6273\n",
      "Epoch 3834/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 812052143.3607 - val_loss: 2940330531.5691\n",
      "Epoch 3835/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 898148232.1396 - val_loss: 3737249494.6835\n",
      "Epoch 3836/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1113513128.6618 - val_loss: 3669166013.5336\n",
      "Epoch 3837/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1071902077.4789 - val_loss: 3019416755.7761\n",
      "Epoch 3838/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1141560749.1277 - val_loss: 2574461479.8762\n",
      "Epoch 3839/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 753106233.0490 - val_loss: 2822902568.9564\n",
      "Epoch 3840/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 857842327.4463 - val_loss: 3371377688.8979\n",
      "Epoch 3841/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 859680182.1677 - val_loss: 3179765149.6326\n",
      "Epoch 3842/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1016019444.2589 - val_loss: 2659920637.8712\n",
      "Epoch 3843/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1147221948.0743 - val_loss: 2982881745.8588\n",
      "Epoch 3844/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 834321824.8374 - val_loss: 2673937004.4579\n",
      "Epoch 3845/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1012770241.1255 - val_loss: 2521817985.4447\n",
      "Epoch 3846/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 732083851.3810 - val_loss: 2580023425.1331\n",
      "Epoch 3847/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1008795980.9297 - val_loss: 2521062136.0068\n",
      "Epoch 3848/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 844585024.3421 - val_loss: 2794461406.0546\n",
      "Epoch 3849/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 882938141.3889 - val_loss: 3378867540.7572\n",
      "Epoch 3850/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 818959335.2212 - val_loss: 2797172014.6543\n",
      "Epoch 3851/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1176750308.8441 - val_loss: 5211742725.7249\n",
      "Epoch 3852/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1130105259.1469 - val_loss: 2904048550.5800\n",
      "Epoch 3853/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 932807433.2200 - val_loss: 2994585475.9516\n",
      "Epoch 3854/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 864193864.9409 - val_loss: 2759001943.7997\n",
      "Epoch 3855/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 898102365.4609 - val_loss: 2545144232.1637\n",
      "Epoch 3856/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 830447451.9842 - val_loss: 5710037131.8098\n",
      "Epoch 3857/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 1199659082.3365 - val_loss: 2771690400.1170\n",
      "Epoch 3858/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 829923792.3151 - val_loss: 4509927152.0855\n",
      "Epoch 3859/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1226507587.4755 - val_loss: 2951488045.8532\n",
      "Epoch 3860/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 857143432.3827 - val_loss: 2739360314.4146\n",
      "Epoch 3861/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 764400417.5667 - val_loss: 3928796336.4726\n",
      "Epoch 3862/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 822659538.5661 - val_loss: 2504403818.0546\n",
      "Epoch 3863/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 754900558.1654 - val_loss: 2554650476.3544\n",
      "Epoch 3864/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 903600851.5566 - val_loss: 2476125007.3114\n",
      "Epoch 3865/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 840632686.6584 - val_loss: 2610847772.3184\n",
      "Epoch 3866/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 931113278.0011 - val_loss: 2790822259.8661\n",
      "Epoch 3867/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 860734903.3922 - val_loss: 2867717475.2675\n",
      "Epoch 3868/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 888598172.8486 - val_loss: 2893401220.3882\n",
      "Epoch 3869/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1005880026.8497 - val_loss: 2593753203.7941\n",
      "Epoch 3870/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 884811096.1317 - val_loss: 4394321171.7131\n",
      "Epoch 3871/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 848729044.2228 - val_loss: 3149177216.2340\n",
      "Epoch 3872/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 871034520.2746 - val_loss: 2544427896.0788\n",
      "Epoch 3873/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1018927997.9111 - val_loss: 5176178990.6993\n",
      "Epoch 3874/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1496580837.2043 - val_loss: 2677367276.1609\n",
      "Epoch 3875/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 960268719.9010 - val_loss: 3144944392.8214\n",
      "Epoch 3876/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 774495508.5470 - val_loss: 2411303123.5195\n",
      "Epoch 3877/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 732533388.6055 - val_loss: 2614167075.8616\n",
      "Epoch 3878/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 859154338.2150 - val_loss: 3800658039.9707\n",
      "Epoch 3879/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1062079104.1801 - val_loss: 2516438409.9961\n",
      "Epoch 3880/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 755223094.8880 - val_loss: 2669105021.8824\n",
      "Epoch 3881/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 979189392.8464 - val_loss: 2524906620.4579\n",
      "Epoch 3882/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 867296647.2752 - val_loss: 3197436265.6405\n",
      "Epoch 3883/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 861834967.4463 - val_loss: 2458238993.3997\n",
      "Epoch 3884/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 809291214.7304 - val_loss: 3496491410.4709\n",
      "Epoch 3885/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 862377504.1441 - val_loss: 2747965588.7752\n",
      "Epoch 3886/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 852522975.1536 - val_loss: 10768849625.6900\n",
      "Epoch 3887/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1404243806.5774 - val_loss: 2765336043.0447\n",
      "Epoch 3888/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1047942132.4074 - val_loss: 2828365593.1139\n",
      "Epoch 3889/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 849655344.1171 - val_loss: 4032695924.3702\n",
      "Epoch 3890/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1068320853.7895 - val_loss: 4051312733.2906\n",
      "Epoch 3891/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1060663264.9252 - val_loss: 2820474430.3617\n",
      "Epoch 3892/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 919979007.6398 - val_loss: 2682631151.8312\n",
      "Epoch 3893/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 962630431.8739 - val_loss: 5652555814.5980\n",
      "Epoch 3894/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1089464859.5160 - val_loss: 3541240680.4321\n",
      "Epoch 3895/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1039250442.8227 - val_loss: 2635775514.2346\n",
      "Epoch 3896/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 79us/step - loss: 740911592.0090 - val_loss: 2723042930.2729\n",
      "Epoch 3897/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 934461549.4159 - val_loss: 2518330848.9181\n",
      "Epoch 3898/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 847948172.5335 - val_loss: 2907121450.9997\n",
      "Epoch 3899/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 921189537.9268 - val_loss: 5903952932.6537\n",
      "Epoch 3900/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1334670012.2183 - val_loss: 2989181522.8219\n",
      "Epoch 3901/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 863725265.1165 - val_loss: 3459213679.5274\n",
      "Epoch 3902/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 913570975.5318 - val_loss: 3242357675.5308\n",
      "Epoch 3903/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 883092059.2639 - val_loss: 2749185973.8667\n",
      "Epoch 3904/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 974984651.2009 - val_loss: 2681436718.8433\n",
      "Epoch 3905/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 766630273.5847 - val_loss: 4437855963.2023\n",
      "Epoch 3906/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 880585459.3225 - val_loss: 2640924565.0723\n",
      "Epoch 3907/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 823253456.1711 - val_loss: 2763905163.5668\n",
      "Epoch 3908/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1196038016.6483 - val_loss: 4974582617.9241\n",
      "Epoch 3909/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1220052728.0405 - val_loss: 3404528506.4101\n",
      "Epoch 3910/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 983217446.5729 - val_loss: 2542865493.5494\n",
      "Epoch 3911/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 940011354.7406 - val_loss: 2554678989.2861\n",
      "Epoch 3912/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 792223573.5734 - val_loss: 3184686979.3125\n",
      "Epoch 3913/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 843313987.8177 - val_loss: 2479988989.8104\n",
      "Epoch 3914/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 800954890.9848 - val_loss: 3101739235.1899\n",
      "Epoch 3915/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 852270284.1373 - val_loss: 2479464723.3755\n",
      "Epoch 3916/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 786972064.0630 - val_loss: 2739027360.4771\n",
      "Epoch 3917/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 780247494.8790 - val_loss: 2537287533.2658\n",
      "Epoch 3918/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1058189816.9859 - val_loss: 2616014861.5741\n",
      "Epoch 3919/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 863175018.4626 - val_loss: 2878167693.3941\n",
      "Epoch 3920/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 843297643.8672 - val_loss: 2689790223.2484\n",
      "Epoch 3921/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 749083588.9702 - val_loss: 2538277911.0346\n",
      "Epoch 3922/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 792362459.2999 - val_loss: 2835479185.0307\n",
      "Epoch 3923/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1037397448.5492 - val_loss: 2880667110.6340\n",
      "Epoch 3924/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 767259217.1165 - val_loss: 2568663739.2338\n",
      "Epoch 3925/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 937664722.0979 - val_loss: 3588439752.7674\n",
      "Epoch 3926/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 801519650.6472 - val_loss: 2981068520.0743\n",
      "Epoch 3927/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 933372926.0957 - val_loss: 3004213452.7640\n",
      "Epoch 3928/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1054694661.3303 - val_loss: 4207466660.8338\n",
      "Epoch 3929/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1224241574.3928 - val_loss: 3346685329.3907\n",
      "Epoch 3930/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 863518398.4513 - val_loss: 2691410174.2717\n",
      "Epoch 3931/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 757000351.0906 - val_loss: 2919994326.4855\n",
      "Epoch 3932/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 804147031.9437 - val_loss: 2632657415.8672\n",
      "Epoch 3933/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 844144996.2319 - val_loss: 4389218761.8295\n",
      "Epoch 3934/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1001132131.7276 - val_loss: 2407042045.3750\n",
      "Epoch 3935/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 734624902.0597 - val_loss: 2695250060.8180\n",
      "Epoch 3936/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 834906693.5824 - val_loss: 2660565974.8816\n",
      "Epoch 3937/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 826923161.5262 - val_loss: 3248677150.5688\n",
      "Epoch 3938/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 800869619.7727 - val_loss: 2710556112.8506\n",
      "Epoch 3939/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 925782920.0675 - val_loss: 2854817090.2323\n",
      "Epoch 3940/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 794711805.2628 - val_loss: 2413341543.9482\n",
      "Epoch 3941/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 713545255.8154 - val_loss: 3070956484.0056\n",
      "Epoch 3942/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 847515153.0895 - val_loss: 2713527847.4622\n",
      "Epoch 3943/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 783595401.0760 - val_loss: 4233858363.1932\n",
      "Epoch 3944/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 872193553.5678 - val_loss: 2491015699.6051\n",
      "Epoch 3945/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 887019998.2532 - val_loss: 2838774294.6745\n",
      "Epoch 3946/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 791079554.3005 - val_loss: 3553388565.7474\n",
      "Epoch 3947/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 812940897.6747 - val_loss: 3165191966.3167\n",
      "Epoch 3948/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1232391499.6331 - val_loss: 4293491315.3620\n",
      "Epoch 3949/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1304143862.8160 - val_loss: 2999044661.6304\n",
      "Epoch 3950/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1067855887.2167 - val_loss: 3126762261.1353\n",
      "Epoch 3951/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 780219349.5622 - val_loss: 2876047644.9755\n",
      "Epoch 3952/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 926496785.0715 - val_loss: 2883474399.4419\n",
      "Epoch 3953/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 819936144.6393 - val_loss: 2677175606.2965\n",
      "Epoch 3954/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 796817478.7349 - val_loss: 2476486030.6532\n",
      "Epoch 3955/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 806815574.8520 - val_loss: 2742822371.9561\n",
      "Epoch 3956/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1154436851.8987 - val_loss: 2740656901.1758\n",
      "Epoch 3957/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 976700684.6505 - val_loss: 2573939305.8205\n",
      "Epoch 3958/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 783744768.7203 - val_loss: 2732593436.0664\n",
      "Epoch 3959/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 948447374.4783 - val_loss: 2597772347.1752\n",
      "Epoch 3960/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 914231451.7276 - val_loss: 2911246700.9170\n",
      "Epoch 3961/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1009173068.6055 - val_loss: 2442799758.7083\n",
      "Epoch 3962/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 793210024.8835 - val_loss: 2607843896.3319\n",
      "Epoch 3963/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 844884835.0523 - val_loss: 2566938703.9235\n",
      "Epoch 3964/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 730308091.7862 - val_loss: 2509461949.2366\n",
      "Epoch 3965/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 821092867.9077 - val_loss: 2644222056.4051\n",
      "Epoch 3966/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 788422275.0974 - val_loss: 2905515368.7764\n",
      "Epoch 3967/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1032165667.2594 - val_loss: 2845019642.3516\n",
      "Epoch 3968/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 912374598.9330 - val_loss: 2879198313.2444\n",
      "Epoch 3969/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 870632642.1970 - val_loss: 2670598638.7533\n",
      "Epoch 3970/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 1149954268.7046 - val_loss: 2591682987.5803\n",
      "Epoch 3971/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1054918414.6854 - val_loss: 2554695394.1243\n",
      "Epoch 3972/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 785468849.0490 - val_loss: 2674532459.7873\n",
      "Epoch 3973/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 780926084.2499 - val_loss: 3006588003.8211\n",
      "Epoch 3974/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 722115927.0861 - val_loss: 3077936708.1226\n",
      "Epoch 3975/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 969436522.3545 - val_loss: 2589032047.2034\n",
      "Epoch 3976/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 687690361.5892 - val_loss: 2939796685.6641\n",
      "Epoch 3977/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 884384240.5914 - val_loss: 2496243426.9885\n",
      "Epoch 3978/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 707931154.3680 - val_loss: 2605786400.7606\n",
      "Epoch 3979/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 926565880.5988 - val_loss: 2537099583.9550\n",
      "Epoch 3980/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 780761006.5504 - val_loss: 2586592815.1471\n",
      "Epoch 3981/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 935443202.3770 - val_loss: 3360213781.2433\n",
      "Epoch 3982/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 873125949.9471 - val_loss: 5002460197.1578\n",
      "Epoch 3983/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1404370311.7074 - val_loss: 2734923552.5311\n",
      "Epoch 3984/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 766441777.7558 - val_loss: 2617717842.9390\n",
      "Epoch 3985/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 855051013.7468 - val_loss: 4111549853.8487\n",
      "Epoch 3986/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 790974803.9887 - val_loss: 2721124395.5713\n",
      "Epoch 3987/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 767227717.7265 - val_loss: 3167596839.6782\n",
      "Epoch 3988/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 747360959.7479 - val_loss: 2551970618.5271\n",
      "Epoch 3989/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 864162571.0929 - val_loss: 3880119091.1460\n",
      "Epoch 3990/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 951140418.1249 - val_loss: 5145244884.2892\n",
      "Epoch 3991/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 840629861.1322 - val_loss: 2506561598.9086\n",
      "Epoch 3992/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 842098202.9218 - val_loss: 2415015532.3229\n",
      "Epoch 3993/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 748179306.1024 - val_loss: 2903953102.0242\n",
      "Epoch 3994/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 881055572.8531 - val_loss: 2785739869.0869\n",
      "Epoch 3995/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 905757818.9398 - val_loss: 2521921649.5662\n",
      "Epoch 3996/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 789307751.5813 - val_loss: 2710684205.3221\n",
      "Epoch 3997/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 744799698.3410 - val_loss: 4037500985.6630\n",
      "Epoch 3998/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1008750454.4918 - val_loss: 2459529638.8051\n",
      "Epoch 3999/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 965755399.8875 - val_loss: 2583851568.4456\n",
      "Epoch 4000/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 729238179.8627 - val_loss: 2426021123.6726\n",
      "Epoch 4001/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 985530181.0782 - val_loss: 2845973862.5229\n",
      "Epoch 4002/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 1033023611.3900 - val_loss: 2461690290.0028\n",
      "Epoch 4003/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 901155388.2183 - val_loss: 3435532441.6641\n",
      "Epoch 4004/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1443217227.2729 - val_loss: 2741264699.9044\n",
      "Epoch 4005/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 769629104.1891 - val_loss: 3111416309.4729\n",
      "Epoch 4006/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 896427124.5740 - val_loss: 2591349547.6208\n",
      "Epoch 4007/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 746927198.3613 - val_loss: 2343991822.7331\n",
      "Epoch 4008/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 773240474.7597 - val_loss: 2372955026.0726\n",
      "Epoch 4009/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 710472129.8008 - val_loss: 2597774525.5876\n",
      "Epoch 4010/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 834300940.6595 - val_loss: 2633029945.7350\n",
      "Epoch 4011/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 786594676.8351 - val_loss: 2511630535.8042\n",
      "Epoch 4012/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 911054098.3770 - val_loss: 2802582432.0810\n",
      "Epoch 4013/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 762276053.6095 - val_loss: 2677140790.2425\n",
      "Epoch 4014/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 736130514.0439 - val_loss: 2928713344.9001\n",
      "Epoch 4015/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 754350963.4665 - val_loss: 2529378305.5415\n",
      "Epoch 4016/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 903619329.3281 - val_loss: 3045093491.4610\n",
      "Epoch 4017/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 999549262.1542 - val_loss: 2720520374.2627\n",
      "Epoch 4018/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 746864569.7693 - val_loss: 2627485798.4360\n",
      "Epoch 4019/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 753652532.2409 - val_loss: 2604617340.1789\n",
      "Epoch 4020/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 843599318.4378 - val_loss: 2990775216.1395\n",
      "Epoch 4021/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 941297674.4626 - val_loss: 2848513555.8639\n",
      "Epoch 4022/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 894124460.0968 - val_loss: 2350907605.9477\n",
      "Epoch 4023/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 964467640.8149 - val_loss: 2424455090.2459\n",
      "Epoch 4024/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 720682815.0366 - val_loss: 2487933044.1733\n",
      "Epoch 4025/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 765403646.8497 - val_loss: 2534674692.1767\n",
      "Epoch 4026/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 76us/step - loss: 743222950.0506 - val_loss: 2552190927.6804\n",
      "Epoch 4027/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 710785410.3050 - val_loss: 2503139672.3938\n",
      "Epoch 4028/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 752125793.6387 - val_loss: 3433050225.0937\n",
      "Epoch 4029/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 764269469.7130 - val_loss: 2429625995.1167\n",
      "Epoch 4030/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 884615737.7693 - val_loss: 5513447684.6087\n",
      "Epoch 4031/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1015431740.3354 - val_loss: 3682898501.0228\n",
      "Epoch 4032/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 814050584.2386 - val_loss: 2382988809.9826\n",
      "Epoch 4033/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 772905522.7822 - val_loss: 3102958142.8568\n",
      "Epoch 4034/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 837158563.7952 - val_loss: 2927974799.0774\n",
      "Epoch 4035/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 927193978.5256 - val_loss: 2506125134.2537\n",
      "Epoch 4036/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 755938392.4907 - val_loss: 5038210005.0093\n",
      "Epoch 4037/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1544654612.3489 - val_loss: 2431898047.8200\n",
      "Epoch 4038/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 680459841.2335 - val_loss: 3188752049.4684\n",
      "Epoch 4039/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 887510550.7957 - val_loss: 3516673868.7460\n",
      "Epoch 4040/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 779818082.5256 - val_loss: 2497293758.9018\n",
      "Epoch 4041/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 746878799.9190 - val_loss: 3185016364.6110\n",
      "Epoch 4042/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 784400041.4041 - val_loss: 2331387133.2952\n",
      "Epoch 4043/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 706609673.0512 - val_loss: 2466158620.2308\n",
      "Epoch 4044/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 801311724.6595 - val_loss: 3106700384.0810\n",
      "Epoch 4045/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 722155833.8413 - val_loss: 2670144973.8982\n",
      "Epoch 4046/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 818227724.5160 - val_loss: 2330842641.4824\n",
      "Epoch 4047/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 732630698.6066 - val_loss: 3358433421.5381\n",
      "Epoch 4048/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 908992792.7788 - val_loss: 3070257316.9058\n",
      "Epoch 4049/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 935557855.1176 - val_loss: 2466493760.7561\n",
      "Epoch 4050/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 885585945.2651 - val_loss: 2338667427.9111\n",
      "Epoch 4051/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 923992218.6021 - val_loss: 2663938890.0456\n",
      "Epoch 4052/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 693493987.9977 - val_loss: 2503954296.6729\n",
      "Epoch 4053/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 694031944.6078 - val_loss: 2496406211.7986\n",
      "Epoch 4054/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 693557041.9178 - val_loss: 2645336751.0549\n",
      "Epoch 4055/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 860513185.3506 - val_loss: 2399103380.7584\n",
      "Epoch 4056/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 749278008.9049 - val_loss: 2427307155.2090\n",
      "Epoch 4057/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 755212725.8796 - val_loss: 2498272973.5291\n",
      "Epoch 4058/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 808366564.5560 - val_loss: 3592887553.2242\n",
      "Epoch 4059/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 915114276.7991 - val_loss: 2600914079.2709\n",
      "Epoch 4060/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 739448913.6792 - val_loss: 2538025141.2523\n",
      "Epoch 4061/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 833428737.8008 - val_loss: 2451086035.1010\n",
      "Epoch 4062/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 708686700.6235 - val_loss: 5175514901.4954\n",
      "Epoch 4063/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 864591685.5824 - val_loss: 3845101907.2450\n",
      "Epoch 4064/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 1018041743.6128 - val_loss: 2834640653.7947\n",
      "Epoch 4065/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1036294154.8137 - val_loss: 2429277412.5367\n",
      "Epoch 4066/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 853889856.7203 - val_loss: 2951600854.4135\n",
      "Epoch 4067/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 826579262.2172 - val_loss: 2552040572.9575\n",
      "Epoch 4068/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 709301525.7535 - val_loss: 2952827314.9300\n",
      "Epoch 4069/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 918661801.1480 - val_loss: 2652924783.1997\n",
      "Epoch 4070/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 926515617.3506 - val_loss: 2403382472.7674\n",
      "Epoch 4071/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 930654374.2487 - val_loss: 2464390065.2197\n",
      "Epoch 4072/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 943958086.3748 - val_loss: 2421349771.7750\n",
      "Epoch 4073/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 846269057.8008 - val_loss: 2396724936.7882\n",
      "Epoch 4074/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 723932907.5791 - val_loss: 2851362847.7030\n",
      "Epoch 4075/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 855524140.7676 - val_loss: 3527309155.4565\n",
      "Epoch 4076/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 867771283.6646 - val_loss: 2541830446.5193\n",
      "Epoch 4077/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 779278972.6505 - val_loss: 2411132791.2776\n",
      "Epoch 4078/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 720961745.1795 - val_loss: 2941570139.0942\n",
      "Epoch 4079/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 751980461.9921 - val_loss: 2290389331.4520\n",
      "Epoch 4080/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 746023338.6427 - val_loss: 2482897270.3775\n",
      "Epoch 4081/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 778323460.5380 - val_loss: 3649403929.7080\n",
      "Epoch 4082/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 968915533.4609 - val_loss: 3037824234.4113\n",
      "Epoch 4083/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 731460948.6370 - val_loss: 2434357557.8644\n",
      "Epoch 4084/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 886953839.9910 - val_loss: 2358364281.0869\n",
      "Epoch 4085/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1005947949.9561 - val_loss: 2767097584.8686\n",
      "Epoch 4086/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 922533617.8728 - val_loss: 2486193558.1435\n",
      "Epoch 4087/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 646189198.8205 - val_loss: 2571277224.5603\n",
      "Epoch 4088/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 827879299.3855 - val_loss: 2850815713.8453\n",
      "Epoch 4089/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 741319661.1277 - val_loss: 2903308255.9100\n",
      "Epoch 4090/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 949084692.1688 - val_loss: 2412772756.7212\n",
      "Epoch 4091/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 750091647.3517 - val_loss: 3493289273.5010\n",
      "Epoch 4092/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 975002789.5363 - val_loss: 2526290526.5598\n",
      "Epoch 4093/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 648148842.6967 - val_loss: 3019114049.5302\n",
      "Epoch 4094/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 754668682.1925 - val_loss: 4069959622.8591\n",
      "Epoch 4095/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 950967289.1210 - val_loss: 2822595591.0639\n",
      "Epoch 4096/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 718961077.3033 - val_loss: 2524576919.2281\n",
      "Epoch 4097/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 647913884.9567 - val_loss: 2415589433.8250\n",
      "Epoch 4098/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 724361703.1806 - val_loss: 2319045708.1519\n",
      "Epoch 4099/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 905182162.6742 - val_loss: 2483712523.4723\n",
      "Epoch 4100/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 722967097.9809 - val_loss: 3180395525.4368\n",
      "Epoch 4101/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 711734107.5160 - val_loss: 2421754301.3333\n",
      "Epoch 4102/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 861311266.4311 - val_loss: 2930352456.2093\n",
      "Epoch 4103/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 626012184.9589 - val_loss: 2334473272.8169\n",
      "Epoch 4104/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 1066302736.3512 - val_loss: 2575186730.7927\n",
      "Epoch 4105/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 795594954.8407 - val_loss: 2529285843.6141\n",
      "Epoch 4106/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 827251769.7693 - val_loss: 2588607269.9319\n",
      "Epoch 4107/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 1003668043.0568 - val_loss: 4598735926.9446\n",
      "Epoch 4108/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1187922295.1761 - val_loss: 2908289897.1904\n",
      "Epoch 4109/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 693006161.1525 - val_loss: 2393010713.1342\n",
      "Epoch 4110/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 958238297.0310 - val_loss: 3239982509.9162\n",
      "Epoch 4111/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 1594712464.8554 - val_loss: 2638804877.1150\n",
      "Epoch 4112/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 694251021.7670 - val_loss: 2896161173.7834\n",
      "Epoch 4113/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 748304588.6775 - val_loss: 2870544991.3429\n",
      "Epoch 4114/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 743695347.8267 - val_loss: 4140149415.1021\n",
      "Epoch 4115/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 746869550.9645 - val_loss: 2441216770.6982\n",
      "Epoch 4116/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 885496167.2932 - val_loss: 2738685422.2436\n",
      "Epoch 4117/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 804010439.7794 - val_loss: 2554439311.1494\n",
      "Epoch 4118/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 743222620.0968 - val_loss: 2877931246.6633\n",
      "Epoch 4119/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 630776280.9049 - val_loss: 2743406960.4321\n",
      "Epoch 4120/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 668142451.1604 - val_loss: 2382034596.9103\n",
      "Epoch 4121/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 796999039.6218 - val_loss: 2826812134.9221\n",
      "Epoch 4122/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 726600965.9516 - val_loss: 2243054631.4397\n",
      "Epoch 4123/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 703712824.9274 - val_loss: 2704296544.3511\n",
      "Epoch 4124/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 715639945.1480 - val_loss: 2505465008.0675\n",
      "Epoch 4125/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 803386112.5943 - val_loss: 2694163449.6990\n",
      "Epoch 4126/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 636250252.8872 - val_loss: 2350791735.9887\n",
      "Epoch 4127/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 685705042.1519 - val_loss: 2570840240.1125\n",
      "Epoch 4128/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 771696270.9105 - val_loss: 2997647948.9440\n",
      "Epoch 4129/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 701199081.2741 - val_loss: 3043575681.1612\n",
      "Epoch 4130/7000\n",
      "3554/3554 [==============================] - 0s 98us/step - loss: 805438474.3365 - val_loss: 2860593961.6945\n",
      "Epoch 4131/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 829803302.2127 - val_loss: 2325563511.2506\n",
      "Epoch 4132/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 983593671.2392 - val_loss: 3270393876.2352\n",
      "Epoch 4133/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 935379337.0039 - val_loss: 2604404671.1944\n",
      "Epoch 4134/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 1034147449.0129 - val_loss: 2372893325.3401\n",
      "Epoch 4135/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 720574476.0653 - val_loss: 2614373361.6608\n",
      "Epoch 4136/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 742431730.5954 - val_loss: 3412417366.2650\n",
      "Epoch 4137/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 803476416.1621 - val_loss: 3219424931.3575\n",
      "Epoch 4138/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1045028993.7198 - val_loss: 2921893116.3274\n",
      "Epoch 4139/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 838755557.2403 - val_loss: 2494730955.1077\n",
      "Epoch 4140/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 883051327.0636 - val_loss: 2573245239.0526\n",
      "Epoch 4141/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 774354419.6646 - val_loss: 2437926496.4951\n",
      "Epoch 4142/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 702869852.4389 - val_loss: 3683663797.3603\n",
      "Epoch 4143/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 820539451.4980 - val_loss: 2982521566.0107\n",
      "Epoch 4144/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 845186137.0670 - val_loss: 2891813218.4214\n",
      "Epoch 4145/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 679629787.9572 - val_loss: 2405948004.3792\n",
      "Epoch 4146/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 685506596.2859 - val_loss: 2326355247.2214\n",
      "Epoch 4147/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 788029879.8965 - val_loss: 2552192986.6982\n",
      "Epoch 4148/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 657006472.6618 - val_loss: 3049700474.3831\n",
      "Epoch 4149/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 654655833.1570 - val_loss: 2434563348.7482\n",
      "Epoch 4150/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1169052262.3028 - val_loss: 2707921009.7418\n",
      "Epoch 4151/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 769236272.1531 - val_loss: 2683413106.0523\n",
      "Epoch 4152/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 757706647.7704 - val_loss: 2849386552.8079\n",
      "Epoch 4153/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 880778296.4727 - val_loss: 2409378454.7134\n",
      "Epoch 4154/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 729586530.2150 - val_loss: 2330376988.4940\n",
      "Epoch 4155/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 724969436.4885 - val_loss: 3352351180.4399\n",
      "Epoch 4156/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 81us/step - loss: 891798577.0310 - val_loss: 2383736503.6017\n",
      "Epoch 4157/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 704298069.8931 - val_loss: 2785192135.1336\n",
      "Epoch 4158/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 713341885.7670 - val_loss: 2575562342.5440\n",
      "Epoch 4159/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 827313238.4378 - val_loss: 4020920996.2577\n",
      "Epoch 4160/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 901511710.0732 - val_loss: 2538075617.7913\n",
      "Epoch 4161/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 839685940.1508 - val_loss: 3816742353.1927\n",
      "Epoch 4162/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 889609128.9859 - val_loss: 2377201682.4529\n",
      "Epoch 4163/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 876480126.2352 - val_loss: 2531490870.4315\n",
      "Epoch 4164/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 643343442.2802 - val_loss: 2544069411.8976\n",
      "Epoch 4165/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 756901506.1249 - val_loss: 2713965871.2844\n",
      "Epoch 4166/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 866820894.7935 - val_loss: 2408870157.4267\n",
      "Epoch 4167/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 630791356.8666 - val_loss: 3591953818.3921\n",
      "Epoch 4168/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 958951458.7192 - val_loss: 3195614498.8399\n",
      "Epoch 4169/7000\n",
      "3554/3554 [==============================] - 0s 93us/step - loss: 740218884.5920 - val_loss: 2366842276.2577\n",
      "Epoch 4170/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 677908481.0805 - val_loss: 2588467618.9435\n",
      "Epoch 4171/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 788023200.0900 - val_loss: 2613712644.7359\n",
      "Epoch 4172/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 802205038.4603 - val_loss: 2335402376.5018\n",
      "Epoch 4173/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 716328807.4373 - val_loss: 3792731910.7443\n",
      "Epoch 4174/7000\n",
      "3554/3554 [==============================] - 0s 104us/step - loss: 827159549.8571 - val_loss: 2256137827.6006\n",
      "Epoch 4175/7000\n",
      "3554/3554 [==============================] - 0s 114us/step - loss: 667801145.1750 - val_loss: 2332261351.4712\n",
      "Epoch 4176/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 700851855.0906 - val_loss: 2505848717.9702\n",
      "Epoch 4177/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 738682173.3348 - val_loss: 2418691563.5398\n",
      "Epoch 4178/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 771213599.5498 - val_loss: 4184132605.3356\n",
      "Epoch 4179/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 883052118.3298 - val_loss: 2982011436.7010\n",
      "Epoch 4180/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 939369694.4333 - val_loss: 4242884518.3145\n",
      "Epoch 4181/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 1592917115.0298 - val_loss: 4833134153.0914\n",
      "Epoch 4182/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 998011964.7946 - val_loss: 2236360650.2436\n",
      "Epoch 4183/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 667751611.2459 - val_loss: 3050423398.8838\n",
      "Epoch 4184/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 817119499.0298 - val_loss: 2373175324.7865\n",
      "Epoch 4185/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 652282638.7541 - val_loss: 2974127289.8228\n",
      "Epoch 4186/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 645477604.9544 - val_loss: 2461043345.5797\n",
      "Epoch 4187/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 681949078.3838 - val_loss: 2395950505.2579\n",
      "Epoch 4188/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 707740674.4491 - val_loss: 2726170440.3353\n",
      "Epoch 4189/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 867442307.4395 - val_loss: 2615480367.1089\n",
      "Epoch 4190/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 759074884.2499 - val_loss: 3031552047.6354\n",
      "Epoch 4191/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 701935574.0788 - val_loss: 2455567477.5224\n",
      "Epoch 4192/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 728773170.7102 - val_loss: 2717902128.6076\n",
      "Epoch 4193/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 931508142.7935 - val_loss: 2330918912.4321\n",
      "Epoch 4194/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 618537619.6646 - val_loss: 2396363015.5454\n",
      "Epoch 4195/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 705050719.6173 - val_loss: 2415009314.2414\n",
      "Epoch 4196/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 708970018.4311 - val_loss: 2249999493.3198\n",
      "Epoch 4197/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 686356951.1131 - val_loss: 2515076438.4495\n",
      "Epoch 4198/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 770260199.9415 - val_loss: 7898716088.2228\n",
      "Epoch 4199/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 1558464871.1491 - val_loss: 2855889010.4619\n",
      "Epoch 4200/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 771867412.3129 - val_loss: 2882839442.8129\n",
      "Epoch 4201/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 697612423.2842 - val_loss: 2307525111.8447\n",
      "Epoch 4202/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 654133580.7676 - val_loss: 2646088275.2563\n",
      "Epoch 4203/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 786233668.3849 - val_loss: 2349396738.6419\n",
      "Epoch 4204/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 640602142.9195 - val_loss: 2960519184.1665\n",
      "Epoch 4205/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 763678533.1503 - val_loss: 2980831762.0208\n",
      "Epoch 4206/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 826992142.9105 - val_loss: 2605961160.6762\n",
      "Epoch 4207/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 729284985.6612 - val_loss: 2373557009.0037\n",
      "Epoch 4208/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 758501399.1941 - val_loss: 2571264103.6861\n",
      "Epoch 4209/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 801280439.7254 - val_loss: 2883391035.2158\n",
      "Epoch 4210/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 751264006.2127 - val_loss: 2508204000.0630\n",
      "Epoch 4211/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 681210162.7282 - val_loss: 2882394735.0684\n",
      "Epoch 4212/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 678725672.8239 - val_loss: 2337403308.4534\n",
      "Epoch 4213/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 742788195.7096 - val_loss: 2689044239.9325\n",
      "Epoch 4214/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 786626988.5875 - val_loss: 2587668060.5705\n",
      "Epoch 4215/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 783414683.0118 - val_loss: 2437820669.9657\n",
      "Epoch 4216/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 751380754.4401 - val_loss: 2632815623.1201\n",
      "Epoch 4217/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 800939218.8362 - val_loss: 2427811651.6636\n",
      "Epoch 4218/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 667696923.2279 - val_loss: 6642920943.8695\n",
      "Epoch 4219/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 1122024390.7349 - val_loss: 3117614660.7955\n",
      "Epoch 4220/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 730866073.2876 - val_loss: 2438177627.7603\n",
      "Epoch 4221/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 744030561.6432 - val_loss: 2511425372.9035\n",
      "Epoch 4222/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 731922809.5172 - val_loss: 2301123661.0745\n",
      "Epoch 4223/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 856100903.6083 - val_loss: 2525608796.9271\n",
      "Epoch 4224/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 816402968.5672 - val_loss: 2643773052.1654\n",
      "Epoch 4225/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 667190172.0203 - val_loss: 2536337901.9702\n",
      "Epoch 4226/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 892232133.6545 - val_loss: 2366742228.3207\n",
      "Epoch 4227/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 658980138.0664 - val_loss: 2522373203.9966\n",
      "Epoch 4228/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 622428415.6038 - val_loss: 2387249329.3007\n",
      "Epoch 4229/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 633898266.3635 - val_loss: 6514382409.2714\n",
      "Epoch 4230/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 855977130.7867 - val_loss: 2348049300.0551\n",
      "Epoch 4231/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 807421449.5442 - val_loss: 2797498370.1063\n",
      "Epoch 4232/7000\n",
      "3554/3554 [==============================] - 0s 96us/step - loss: 879691258.2375 - val_loss: 3061641316.5997\n",
      "Epoch 4233/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 880664302.9105 - val_loss: 2303308586.9187\n",
      "Epoch 4234/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 775726622.9736 - val_loss: 2743096719.3384\n",
      "Epoch 4235/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 682419409.9178 - val_loss: 2386624549.6416\n",
      "Epoch 4236/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 592596775.1131 - val_loss: 2807037977.0419\n",
      "Epoch 4237/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 905803013.6185 - val_loss: 2471472807.1786\n",
      "Epoch 4238/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 699778427.4620 - val_loss: 3028905505.7342\n",
      "Epoch 4239/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 837795099.5521 - val_loss: 2277405063.4622\n",
      "Epoch 4240/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 780003075.3855 - val_loss: 2208512317.0340\n",
      "Epoch 4241/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 756928082.1159 - val_loss: 3213185481.7215\n",
      "Epoch 4242/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 816706962.4896 - val_loss: 2491193335.5567\n",
      "Epoch 4243/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 666104526.7304 - val_loss: 2621077236.9598\n",
      "Epoch 4244/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 736116049.1795 - val_loss: 3078207483.1752\n",
      "Epoch 4245/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 1029029039.4688 - val_loss: 3605981323.9899\n",
      "Epoch 4246/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 712954061.4339 - val_loss: 2872953041.4807\n",
      "Epoch 4247/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 994344029.7220 - val_loss: 2289473169.7733\n",
      "Epoch 4248/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 672380874.6967 - val_loss: 2481151424.7921\n",
      "Epoch 4249/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 757069072.4952 - val_loss: 2534462057.5662\n",
      "Epoch 4250/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 730662256.4052 - val_loss: 2267633162.0546\n",
      "Epoch 4251/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 759240835.2459 - val_loss: 2507563070.6903\n",
      "Epoch 4252/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 655476934.5819 - val_loss: 2306924443.4858\n",
      "Epoch 4253/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 564530656.6303 - val_loss: 2359269203.2349\n",
      "Epoch 4254/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 619855541.3393 - val_loss: 2299530356.1632\n",
      "Epoch 4255/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 836799863.7164 - val_loss: 2345289304.9969\n",
      "Epoch 4256/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 776758528.8959 - val_loss: 2378484286.1817\n",
      "Epoch 4257/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 835588858.5256 - val_loss: 3590262247.2990\n",
      "Epoch 4258/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 898468401.2155 - val_loss: 2712275730.4349\n",
      "Epoch 4259/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 628403920.6213 - val_loss: 2608525097.7665\n",
      "Epoch 4260/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 680909175.0771 - val_loss: 3366975864.8349\n",
      "Epoch 4261/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 809333756.9747 - val_loss: 2678777014.3685\n",
      "Epoch 4262/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 790422127.7569 - val_loss: 2580049270.3865\n",
      "Epoch 4263/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1101072058.7777 - val_loss: 2412345838.5013\n",
      "Epoch 4264/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 761358502.4873 - val_loss: 2726647250.2909\n",
      "Epoch 4265/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 654208564.0338 - val_loss: 2193554301.5876\n",
      "Epoch 4266/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 552459763.2234 - val_loss: 2237027738.6037\n",
      "Epoch 4267/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 630366693.2583 - val_loss: 2339819418.7072\n",
      "Epoch 4268/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 623934919.6714 - val_loss: 2509832169.4965\n",
      "Epoch 4269/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 696831548.6505 - val_loss: 2319586251.1707\n",
      "Epoch 4270/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 720223205.7040 - val_loss: 2430781492.5502\n",
      "Epoch 4271/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 705754038.0957 - val_loss: 2472836464.8866\n",
      "Epoch 4272/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 906953391.5048 - val_loss: 2537895663.4037\n",
      "Epoch 4273/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 627955257.6252 - val_loss: 2440136246.5485\n",
      "Epoch 4274/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 931635387.7862 - val_loss: 2321353477.1308\n",
      "Epoch 4275/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 914941075.4665 - val_loss: 2631148561.3187\n",
      "Epoch 4276/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 621708555.5971 - val_loss: 2572204821.6394\n",
      "Epoch 4277/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 868945523.7907 - val_loss: 3131810719.5049\n",
      "Epoch 4278/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 649445886.5594 - val_loss: 2678728660.4512\n",
      "Epoch 4279/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 942141035.7591 - val_loss: 4161184165.6259\n",
      "Epoch 4280/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 788591053.9471 - val_loss: 2630027195.5893\n",
      "Epoch 4281/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 777953650.3860 - val_loss: 2304322368.6571\n",
      "Epoch 4282/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 732048399.7029 - val_loss: 2745236549.1398\n",
      "Epoch 4283/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 694117800.2836 - val_loss: 2965585818.0208\n",
      "Epoch 4284/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 655798766.9747 - val_loss: 2259678173.4346\n",
      "Epoch 4285/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 713446256.6573 - val_loss: 2355374100.0281\n",
      "Epoch 4286/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 98us/step - loss: 829513872.6393 - val_loss: 3461801247.0368\n",
      "Epoch 4287/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 1024880025.9313 - val_loss: 2330991458.4056\n",
      "Epoch 4288/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 579403788.3534 - val_loss: 2392859238.0118\n",
      "Epoch 4289/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 810397021.0647 - val_loss: 3174637643.6748\n",
      "Epoch 4290/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 612376703.3517 - val_loss: 2402686965.9724\n",
      "Epoch 4291/7000\n",
      "3554/3554 [==============================] - 0s 103us/step - loss: 710022513.1435 - val_loss: 2343047486.5418\n",
      "Epoch 4292/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 760163187.8627 - val_loss: 2358729746.1693\n",
      "Epoch 4293/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 661944011.8492 - val_loss: 2893185077.5764\n",
      "Epoch 4294/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 645535555.4575 - val_loss: 2636152605.0205\n",
      "Epoch 4295/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 657294019.6736 - val_loss: 2987667984.6256\n",
      "Epoch 4296/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 903790650.3815 - val_loss: 2930934145.5212\n",
      "Epoch 4297/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 734674766.1542 - val_loss: 2341930330.8692\n",
      "Epoch 4298/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 833546202.0394 - val_loss: 2679638835.3620\n",
      "Epoch 4299/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 895004626.5841 - val_loss: 3974643465.3682\n",
      "Epoch 4300/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 865142385.0174 - val_loss: 2886220676.1001\n",
      "Epoch 4301/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 743522906.3455 - val_loss: 4179201294.7983\n",
      "Epoch 4302/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 817545247.0456 - val_loss: 2250669952.5761\n",
      "Epoch 4303/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 730424130.5031 - val_loss: 2600251263.3024\n",
      "Epoch 4304/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 643987601.8638 - val_loss: 2807033724.7010\n",
      "Epoch 4305/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 667870127.1086 - val_loss: 2483558761.7305\n",
      "Epoch 4306/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 709064076.7136 - val_loss: 3054640883.0020\n",
      "Epoch 4307/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 798355960.9859 - val_loss: 2272262676.4107\n",
      "Epoch 4308/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 578052484.2228 - val_loss: 2221732815.4644\n",
      "Epoch 4309/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 690779142.9871 - val_loss: 3094353945.4920\n",
      "Epoch 4310/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 1021665944.8644 - val_loss: 2622375435.6703\n",
      "Epoch 4311/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 679513967.8289 - val_loss: 2288870867.6456\n",
      "Epoch 4312/7000\n",
      "3554/3554 [==============================] - 0s 110us/step - loss: 640664436.1238 - val_loss: 2635660036.8428\n",
      "Epoch 4313/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 724063427.9257 - val_loss: 2306446993.3457\n",
      "Epoch 4314/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 744527132.8846 - val_loss: 2260071813.9049\n",
      "Epoch 4315/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 730601018.9083 - val_loss: 2579052514.9165\n",
      "Epoch 4316/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 582463235.3855 - val_loss: 2446654629.0048\n",
      "Epoch 4317/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 662473982.6314 - val_loss: 4044617920.9181\n",
      "Epoch 4318/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 897085985.4226 - val_loss: 2379953005.9814\n",
      "Epoch 4319/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 845079116.8576 - val_loss: 2262713432.8079\n",
      "Epoch 4320/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 770498398.4333 - val_loss: 2403061893.2523\n",
      "Epoch 4321/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 951322558.3073 - val_loss: 2427815427.8796\n",
      "Epoch 4322/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 688679255.1581 - val_loss: 2590384081.1027\n",
      "Epoch 4323/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 637777595.2279 - val_loss: 2513619001.6450\n",
      "Epoch 4324/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 771867127.1401 - val_loss: 2282504132.5187\n",
      "Epoch 4325/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 717131644.9747 - val_loss: 8241959113.7035\n",
      "Epoch 4326/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1073505576.4817 - val_loss: 3960610975.0729\n",
      "Epoch 4327/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1281301084.0788 - val_loss: 2383539007.6579\n",
      "Epoch 4328/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 604414997.9156 - val_loss: 2464817375.5387\n",
      "Epoch 4329/7000\n",
      "3554/3554 [==============================] - 0s 100us/step - loss: 606436566.6539 - val_loss: 3056444946.2819\n",
      "Epoch 4330/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 710030115.5892 - val_loss: 2920234753.7643\n",
      "Epoch 4331/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 704988871.9781 - val_loss: 2851195501.2231\n",
      "Epoch 4332/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 664803971.2594 - val_loss: 2252919744.0540\n",
      "Epoch 4333/7000\n",
      "3554/3554 [==============================] - 0s 105us/step - loss: 634681278.2172 - val_loss: 2349314629.0948\n",
      "Epoch 4334/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 627955498.4986 - val_loss: 6540051587.6546\n",
      "Epoch 4335/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 1684815801.6072 - val_loss: 2306469179.3013\n",
      "Epoch 4336/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 572919398.4648 - val_loss: 2432219534.9963\n",
      "Epoch 4337/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 659095784.6978 - val_loss: 2832374793.0734\n",
      "Epoch 4338/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 650753693.6050 - val_loss: 3072664414.6228\n",
      "Epoch 4339/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 797486884.4479 - val_loss: 3245615134.3167\n",
      "Epoch 4340/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 656026102.6246 - val_loss: 2255014346.5857\n",
      "Epoch 4341/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 595984046.8925 - val_loss: 3276550697.6045\n",
      "Epoch 4342/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 738308089.6680 - val_loss: 2592099458.5789\n",
      "Epoch 4343/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 833443908.8981 - val_loss: 2263746844.2644\n",
      "Epoch 4344/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 897561240.9949 - val_loss: 2232401047.1224\n",
      "Epoch 4345/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 741138907.4080 - val_loss: 2322696748.3567\n",
      "Epoch 4346/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 617468505.3146 - val_loss: 2331560743.7547\n",
      "Epoch 4347/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 670116435.7006 - val_loss: 2217946990.1457\n",
      "Epoch 4348/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 633269025.3866 - val_loss: 2198034782.0062\n",
      "Epoch 4349/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 726940867.9977 - val_loss: 2407519197.0025\n",
      "Epoch 4350/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 636086038.4378 - val_loss: 3487424610.9075\n",
      "Epoch 4351/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 700361145.1210 - val_loss: 2278039262.6678\n",
      "Epoch 4352/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 679092154.3635 - val_loss: 3181815523.4115\n",
      "Epoch 4353/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 800415905.8188 - val_loss: 2671145685.3502\n",
      "Epoch 4354/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 770264071.7074 - val_loss: 2601900984.0248\n",
      "Epoch 4355/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 617545206.5864 - val_loss: 2413821598.8478\n",
      "Epoch 4356/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 594764449.9924 - val_loss: 2837634214.1840\n",
      "Epoch 4357/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 699938321.4316 - val_loss: 2412129836.3792\n",
      "Epoch 4358/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 919583808.6843 - val_loss: 2483503733.2973\n",
      "Epoch 4359/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 770348198.0687 - val_loss: 3248961907.3620\n",
      "Epoch 4360/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 597071152.8993 - val_loss: 2449789160.0563\n",
      "Epoch 4361/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 596809094.9871 - val_loss: 4053142523.6793\n",
      "Epoch 4362/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 808358309.8165 - val_loss: 2686638039.6737\n",
      "Epoch 4363/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 727317520.5312 - val_loss: 2714533925.1578\n",
      "Epoch 4364/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 763739996.4164 - val_loss: 2185643332.4557\n",
      "Epoch 4365/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 739349925.4609 - val_loss: 2395091684.2757\n",
      "Epoch 4366/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 767460206.2082 - val_loss: 3558988246.7376\n",
      "Epoch 4367/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 946320867.1154 - val_loss: 3020789773.8982\n",
      "Epoch 4368/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 868195983.3427 - val_loss: 3806023811.0605\n",
      "Epoch 4369/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 650822956.2093 - val_loss: 2231765005.8532\n",
      "Epoch 4370/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 559277644.4254 - val_loss: 2389950474.7162\n",
      "Epoch 4371/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 725351315.2684 - val_loss: 2233586965.7654\n",
      "Epoch 4372/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 598854449.3416 - val_loss: 2298853672.4343\n",
      "Epoch 4373/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 643875939.2594 - val_loss: 2250646521.3840\n",
      "Epoch 4374/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 703226389.1412 - val_loss: 2329654995.5421\n",
      "Epoch 4375/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 674474241.8908 - val_loss: 2614949253.1308\n",
      "Epoch 4376/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 641634386.4401 - val_loss: 2194654426.8422\n",
      "Epoch 4377/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 636030147.1874 - val_loss: 3072758719.0540\n",
      "Epoch 4378/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 657453023.2977 - val_loss: 4306123583.7840\n",
      "Epoch 4379/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 1334891333.6095 - val_loss: 2296800560.0855\n",
      "Epoch 4380/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 675073636.7001 - val_loss: 2807352059.8774\n",
      "Epoch 4381/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 858722443.5611 - val_loss: 2173103213.9297\n",
      "Epoch 4382/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 600375728.2431 - val_loss: 2223540401.6968\n",
      "Epoch 4383/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 556867718.1913 - val_loss: 2264241195.3688\n",
      "Epoch 4384/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 640605389.2178 - val_loss: 2393215881.7530\n",
      "Epoch 4385/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 674181707.2639 - val_loss: 2320844788.5772\n",
      "Epoch 4386/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 614579540.9252 - val_loss: 4675320264.2453\n",
      "Epoch 4387/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 964953125.0692 - val_loss: 3269778671.4734\n",
      "Epoch 4388/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 673335528.9499 - val_loss: 2754385206.6925\n",
      "Epoch 4389/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 973436024.3647 - val_loss: 2887749787.0807\n",
      "Epoch 4390/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 706550218.8497 - val_loss: 2198012493.1781\n",
      "Epoch 4391/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 616829142.2938 - val_loss: 2620666837.9432\n",
      "Epoch 4392/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 723639948.7496 - val_loss: 2511161509.9679\n",
      "Epoch 4393/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 718609675.5250 - val_loss: 2454700150.8748\n",
      "Epoch 4394/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 633291558.3208 - val_loss: 2341138418.6059\n",
      "Epoch 4395/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 643946827.2909 - val_loss: 2184000659.8571\n",
      "Epoch 4396/7000\n",
      "3554/3554 [==============================] - ETA: 0s - loss: 906036119.692 - 0s 71us/step - loss: 885637857.9449 - val_loss: 2522342219.3148\n",
      "Epoch 4397/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 726074065.9358 - val_loss: 2425369063.0751\n",
      "Epoch 4398/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 648696083.1604 - val_loss: 2850462535.5072\n",
      "Epoch 4399/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 954669099.7952 - val_loss: 2796764574.7488\n",
      "Epoch 4400/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1025913221.4271 - val_loss: 2428080293.8959\n",
      "Epoch 4401/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 643271604.0788 - val_loss: 3326926156.2509\n",
      "Epoch 4402/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 780153664.5312 - val_loss: 3069929231.6985\n",
      "Epoch 4403/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 666541932.6640 - val_loss: 2620424473.2759\n",
      "Epoch 4404/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 549129421.1097 - val_loss: 2182213270.8186\n",
      "Epoch 4405/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 598579781.6545 - val_loss: 2416232643.4565\n",
      "Epoch 4406/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 784861386.1204 - val_loss: 2634927004.6605\n",
      "Epoch 4407/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 989768001.7648 - val_loss: 2782087412.1722\n",
      "Epoch 4408/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 824474771.9437 - val_loss: 2196410239.6219\n",
      "Epoch 4409/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 566161294.6944 - val_loss: 4974696087.9437\n",
      "Epoch 4410/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 837497476.9882 - val_loss: 2343117366.7015\n",
      "Epoch 4411/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 679005755.7141 - val_loss: 2771371128.0518\n",
      "Epoch 4412/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 757739711.6038 - val_loss: 2539188380.6020\n",
      "Epoch 4413/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 825813031.3292 - val_loss: 2332316740.2667\n",
      "Epoch 4414/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 593224503.8244 - val_loss: 2219401860.5502\n",
      "Epoch 4415/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 630028179.5205 - val_loss: 3647826931.9117\n",
      "Epoch 4416/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 75us/step - loss: 896451682.6832 - val_loss: 2568299435.0582\n",
      "Epoch 4417/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 801214935.3742 - val_loss: 2348395808.5401\n",
      "Epoch 4418/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 646212648.8779 - val_loss: 2357962541.1150\n",
      "Epoch 4419/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 563903183.9550 - val_loss: 4018924869.7789\n",
      "Epoch 4420/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 896679777.0985 - val_loss: 2590397320.4253\n",
      "Epoch 4421/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 902979180.8036 - val_loss: 2645854936.3758\n",
      "Epoch 4422/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 732414389.9696 - val_loss: 2476374543.4374\n",
      "Epoch 4423/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 1034137979.3180 - val_loss: 4941480344.8799\n",
      "Epoch 4424/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 911585182.6854 - val_loss: 2305166678.1322\n",
      "Epoch 4425/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 566840662.0056 - val_loss: 3560244713.7485\n",
      "Epoch 4426/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 1194790906.2735 - val_loss: 2200309823.0278\n",
      "Epoch 4427/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 683983417.1210 - val_loss: 2194409756.3274\n",
      "Epoch 4428/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 594286098.4581 - val_loss: 2302281294.7983\n",
      "Epoch 4429/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 753200627.9527 - val_loss: 2375377878.8636\n",
      "Epoch 4430/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 563935546.7417 - val_loss: 2229640053.0183\n",
      "Epoch 4431/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 572433255.3652 - val_loss: 2501497288.0495\n",
      "Epoch 4432/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 701667475.8087 - val_loss: 2442888672.7111\n",
      "Epoch 4433/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 694549308.1823 - val_loss: 2300596892.5075\n",
      "Epoch 4434/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 780922454.0551 - val_loss: 2443134626.2008\n",
      "Epoch 4435/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 543865575.6759 - val_loss: 2294228123.3103\n",
      "Epoch 4436/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 603483246.4783 - val_loss: 2502627629.3311\n",
      "Epoch 4437/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 534468763.1019 - val_loss: 3502693243.5713\n",
      "Epoch 4438/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 875433710.7485 - val_loss: 2404484433.6518\n",
      "Epoch 4439/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 644754328.4187 - val_loss: 2485189010.5429\n",
      "Epoch 4440/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 761885534.7462 - val_loss: 2356079027.5421\n",
      "Epoch 4441/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 625541303.1761 - val_loss: 3859647036.2014\n",
      "Epoch 4442/7000\n",
      "3554/3554 [==============================] - 0s 107us/step - loss: 708310637.8481 - val_loss: 2303563014.7826\n",
      "Epoch 4443/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 734553297.7198 - val_loss: 3057133801.2444\n",
      "Epoch 4444/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 838859028.7361 - val_loss: 2320149361.0847\n",
      "Epoch 4445/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 694862891.0028 - val_loss: 2267432016.3297\n",
      "Epoch 4446/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 641668655.6443 - val_loss: 2203711604.5232\n",
      "Epoch 4447/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 553002927.5408 - val_loss: 2952063277.5831\n",
      "Epoch 4448/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 817572522.0304 - val_loss: 2226456849.7238\n",
      "Epoch 4449/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 581544962.0259 - val_loss: 2118028387.6186\n",
      "Epoch 4450/7000\n",
      "3554/3554 [==============================] - 0s 89us/step - loss: 614794198.4159 - val_loss: 2622994956.6200\n",
      "Epoch 4451/7000\n",
      "3554/3554 [==============================] - 0s 97us/step - loss: 592371534.8745 - val_loss: 3454492603.1572\n",
      "Epoch 4452/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 914389334.7259 - val_loss: 2682224405.5494\n",
      "Epoch 4453/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 777325253.6950 - val_loss: 2239031547.7333\n",
      "Epoch 4454/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 687879585.2786 - val_loss: 2853124366.5463\n",
      "Epoch 4455/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 696366904.5492 - val_loss: 2157148369.3367\n",
      "Epoch 4456/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 553782734.6584 - val_loss: 2225064651.3508\n",
      "Epoch 4457/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 648005534.0371 - val_loss: 2395315197.2006\n",
      "Epoch 4458/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 642081903.3495 - val_loss: 2435252629.0093\n",
      "Epoch 4459/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 617328165.2943 - val_loss: 2266716608.5446\n",
      "Epoch 4460/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 672808019.4845 - val_loss: 2740684956.5525\n",
      "Epoch 4461/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 816117500.3264 - val_loss: 2196724761.8982\n",
      "Epoch 4462/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 691342368.5222 - val_loss: 3044247707.0222\n",
      "Epoch 4463/7000\n",
      "3554/3554 [==============================] - 0s 94us/step - loss: 801795691.9032 - val_loss: 2212679548.7145\n",
      "Epoch 4464/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 663939831.9685 - val_loss: 2218418322.1018\n",
      "Epoch 4465/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 639888619.1649 - val_loss: 2205449982.8208\n",
      "Epoch 4466/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 640690769.2425 - val_loss: 3116174338.1243\n",
      "Epoch 4467/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 837163192.5988 - val_loss: 2129447925.1353\n",
      "Epoch 4468/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 578021598.2172 - val_loss: 2615414157.5111\n",
      "Epoch 4469/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 594751582.5954 - val_loss: 2175831206.7421\n",
      "Epoch 4470/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 653076577.2335 - val_loss: 2262274701.6731\n",
      "Epoch 4471/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 649648559.0366 - val_loss: 2514477924.9710\n",
      "Epoch 4472/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 879659979.6331 - val_loss: 2755995798.0444\n",
      "Epoch 4473/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 603193877.7895 - val_loss: 2455270830.3527\n",
      "Epoch 4474/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 780189592.2746 - val_loss: 2520894662.7331\n",
      "Epoch 4475/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 692568703.2077 - val_loss: 2791263150.8613\n",
      "Epoch 4476/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 855379119.6849 - val_loss: 2723987357.4076\n",
      "Epoch 4477/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 860539062.4918 - val_loss: 2465785130.5181\n",
      "Epoch 4478/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 887477309.9471 - val_loss: 2271529405.5381\n",
      "Epoch 4479/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 927573829.9516 - val_loss: 2182116427.5083\n",
      "Epoch 4480/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 558821132.1013 - val_loss: 3323408842.5677\n",
      "Epoch 4481/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 669380449.9358 - val_loss: 2185146553.1679\n",
      "Epoch 4482/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 654323215.3967 - val_loss: 2454890523.3485\n",
      "Epoch 4483/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 743301659.2279 - val_loss: 2426126550.3955\n",
      "Epoch 4484/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 658311592.6618 - val_loss: 2285275025.4447\n",
      "Epoch 4485/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 555405793.8548 - val_loss: 3635970194.6149\n",
      "Epoch 4486/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 827845532.7766 - val_loss: 2297271197.3266\n",
      "Epoch 4487/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 653524566.5459 - val_loss: 3094457665.1702\n",
      "Epoch 4488/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 582494252.5875 - val_loss: 2388708226.7454\n",
      "Epoch 4489/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 635816099.1154 - val_loss: 4376293601.2512\n",
      "Epoch 4490/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 962622389.4204 - val_loss: 2270819264.2070\n",
      "Epoch 4491/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 670085631.3517 - val_loss: 3294317126.2920\n",
      "Epoch 4492/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 808988880.8914 - val_loss: 2449232500.5232\n",
      "Epoch 4493/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 1084391908.5560 - val_loss: 2904462550.1997\n",
      "Epoch 4494/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 575857346.0529 - val_loss: 2370102497.0712\n",
      "Epoch 4495/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 590522788.3039 - val_loss: 2683638459.4903\n",
      "Epoch 4496/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 647164102.2307 - val_loss: 2748036720.9496\n",
      "Epoch 4497/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 746886616.7068 - val_loss: 3349204288.6661\n",
      "Epoch 4498/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 664748702.1542 - val_loss: 2191596027.0852\n",
      "Epoch 4499/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 529270262.1677 - val_loss: 2350127675.3328\n",
      "Epoch 4500/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 593019119.7209 - val_loss: 2285841061.2298\n",
      "Epoch 4501/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 530761968.1531 - val_loss: 2396119277.6011\n",
      "Epoch 4502/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 614202718.1452 - val_loss: 2840024053.1173\n",
      "Epoch 4503/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 762559146.1384 - val_loss: 2151943467.5893\n",
      "Epoch 4504/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 755428108.4615 - val_loss: 2706581996.6830\n",
      "Epoch 4505/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 761047491.2774 - val_loss: 2594591563.8819\n",
      "Epoch 4506/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 649456453.3303 - val_loss: 2353890633.4515\n",
      "Epoch 4507/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 736799687.8694 - val_loss: 2323668778.5496\n",
      "Epoch 4508/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 646622406.0867 - val_loss: 2962050866.7679\n",
      "Epoch 4509/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 941737809.0219 - val_loss: 2276031826.8039\n",
      "Epoch 4510/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 752561379.5836 - val_loss: 2543692658.4214\n",
      "Epoch 4511/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 688897669.6815 - val_loss: 2355045981.3086\n",
      "Epoch 4512/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 582597855.3697 - val_loss: 2376042651.2383\n",
      "Epoch 4513/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 809071319.5183 - val_loss: 2481539059.5421\n",
      "Epoch 4514/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 598652270.8565 - val_loss: 3448968683.9269\n",
      "Epoch 4515/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 835594972.7046 - val_loss: 2534194185.3615\n",
      "Epoch 4516/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 550282984.2521 - val_loss: 2665886180.5277\n",
      "Epoch 4517/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 584433313.9989 - val_loss: 2374205202.9480\n",
      "Epoch 4518/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 572953650.9443 - val_loss: 2233142166.0895\n",
      "Epoch 4519/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 591181107.2752 - val_loss: 2624675145.8115\n",
      "Epoch 4520/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 616323979.0838 - val_loss: 2423196789.3783\n",
      "Epoch 4521/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1009069804.5515 - val_loss: 7237056608.1710\n",
      "Epoch 4522/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 932077511.9055 - val_loss: 2163125115.3553\n",
      "Epoch 4523/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 564444745.7603 - val_loss: 3750284209.0397\n",
      "Epoch 4524/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 803589027.6151 - val_loss: 2116729704.5873\n",
      "Epoch 4525/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 516165777.3416 - val_loss: 2312203298.4754\n",
      "Epoch 4526/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 583982630.4828 - val_loss: 2492677505.3502\n",
      "Epoch 4527/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 604172266.2465 - val_loss: 3827603965.4436\n",
      "Epoch 4528/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 878615076.4479 - val_loss: 2647743992.5288\n",
      "Epoch 4529/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 710824209.5217 - val_loss: 2214420024.6188\n",
      "Epoch 4530/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 542118097.4676 - val_loss: 2568931205.8329\n",
      "Epoch 4531/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 761685367.6263 - val_loss: 2564463517.0565\n",
      "Epoch 4532/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 598185677.0557 - val_loss: 2245494063.6354\n",
      "Epoch 4533/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 672112638.1767 - val_loss: 2340816435.3350\n",
      "Epoch 4534/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 706563199.3517 - val_loss: 3028261847.0976\n",
      "Epoch 4535/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 914961017.3731 - val_loss: 3315638293.0318\n",
      "Epoch 4536/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 720293689.6252 - val_loss: 2244360524.7370\n",
      "Epoch 4537/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 591828235.7411 - val_loss: 2199092921.2669\n",
      "Epoch 4538/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 711550341.6275 - val_loss: 3179154365.1916\n",
      "Epoch 4539/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 657805106.9083 - val_loss: 2174772963.3935\n",
      "Epoch 4540/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 573163414.9060 - val_loss: 2185502869.1893\n",
      "Epoch 4541/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 601947794.4041 - val_loss: 2184481297.6518\n",
      "Epoch 4542/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 712856525.3618 - val_loss: 3456831951.0414\n",
      "Epoch 4543/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 716417318.7079 - val_loss: 2559077077.9454\n",
      "Epoch 4544/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 696768935.4553 - val_loss: 3442096016.7786\n",
      "Epoch 4545/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 617230237.9471 - val_loss: 2232747183.3654\n",
      "Epoch 4546/7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554/3554 [==============================] - 0s 98us/step - loss: 662054629.1120 - val_loss: 2703065195.6028\n",
      "Epoch 4547/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 971183006.3973 - val_loss: 3444286444.7730\n",
      "Epoch 4548/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 788921464.6798 - val_loss: 2498870589.5696\n",
      "Epoch 4549/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 616948094.8655 - val_loss: 2089530553.0014\n",
      "Epoch 4550/7000\n",
      "3554/3554 [==============================] - 0s 92us/step - loss: 570236599.3922 - val_loss: 3057568307.1730\n",
      "Epoch 4551/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 833407259.5881 - val_loss: 2472005852.7190\n",
      "Epoch 4552/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 573572724.9071 - val_loss: 3193904040.7224\n",
      "Epoch 4553/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 806204171.8852 - val_loss: 2973957759.0639\n",
      "Epoch 4554/7000\n",
      "3554/3554 [==============================] - 0s 87us/step - loss: 619319141.7085 - val_loss: 3007212005.2478\n",
      "Epoch 4555/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 676316268.8374 - val_loss: 2560905627.5083\n",
      "Epoch 4556/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 559800184.7473 - val_loss: 2070142232.3961\n",
      "Epoch 4557/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 638997464.3827 - val_loss: 2658490860.5570\n",
      "Epoch 4558/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 914726898.3500 - val_loss: 2696397338.8444\n",
      "Epoch 4559/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 707594443.4890 - val_loss: 2519317794.8354\n",
      "Epoch 4560/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 626720159.3427 - val_loss: 2621947978.0456\n",
      "Epoch 4561/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 570576718.3703 - val_loss: 2152655455.8695\n",
      "Epoch 4562/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 524803653.2403 - val_loss: 2522345462.9266\n",
      "Epoch 4563/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 880299398.2667 - val_loss: 2498941504.7741\n",
      "Epoch 4564/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 887302977.9809 - val_loss: 2386618221.8172\n",
      "Epoch 4565/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 589167628.2814 - val_loss: 2269367479.3046\n",
      "Epoch 4566/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 531079435.2369 - val_loss: 2227133797.4188\n",
      "Epoch 4567/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 611148100.9342 - val_loss: 2220811693.3311\n",
      "Epoch 4568/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 523397439.6109 - val_loss: 2243060448.9271\n",
      "Epoch 4569/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 938695749.7625 - val_loss: 3990383195.8504\n",
      "Epoch 4570/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 575464365.8571 - val_loss: 2535245256.1553\n",
      "Epoch 4571/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 612641929.9403 - val_loss: 3734167260.7865\n",
      "Epoch 4572/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 773563634.4041 - val_loss: 2187716449.7058\n",
      "Epoch 4573/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 525340276.5560 - val_loss: 2142152579.3125\n",
      "Epoch 4574/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 606029427.0535 - val_loss: 2086082615.6377\n",
      "Epoch 4575/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 601623828.8621 - val_loss: 2639714463.5769\n",
      "Epoch 4576/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 788591282.9443 - val_loss: 2202459986.5429\n",
      "Epoch 4577/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 570751980.0203 - val_loss: 3434556023.7187\n",
      "Epoch 4578/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 650190957.7940 - val_loss: 2846886915.9966\n",
      "Epoch 4579/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 656792095.7299 - val_loss: 2616774447.0594\n",
      "Epoch 4580/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 668053799.1851 - val_loss: 2541837222.9221\n",
      "Epoch 4581/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 549682011.0838 - val_loss: 2497977312.3511\n",
      "Epoch 4582/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 717082926.0642 - val_loss: 2388598217.7215\n",
      "Epoch 4583/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 561282439.8334 - val_loss: 2271834714.5181\n",
      "Epoch 4584/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 598747734.5819 - val_loss: 3364589031.5679\n",
      "Epoch 4585/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 879796608.7203 - val_loss: 3160102022.5350\n",
      "Epoch 4586/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 754297453.6320 - val_loss: 3691990190.3212\n",
      "Epoch 4587/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 721688690.3500 - val_loss: 2593875434.1446\n",
      "Epoch 4588/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 667499254.6179 - val_loss: 2126190248.9114\n",
      "Epoch 4589/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 628431246.7259 - val_loss: 2427503688.1823\n",
      "Epoch 4590/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 496581087.4418 - val_loss: 2727264912.4186\n",
      "Epoch 4591/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 636898371.5656 - val_loss: 2648092932.6537\n",
      "Epoch 4592/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 894921145.5757 - val_loss: 2307854299.5443\n",
      "Epoch 4593/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 665682578.2240 - val_loss: 2436855851.1077\n",
      "Epoch 4594/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 598679726.0371 - val_loss: 2600525344.8371\n",
      "Epoch 4595/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 550592791.9685 - val_loss: 2250379845.1668\n",
      "Epoch 4596/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 675696693.6241 - val_loss: 2109010954.5632\n",
      "Epoch 4597/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 591633640.0568 - val_loss: 3082929983.4779\n",
      "Epoch 4598/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 689176102.3568 - val_loss: 3196383830.8456\n",
      "Epoch 4599/7000\n",
      "3554/3554 [==============================] - 0s 101us/step - loss: 1155327967.5768 - val_loss: 2234954003.5601\n",
      "Epoch 4600/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 537157495.6443 - val_loss: 2141556926.4158\n",
      "Epoch 4601/7000\n",
      "3554/3554 [==============================] - 0s 81us/step - loss: 513972430.1182 - val_loss: 2114739801.1724\n",
      "Epoch 4602/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 529470789.3056 - val_loss: 3814239541.0003\n",
      "Epoch 4603/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 979963212.4615 - val_loss: 2114309050.3786\n",
      "Epoch 4604/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 543555069.4429 - val_loss: 2071112438.0759\n",
      "Epoch 4605/7000\n",
      "3554/3554 [==============================] - 0s 70us/step - loss: 797597528.8059 - val_loss: 2072134488.4343\n",
      "Epoch 4606/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 614573152.0900 - val_loss: 2923770344.4523\n",
      "Epoch 4607/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 741885585.6477 - val_loss: 2781967207.4183\n",
      "Epoch 4608/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 1003632687.6353 - val_loss: 2743273021.6416\n",
      "Epoch 4609/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 695691783.4192 - val_loss: 3223461630.4158\n",
      "Epoch 4610/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 735127866.4052 - val_loss: 2136922869.8644\n",
      "Epoch 4611/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 604237505.0805 - val_loss: 2180008008.7854\n",
      "Epoch 4612/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 547974430.9195 - val_loss: 2749717343.0188\n",
      "Epoch 4613/7000\n",
      "3554/3554 [==============================] - 0s 82us/step - loss: 692274401.7828 - val_loss: 2389037332.8203\n",
      "Epoch 4614/7000\n",
      "3554/3554 [==============================] - 0s 91us/step - loss: 809517891.2954 - val_loss: 2220702612.4827\n",
      "Epoch 4615/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 553147855.6308 - val_loss: 2331211110.5980\n",
      "Epoch 4616/7000\n",
      "3554/3554 [==============================] - 0s 86us/step - loss: 597799538.2420 - val_loss: 3139744967.6872\n",
      "Epoch 4617/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 817041318.9510 - val_loss: 2222100340.8923\n",
      "Epoch 4618/7000\n",
      "3554/3554 [==============================] - 0s 90us/step - loss: 535811845.1142 - val_loss: 2373394160.6616\n",
      "Epoch 4619/7000\n",
      "3554/3554 [==============================] - 0s 71us/step - loss: 627180432.5582 - val_loss: 2129464809.1724\n",
      "Epoch 4620/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 526858721.7359 - val_loss: 2589262098.7724\n",
      "Epoch 4621/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 675014927.5948 - val_loss: 2201157799.9122\n",
      "Epoch 4622/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 652565604.2679 - val_loss: 2593218919.9437\n",
      "Epoch 4623/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 556735135.0276 - val_loss: 2669346517.2253\n",
      "Epoch 4624/7000\n",
      "3554/3554 [==============================] - 0s 85us/step - loss: 537360835.2594 - val_loss: 2733803408.6706\n",
      "Epoch 4625/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 629414746.4806 - val_loss: 2147852582.3685\n",
      "Epoch 4626/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 595764776.7338 - val_loss: 2224714615.8807\n",
      "Epoch 4627/7000\n",
      "3554/3554 [==============================] - 0s 73us/step - loss: 603157770.2825 - val_loss: 2163284200.9924\n",
      "Epoch 4628/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 616338100.3759 - val_loss: 2664116166.6790\n",
      "Epoch 4629/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 1033194225.0670 - val_loss: 2074536210.0253\n",
      "Epoch 4630/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 484100934.9330 - val_loss: 2097440412.6650\n",
      "Epoch 4631/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 748078039.4508 - val_loss: 2149474012.4534\n",
      "Epoch 4632/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 766502647.7164 - val_loss: 2463171317.5269\n",
      "Epoch 4633/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 604286705.0355 - val_loss: 2154034974.8208\n",
      "Epoch 4634/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 596724589.5149 - val_loss: 2917541273.0239\n",
      "Epoch 4635/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 879436166.8610 - val_loss: 2823924433.0217\n",
      "Epoch 4636/7000\n",
      "3554/3554 [==============================] - 0s 95us/step - loss: 722934198.6899 - val_loss: 2457564180.0191\n",
      "Epoch 4637/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 672518130.6742 - val_loss: 2105994882.9300\n",
      "Epoch 4638/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 668550187.1469 - val_loss: 2644266244.0349\n",
      "Epoch 4639/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 798382405.5779 - val_loss: 2325944802.2143\n",
      "Epoch 4640/7000\n",
      "3554/3554 [==============================] - 0s 77us/step - loss: 582275944.1576 - val_loss: 2307184362.1266\n",
      "Epoch 4641/7000\n",
      "3554/3554 [==============================] - 0s 78us/step - loss: 658347351.9010 - val_loss: 2277767754.6689\n",
      "Epoch 4642/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 533607964.8486 - val_loss: 2871007130.2391\n",
      "Epoch 4643/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 690210847.9932 - val_loss: 2242168295.4622\n",
      "Epoch 4644/7000\n",
      "3554/3554 [==============================] - 0s 72us/step - loss: 560207653.4654 - val_loss: 2337066827.2338\n",
      "Epoch 4645/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 630219552.8464 - val_loss: 2834913364.9463\n",
      "Epoch 4646/7000\n",
      "3554/3554 [==============================] - 0s 83us/step - loss: 599682114.7732 - val_loss: 2403242629.3648\n",
      "Epoch 4647/7000\n",
      "3554/3554 [==============================] - 0s 88us/step - loss: 804369093.6905 - val_loss: 2250697492.5952\n",
      "Epoch 4648/7000\n",
      "3554/3554 [==============================] - 0s 79us/step - loss: 589162970.5436 - val_loss: 2536012440.7629\n",
      "Epoch 4649/7000\n",
      "3554/3554 [==============================] - 0s 75us/step - loss: 623423570.0439 - val_loss: 2160709117.2816\n",
      "Epoch 4650/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 962335133.1728 - val_loss: 2164736158.8028\n",
      "Epoch 4651/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 581613735.6173 - val_loss: 2262226703.8965\n",
      "Epoch 4652/7000\n",
      "3554/3554 [==============================] - 0s 99us/step - loss: 598130497.0084 - val_loss: 3075868659.0380\n",
      "Epoch 4653/7000\n",
      "3554/3554 [==============================] - 0s 84us/step - loss: 618783783.8694 - val_loss: 2140686773.4504\n",
      "Epoch 4654/7000\n",
      "3554/3554 [==============================] - 0s 74us/step - loss: 530059733.7085 - val_loss: 2366950703.1156\n",
      "Epoch 4655/7000\n",
      "3554/3554 [==============================] - 0s 80us/step - loss: 585114267.5374 - val_loss: 2959355776.0180\n",
      "Epoch 4656/7000\n",
      "3554/3554 [==============================] - 0s 76us/step - loss: 977008445.6590 - val_loss: 2912083801.5820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb36a49e550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model22.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(\"model22 loss:\",model22.loss)\n",
    "\n",
    "model_train22 = model22.fit(predictors,target, epochs=7000, validation_split=0.5, callbacks=[early_stopping_monitor], verbose=True)\n",
    "model_train22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3698f3208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(model_train22.history['val_loss'], 'r',\n",
    "         )\n",
    "plt.title(\"Adam only\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.savefig(\"Adam graph for 7000 epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model22.save(\"adam_7000_epochs_only.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('adam_7000_epochs_only.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7829519.],\n",
       "       [ 14856532.],\n",
       "       [  8116332.],\n",
       "       ..., \n",
       "       [ 18198898.],\n",
       "       [ 15085408.],\n",
       "       [ 13068341.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(X_test.values)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRT_ID</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P05996</td>\n",
       "      <td>7829519.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P09294</td>\n",
       "      <td>14856532.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P03807</td>\n",
       "      <td>8116332.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P00539</td>\n",
       "      <td>14138891.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P01448</td>\n",
       "      <td>6990885.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P06667</td>\n",
       "      <td>13885251.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P09803</td>\n",
       "      <td>14848282.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P05568</td>\n",
       "      <td>21816498.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P05102</td>\n",
       "      <td>14917302.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P04556</td>\n",
       "      <td>5640275.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P03782</td>\n",
       "      <td>9074870.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P02221</td>\n",
       "      <td>10836339.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>P03299</td>\n",
       "      <td>11575795.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>P08959</td>\n",
       "      <td>8974999.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P09377</td>\n",
       "      <td>8067211.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P09924</td>\n",
       "      <td>9985919.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P01394</td>\n",
       "      <td>8284280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>P07407</td>\n",
       "      <td>11760596.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>P08729</td>\n",
       "      <td>12722913.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>P09542</td>\n",
       "      <td>11681933.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>P01853</td>\n",
       "      <td>9996159.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P08051</td>\n",
       "      <td>12991281.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>P08271</td>\n",
       "      <td>5748562.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P02055</td>\n",
       "      <td>9330313.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P01462</td>\n",
       "      <td>6611826.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P03664</td>\n",
       "      <td>13654450.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P02370</td>\n",
       "      <td>6521503.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P07852</td>\n",
       "      <td>19794644.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P06278</td>\n",
       "      <td>15728709.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P08467</td>\n",
       "      <td>13522635.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>P09741</td>\n",
       "      <td>13827377.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>P10029</td>\n",
       "      <td>13358692.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>P07554</td>\n",
       "      <td>9605809.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>P06496</td>\n",
       "      <td>7642491.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>P02878</td>\n",
       "      <td>6096207.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>P00641</td>\n",
       "      <td>8172257.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>P07136</td>\n",
       "      <td>15716956.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>P07199</td>\n",
       "      <td>9103513.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>P08123</td>\n",
       "      <td>18231158.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>P07963</td>\n",
       "      <td>13734205.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>P04055</td>\n",
       "      <td>16961624.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>P07269</td>\n",
       "      <td>10543349.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>P06245</td>\n",
       "      <td>14169439.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>P06008</td>\n",
       "      <td>9329066.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>P02839</td>\n",
       "      <td>7052521.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>P07594</td>\n",
       "      <td>15777341.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>P05269</td>\n",
       "      <td>8745153.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>P09638</td>\n",
       "      <td>7660533.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>P00711</td>\n",
       "      <td>22934614.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>P05814</td>\n",
       "      <td>22498730.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>P01947</td>\n",
       "      <td>3514255.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>P09979</td>\n",
       "      <td>14159429.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>P04405</td>\n",
       "      <td>8487155.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>P01163</td>\n",
       "      <td>8093279.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>P04268</td>\n",
       "      <td>7753186.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>P01065</td>\n",
       "      <td>11921525.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>P08603</td>\n",
       "      <td>9398517.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>P00474</td>\n",
       "      <td>18198898.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>P08173</td>\n",
       "      <td>15085408.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>P00643</td>\n",
       "      <td>13068341.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2925 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PRT_ID            0\n",
       "0     P05996   7829519.00\n",
       "1     P09294  14856532.00\n",
       "2     P03807   8116332.00\n",
       "3     P00539  14138891.00\n",
       "4     P01448   6990885.50\n",
       "5     P06667  13885251.00\n",
       "6     P09803  14848282.00\n",
       "7     P05568  21816498.00\n",
       "8     P05102  14917302.00\n",
       "9     P04556   5640275.00\n",
       "10    P03782   9074870.00\n",
       "11    P02221  10836339.00\n",
       "12    P03299  11575795.00\n",
       "13    P08959   8974999.00\n",
       "14    P09377   8067211.50\n",
       "15    P09924   9985919.00\n",
       "16    P01394   8284280.00\n",
       "17    P07407  11760596.00\n",
       "18    P08729  12722913.00\n",
       "19    P09542  11681933.00\n",
       "20    P01853   9996159.00\n",
       "21    P08051  12991281.00\n",
       "22    P08271   5748562.00\n",
       "23    P02055   9330313.00\n",
       "24    P01462   6611826.00\n",
       "25    P03664  13654450.00\n",
       "26    P02370   6521503.00\n",
       "27    P07852  19794644.00\n",
       "28    P06278  15728709.00\n",
       "29    P08467  13522635.00\n",
       "...      ...          ...\n",
       "2895  P09741  13827377.00\n",
       "2896  P10029  13358692.00\n",
       "2897  P07554   9605809.00\n",
       "2898  P06496   7642491.00\n",
       "2899  P02878   6096207.00\n",
       "2900  P00641   8172257.00\n",
       "2901  P07136  15716956.00\n",
       "2902  P07199   9103513.00\n",
       "2903  P08123  18231158.00\n",
       "2904  P07963  13734205.00\n",
       "2905  P04055  16961624.00\n",
       "2906  P07269  10543349.00\n",
       "2907  P06245  14169439.00\n",
       "2908  P06008   9329066.00\n",
       "2909  P02839   7052521.00\n",
       "2910  P07594  15777341.00\n",
       "2911  P05269   8745153.00\n",
       "2912  P09638   7660533.50\n",
       "2913  P00711  22934614.00\n",
       "2914  P05814  22498730.00\n",
       "2915  P01947   3514255.75\n",
       "2916  P09979  14159429.00\n",
       "2917  P04405   8487155.00\n",
       "2918  P01163   8093279.50\n",
       "2919  P04268   7753186.00\n",
       "2920  P01065  11921525.00\n",
       "2921  P08603   9398517.00\n",
       "2922  P00474  18198898.00\n",
       "2923  P08173  15085408.00\n",
       "2924  P00643  13068341.00\n",
       "\n",
       "[2925 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(data = {\"PRT_ID\":test[\"PRT_ID\"]})\n",
    "\n",
    "result = pd.concat([output, predictions], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output file to neural.csv\n"
     ]
    }
   ],
   "source": [
    "result.columns = ['PRT_ID','SALES_PRICE']\n",
    "print(\"Writing output file to neural.csv\")\n",
    "result.to_csv(\"../output/neural.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
